import pandas as pd
import numpy as np
unemp = r.unemp
#unemp.index = unemp.DATE
df = unemp.copy()
df = df.rename(columns={"UNRATE": "data"})[['data']]
df.reset_index(drop=True, inplace=True)
train = df.iloc[-100:-50, :]
test = df.iloc[-50:-40, :]
# train.index = pd.to_datetime(train.index)
# test.index = pd.to_datetime(test.index)
## We can use the pandas.DataFrame.ewm() function to calculate the exponentially weighted moving average for a certain number of previous periods.
def average(series):
return float(sum(series))/len(series)
# moving average using n last points
def moving_average(series, n):
return average(series[-n:])
moving_average(train.data,4)
# weighted average, weights is a list of weights
def weighted_average(series, weights):
result = 0.0
weights.reverse()
for n in range(len(weights)):
result += series[-n-1] * weights[n]
return result
weights = [0.1, 0.15, 0.25, 0.5]
weighted_average(train.data.values, weights)
# given a series and alpha, return series of smoothed points
def exponential_smoothing(series, alpha):
result = [np.NaN,series[0]] # first value is same as series
for n in range(1, len(series)+10):
if n < len(series):
result.append(alpha * series[n] + (1 - alpha) * result[n])
else: # we are forecasting
value = result[-1]
result.append(value)
return result
res_exp_smooth8 = exponential_smoothing(train.data.values, alpha=0.8)
res_exp_smooth5 = exponential_smoothing(train.data.values, alpha=0.5)
res_exp_smooth2 = exponential_smoothing(train.data.values, alpha=0.2)
# given a series and alpha, return series of smoothed points
def double_exponential_smoothing(series, alpha, beta):
result = [series[0]]
for n in range(1, len(series)+10):
if n == 1:
level, trend = series[0], series[1] - series[0]
if n >= len(series): # we are forecasting
value = result[-1]
else:
value = series[n]
last_level, level = level, alpha*value + (1-alpha)*(level+trend)
trend = beta*(level-last_level) + (1-beta)*trend
result.append(level+trend)
return result
res_double_exp_smooth_alpha_9_beta9=double_exponential_smoothing(train.data.values, alpha=0.9, beta=0.9)
len(res_double_exp_smooth_alpha_9_beta9)
len(train.data.values)
def initial_trend(series, slen):
sum = 0.0
for i in range(slen):
sum += float(series[i+slen] - series[i]) / slen
return sum / slen
initial_trend(train.data.values,12)
def initial_seasonal_components(series, slen):
seasonals = {}
season_averages = []
n_seasons = int(len(series)/slen)
# compute season averages
for j in range(n_seasons):
season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))
# compute initial values
for i in range(slen):
sum_of_vals_over_avg = 0.0
for j in range(n_seasons):
sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]
seasonals[i] = sum_of_vals_over_avg/n_seasons
return seasonals
initial_seasonal_components(train.data.values,12)
def triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):
result = []
seasonals = initial_seasonal_components(series, slen)
for i in range(len(series)+n_preds):
if i == 0: # initial values
smooth = series[0]
trend = initial_trend(series, slen)
result.append(series[0])
continue
if i >= len(series): # we are forecasting
m = i - len(series) + 1
result.append((smooth + m*trend) + seasonals[i%slen])
else:
val = series[i]
last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)
trend = beta * (smooth-last_smooth) + (1-beta)*trend
seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]
result.append(smooth+trend+seasonals[i%slen])
return result
res_triple_exp_smooth = triple_exponential_smoothing(train.data.values, 12, 0.716, 0.029, 0.993, 10)
res = [res_exp_smooth8,res_exp_smooth5,res_exp_smooth2,res_double_exp_smooth_alpha_9_beta9,res_triple_exp_smooth]
RMSE = []
i=1
for i in range(len(res)):
RMSE.append(np.sqrt(np.mean(np.square((train.data.values[1:50]- res[i][1:50])))))
RMSE
import matplotlib.pyplot as plt
import datetime
plt.style.use('Solarize_Light2')
plt.clf()
fig = plt.figure()
ax1 = fig.add_subplot(5, 1, 1)
plt.plot(train.data.values, label='raw')
plt.plot(res_exp_smooth8, label='exp_smooth_alpha_0.8')
ax2 =fig.add_subplot(5, 1, 2)
plt.plot(train.data.values, label='raw')
plt.plot(res_exp_smooth5, label='exp_smooth_alpha_0.5')
ax3 =fig.add_subplot(5, 1, 3)
plt.plot(train.data.values, label='raw')
plt.plot(res_exp_smooth2, label='exp_smooth_alpha_0.2')
ax4 =fig.add_subplot(5, 1, 4)
plt.plot(train.data.values, label='raw')
plt.plot(res_double_exp_smooth_alpha_9_beta9, label='res_double_exp_smooth_alpha_9_beta9')
ax5 =fig.add_subplot(5, 1, 5)
plt.plot(train.data.values, label='raw')
plt.plot(res_triple_exp_smooth, label='res_triple_exp_smooth')
ax1.set_title('raw data vs exponential forevast')
ax1.legend(loc="upper left")
ax2.legend(loc="upper left")
ax3.legend(loc="upper left")
ax4.legend(loc="upper left")
ax5.legend(loc="upper left")
fig.tight_layout()
plt.show()
blogdown:::preview_site()
blogdown:::preview_site()
reticulate::repl_python()
blogdown::serve_site()
reticulate::repl_python()
blogdown::serve_site()
reticulate::repl_python()
---
title: "TS - missing data imputation"
reticulate::repl_python()
py$res
round(py$res)
?round
round(py$res, digits=3)
reticulate::repl_python()
round(py$res, digits=3)
round(py$res, digits=3)
round(py$res, digits=3)
round(py$res_initial, digits=3)
py$res_initial
r round(py$res_initial, digits=3)
reticulate::repl_python()
round(py$res_initial, digits=3)
reticulate::repl_python()
devtools::install_github("rstudio/reticulate")
reticulate::repl_python()
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python")
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
#markdown ----
knitr::opts_chunk$set(#fig.width=12,
fig.height=4,
out.width = '100%'
)
knitr::opts_chunk$set(include =TRUE, #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
warning = FALSE,
message =FALSE,
collapse=TRUE,
error=TRUE
)
options(scipen=999)
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python")
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
data_file_path = '/Users/lrabalski1/Desktop/prv/data/'
require(zoo)
require(data.table)
library(dplyr)
library(lubridate)
unemp <- fread(paste0(data_file_path,"bezrobocie_USA.csv")) %>% data.table::melt( id.vars='Year',
variable.name = "months",
value.name='UNRATE') %>% left_join(
data.frame(month_nr=c(1:12),
months= c("Jan","Feb","Mar",
"Apr","May","Jun",
"Jul","Aug","Sep",
"Oct","Nov","Dec"))
) %>% mutate(DATE=as_date('0000-01-01',format = '%Y-%m-%d')+years(as.numeric(Year)) + months(month_nr-1))
head(unemp)
unemp = unemp[, DATE := as.Date(DATE)][!is.na(UNRATE),.(DATE, UNRATE)]
setkey(unemp, DATE)
## Creating dataset with random missing values
rand.unemp.idx <- sample(1:nrow(unemp), .1*nrow(unemp))
rand.unemp <- unemp[-rand.unemp.idx]
## Creating dataset with systematical missing values, appearing in month with highest unemployment rate
high.unemp.idx <- which(unemp$UNRATE > 8)
high.unemp.idx <- sample(high.unemp.idx, .5 * length(high.unemp.idx))
bias.unemp <- unemp[-high.unemp.idx]
## to identyfy missing data I wil use rolling joins tool from data.table package
all.dates <- seq(from = unemp$DATE[1], to = tail(unemp$DATE, 1), by = "months")
rand.unemp = rand.unemp[J(all.dates), roll=FALSE]
bias.unemp = bias.unemp[J(all.dates), roll=FALSE]
## forward filling
rand.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]
bias.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]
## Mean moving average with use of lookahead phenomen
rand.unemp[, impute.rm.lookahead := rollapply(data=c(UNRATE,NA, NA), width=3,
FUN= function(x) {
if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)
})]
bias.unemp[, impute.rm.lookahead := rollapply(c(UNRATE, NA,NA), 3,
FUN= function(x) {
if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)
})]
## Mean moving average withou use of lookahead phenomen
rand.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,
function(x) {
if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)
})]
bias.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,
function(x) {
if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)
})]
## linear interpolation fullfilling NA with linear interpolation between two data points
rand.unemp[, impute.li := na.approx(UNRATE, maxgap=Inf)]
bias.unemp[, impute.li := na.approx(UNRATE)]
zz <- c(NA, 9, 3, NA, 3, 2,NA,5,6,10,NA,NA,NA,0)
na.approx(zz, na.rm = FALSE, maxgap=2)
na.approx(zz, na.rm = FALSE, maxgap=Inf)
na.approx(zz,xout=11, na.rm = FALSE, maxgap=Inf)
## Using root mean square error to compare methods
print(rand.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),
.SDcols = c("impute.ff", "impute.rm.nolookahead", "impute.rm.lookahead", "impute.li")])
print(bias.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),
.SDcols = c("impute.ff", "impute.rm.nolookahead", "impute.rm.lookahead", "impute.li")])
reticulate::repl_python()
