{
  "hash": "aaba7a58d35db0bf7e87902f5e5103ac",
  "result": {
    "markdown": "---\ntitle-block-banner: true\nauthor: Łukasz Rąbalski\ntitle: Statistics\ndescription: Describing statistical methods\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    toc-depth: 5\n    embed-resources: true\ncategories: [\"Statistics\"]\ntags: [\"R\", \"Statistics\", \"Bayes\"]\neditor: source\nfig.height: 4\nout.width: '100%'\ninclude: TRUE  #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho: TRUE  #echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nwarning: FALSE\nmessage: FALSE\nerror: FALSE\n---\n\n\n## Logistic Regression\n\ninspirations: \n[url](https://quantifyinghealth.com/interpret-logistic-regression-intercept/) [url](https://quantifyinghealth.com/interpret-logistic-regression-coefficients/)\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n| Survived| independent|\n|--------:|-----------:|\n|        0|    1.945910|\n|        1|    4.262680|\n|        1|    2.079442|\n|        1|    3.970292|\n|        0|    2.079442|\n|        0|    2.079442|\n:::\n\n::: {.cell-output-display}\nTable: Table 1. Summary statistics for logistic regression model\n\n|term        |   estimate| std.error|  statistic| p.value|\n|:-----------|----------:|---------:|----------:|-------:|\n|(Intercept) | -2.6623408| 0.2493139| -10.678668|       0|\n|independent |  0.7429505| 0.0798121|   9.308746|       0|\n:::\n:::\n\n\n### Matematyczna interpretacja modelu\n\n### equation\n\nHere's the equation of a logistic regression model with 1 predictor X:\n\n$log(\\frac{P}{1-P})=\\beta_0 + \\beta_1X$\n\nWhere: ***P*** is the probability of having the outcome $\\frac{P}{1-P}$ is the odds of the outcome. Left side called ***log odds*** or ***logit*** and P / (1-P) is the odds of the outcome. $\\beta_0$ called intercept\n\n### From log odds to probability\n\nTo solve for the probability P, we exponentiate both sides of the equation above to get:\n\n`Equation 1`.: $\\frac{P}{1-P}= e^{\\beta_0 + \\beta_1X}$\n\nWith this equation, we can calculate the probability P for any given value of X\n\n### intercept interpretation\n\nWhen X = 0 the interpretation of intercept become simple using `Equation 1.` $P=\\frac{e^{\\beta_0}}{(1 + e^{\\beta_0})}$ :\n\n-   If the intercept has a negative sign: then the probability of having the outcome will be \\< 0.5.\n\n-   If the intercept has a positive sign: then the probability of having the outcome will be \\> 0.5.\n\n-   If the intercept is equal to zero: then the probability of having the outcome will be exactly 0.5.\n\nSo in case of my log model\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nprint(coef(model)[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n  -2.662341 \n```\n:::\n\n```{.r .cell-code}\nprob_of_survival<-round(exp(coef(model)[1])/(1+exp(coef(model)[1])),2)\nprint(paste0('according to model probability of surving the trip if the independent  variable was = 0 is: ',prob_of_survival))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"according to model probability of surving the trip if the independent  variable was = 0 is: 0.07\"\n```\n:::\n:::\n\n\n### estimate interpretation\n\n`Remember`, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n#Tak przemnozone wspolczynniki interpretujemy nastepujaco:\n#  o ile % wzrosnie odds wystapienia zdarzenia jezeli wzrosnie nam wartosc predyktora o 1\nexp(coef(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) independent \n 0.06978468  2.10212868 \n```\n:::\n:::\n\n\nPonizej w sposob matematyczny pokazuje ze to wlasnie oznacza interpretacja wzrostu parametra stajacego przy predyktorze.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ndf_aug <- augment(model, type.predict = \"response\") # without response argument, the fitted value will be on log-odds scale\n\np1 = df_aug$.fitted[df_aug$independent==1][1]\np2 = df_aug$.fitted[df_aug$independent==2][1]\n\nx <- round(p2/(1-p2)/(p1/(1-p1)),5)\n\n# i sprawdzenie czy dobrze rozumiem zależnosc\nx1<-round(exp(coef(model))['independent'],5)\nx1==x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nindependent \n         NA \n```\n:::\n:::\n\n\nProb for independent= 1 was equal to NA while for independent = 2 was equal to NA. The odds increase by NA. The same what model results suggests -\\> 2.10213.\n\n**`Quiz`**\n\nThe fitted coefficient from the medical school logistic regression model is 5.45. The exponential of this is 233.73.\n\nDonald's GPA is 2.9, and thus the model predicts that the probability of him getting into medical school is 3.26%. The odds of Donald getting into medical school are 0.0337, or---phrased in gambling terms---29.6:1. If Donald hacks the school's registrar and changes his GPA to 3.9, then which of the following statements is FALSE:\n\nPossible Answers\n\na)  His expected odds of getting into medical school improve to 7.8833 (or about 9:8).\nb)  His expected probability of getting into medical school improves to 88.7%.\nc)  His expected log-odds of getting into medical school improve by 5.45.\nd)  His expected probability of getting into medical school improves to 7.9%.\n\nCorrect answers on the bottom of the page.\n\n### Graficzna interpretacja modelu\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-6-1.png){fig-align='left' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ndf_aug %>% mutate(Survived_hat=round(.fitted)) %>%\n  select(Survived, Survived_hat) %>% table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Survived_hat\nSurvived   0   1\n       0 470  75\n       1 219 123\n```\n:::\n\n```{.r .cell-code}\n#Out of sample predictions\nDiCaprio<-data.frame(independent=1)\naugment(model, newdata = DiCaprio, type.predict = 'response')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  independent .fitted\n        <dbl>   <dbl>\n1           1   0.128\n```\n:::\n:::\n\n\n### stepwise\nIn order to run stepwise regression you should use library SignifReg.\n\nSignifReg selects only significant predictors according to a designated criterion. More to be find [url](https://cran.r-project.org/web/packages/SignifReg/SignifReg.pdf)\n\nMethod based on AIC criterion not basing on significance of predictors to be found here [url](https://quantifyinghealth.com/stepwise-regression-in-r/)\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nlibrary(SignifReg)\ndep_var_temp = 'Survived'\n\nfullmodel = glm(paste0(dep_var_temp,\" ~ .\"), family = 'binomial', data = df_titanic %>% select(-Name)) # model with all variables\nnullmodel = glm(paste0(dep_var_temp,\" ~ 1\"), family = 'binomial', data = df_titanic) # model with the intercept only\n  \nscope = list(lower=formula(nullmodel),upper=formula(fullmodel))\nfit1 <- nullmodel\nselect.fit = SignifReg(fit1, scope = scope, direction = \"forward\", trace = FALSE)\n\n\nbroom::tidy(select.fit) %>% \n  mutate(dep_var=dep_var_temp) %>% rename(indep_var=term) %>%  dplyr::select(dep_var,indep_var, everything()) %>%\n  mutate_if(is.numeric, round, digits=2) %>% rename(t.statistic=statistic) %>%\n  mutate(`exp_(estimate)`=ifelse(indep_var!='(Intercept)',exp(estimate),NA)) %>%\n  relocate(`exp_(estimate)`,.after=estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  dep_var  indep_var               estimate exp_(estim…¹ std.e…² t.sta…³ p.value\n  <chr>    <chr>                      <dbl>        <dbl>   <dbl>   <dbl>   <dbl>\n1 Survived (Intercept)                 4.03      NA         0.82    4.91    0   \n2 Survived Sexmale                    -2.67       0.0693    0.2   -13.5     0   \n3 Survived Pclass                     -1.01       0.364     0.17   -6.03    0   \n4 Survived Age                        -0.04       0.961     0.01   -5.65    0   \n5 Survived Siblings.Spouses.Aboard    -0.53       0.589     0.12   -4.41    0   \n6 Survived independent                 0.33       1.39      0.14    2.31    0.02\n# … with abbreviated variable names ¹​`exp_(estimate)`, ²​std.error, ³​t.statistic\n```\n:::\n:::\n\n\n\n\n\n## Bayesian Statistics - Introduction\n\n### Introduction\n\nThe role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update these probability distributions to reflect what has been learned from data.\n\nLet say I want to set an advertisement on social media. They claim, adds on their surface has 10% of clicks. I a bit sceptical and asses probable efectivnes may range between 0 and 0.20. I assume that binomial model will imitate process generating visitors. Binomial model is my generative model then.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nn_samples <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_samples, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)\n\npar(mfrow=c(1,2))\n# Visualize proportion clicks\nhist(proportion_clicks)\n# Visualize n_visitors\nhist(n_visitors)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-8-1.png){fig-align='left' width=672}\n:::\n:::\n\n\nBelow I present joint distribution over both the underlying proportion of clicks and how many visitors I would get.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-9-1.png){fig-align='left' width=672}\n:::\n:::\n\n\nI ran my ad campaign, and 13 people clicked and visited your site when the ad was shown a 100 times. I would now like to use this new information to update the Bayesian model. The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Create the posterior data frame\nposterior <- prior[prior$n_visitors == 13, ]\n\n# Visualize posterior proportion clicks - below I condition the joint distribution - of prior distribution of proportion_clicks and distribution of n_visitors \nhist(posterior$proportion_clicks)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-10-1.png){fig-align='left' width=672}\n:::\n:::\n\n\nNow we want to use this updated proportion_clicks to predict how many visitors we would get if we reran the ad campaign.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Assign posterior to a new variable called prior\nprior <- posterior\n\n# Take a look at the first rows in prior\nhead(prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   proportion_clicks n_visitors\n10         0.1973504         13\n22         0.1555724         13\n66         0.1213712         13\n68         0.1366464         13\n93         0.1340447         13\n96         0.1325997         13\n```\n:::\n\n```{.r .cell-code}\n# Replace prior$n_visitors with a new sample and visualize the result\nn_samples <-  nrow(prior)\nn_ads_shown <- 100\nprior$n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = prior$proportion_clicks)\nhist(prior$n_visitors)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-11-1.png){fig-align='left' width=672}\n:::\n:::\n\n\n### Priors\n\n#### Beta distribution\n\nThe Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you'll explore how the two parameters of the Beta distribution determine its shape.\n\nSo the larger the shape parameters are, the more concentrated the beta distribution becomes.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Explore using the rbeta function\nbeta_1 <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)\nbeta_2 <- rbeta(n = 1000000, shape1 = 100, shape2 = 100)\nbeta_3 <- rbeta(n = 1000000, shape1 = 100, shape2 = 20)\nbeta_4 <- rbeta(n = 1000000, shape1 = 5, shape2 = 95)\n\n\n\npar(mfrow=c(2,2))\nhist(beta_1, breaks=seq(0,1,0.02), main = \"shape1 = 1, shape2 = 1\")\nhist(beta_2, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 100\")\nhist(beta_3, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 20\")\nhist(beta_4, breaks=seq(0,1,0.02), main = \"shape1 = 5, shape2 = 95\")\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-12-1.png){fig-align='left' width=672}\n:::\n:::\n\n\nThe 4th graphs represents the best following setence: *Most ads get clicked on 5% of the time, but for some ads it is as low as 2% and for others as high as 8%.*\n\n### Contrasts and comparison\n\nLet say, I initialize also text add campaign, get 6 visitors out of 100 views and now I want to compare which one video or text add is more cost effective.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Define parameters\nn_draws <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_draws, size = n_ads_shown, \n                     prob = proportion_clicks)\nprior <- data.frame(proportion_clicks, n_visitors)\n\n# Create the posteriors for video and text ads\nposterior_video <- prior[prior$n_visitors == 13, ]\nposterior_text <- prior[prior$n_visitors == 6, ]\n\n# Visualize the posteriors\nhist(posterior_video$proportion_clicks, xlim = c(0, 0.25))\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-1.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\nhist(posterior_text$proportion_clicks, xlim = c(0, 0.25))\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-2.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\nposterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000],\n                        text_prop = posterior_text$proportion_click[1:4000])\n\n# Calculate the posterior difference: video_prop - text_prop\nposterior$prop_diff <- posterior$video_prop - posterior$text_prop \n\n# Visualize prop_diff\nhist(posterior$prop_diff)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-3.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\n# Calculate the median of prop_diff\nmedian(posterior$prop_diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0647875\n```\n:::\n\n```{.r .cell-code}\n# Calculate the proportion\nmean(posterior$prop_diff > 0.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9475\n```\n:::\n\n```{.r .cell-code}\n#Different adds have differnt costs then:\nvisitor_spend <- 2.53\nvideo_cost <- 0.25\ntext_cost <- 0.05\n\n# Add the column posterior$video_profit\nposterior$video_profit <- posterior$video_prop * visitor_spend - video_cost\n\n# Add the column posterior$text_profit\nposterior$text_profit <- posterior$text_prop * visitor_spend - text_cost\n\n# Visualize the video_profit and text_profit columns\nhist(posterior$video_profit)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-4.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\nhist(posterior$text_profit)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-5.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\n# Add the column posterior$profit_diff\nposterior$profit_diff <- posterior$video_profit - posterior$text_profit\n\n# Visualize posterior$profit_diff\nhist(posterior$profit_diff)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-13-6.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\n# Calculate a \"best guess\" for the difference in profits\nmedian(posterior$profit_diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.03608763\n```\n:::\n\n```{.r .cell-code}\n# Calculate the probability that text ads are better than video ads\nmean(posterior$profit_diff < 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.64175\n```\n:::\n\n```{.r .cell-code}\n#So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose the text ads is better.\n```\n:::\n\n\n#### Changeing Generative model\n\nCompany has changed the way how they price adds. Now they take money just for full day of exposition. Binomial model, which approximate participation of succes in all trials (click in all views) is no longer valid. For new scenario. **Poison distribution** is now needed.\n\n**The Poison distribution takes only one parameter which is the mean number of events per time unit**\n\nIn R you can simulate from a Poisson distribution using rpois where lambda is the average number of occurrences:\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Change the model according to instructions\nn_draws <- 100000\nmean_clicks <- runif(n_draws, min = 0, max = 80) #this is my prior\nn_visitors <- rpois(n = n_draws, mean_clicks)\n\nprior <- data.frame(mean_clicks, n_visitors)\nposterior <- prior[prior$n_visitors == 19, ]\n\nhist(prior$mean_clicks)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-14-1.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\nhist(posterior$mean_clicks)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-14-2.png){fig-align='left' width=672}\n:::\n:::\n\n\n### Dealing with 2 parameter model\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n#  the temperatures of Sweden water in 21 th of June in few following year\ntemp <- c(19,23,20,17,23)\n# Defining the parameter grid - here are are my priors about the posible values of parameters of distribution\npars <- expand.grid(mu = seq(8,30, by = 0.5), \n                    sigma = seq(0.1, 10, by= 0.3))\n# Defining and calculating the prior density for each parameter combination\npars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)\npars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)\npars$prior <- pars$mu_prior * pars$sigma_prior\n# Calculating the likelihood for each parameter combination\nfor(i in 1:nrow(pars)) {\n  likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])\n  pars$likelihood[i] <- prod(likelihoods)\n}\n# Calculate the probability of each parameter combination\npars$probability <- pars$likelihood * pars$prior\npars$probability <- pars$probability / sum(pars$probability )\n\nlibrary(lattice)\nlevelplot(probability ~ mu * sigma, data = pars)\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-15-1.png){fig-align='left' width=672}\n:::\n:::\n\n\nWhat's likely the average water temperature for this lake on 20th of Julys, and what's the probability the water temperature is going to be 18 or more on the next 20th?\n\nRight now the posterior probability distribution is represented as a data frame with one row per parameter combination with the corresponding probability.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nhead(pars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mu sigma   mu_prior sigma_prior       prior likelihood probability\n1  8.0   0.1 0.01079819         0.1 0.001079819          0           0\n2  8.5   0.1 0.01312316         0.1 0.001312316          0           0\n3  9.0   0.1 0.01579003         0.1 0.001579003          0           0\n4  9.5   0.1 0.01880982         0.1 0.001880982          0           0\n5 10.0   0.1 0.02218417         0.1 0.002218417          0           0\n6 10.5   0.1 0.02590352         0.1 0.002590352          0           0\n```\n:::\n:::\n\n\nBut my questions are much easier to answer if the posterior is represented as a large number of samples, like in earlier chapters. So, let's draw a sample from this posterior.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nsample_indices <- sample(1:nrow(pars), size=10000, replace=TRUE, prob=pars$probability)\npars_sample <- pars[sample_indices,c(\"mu\",\"sigma\")]\nhead(pars_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mu sigma\n252 21.0   1.6\n793 21.5   5.2\n519 19.5   3.4\n518 19.0   3.4\n341 20.5   2.2\n296 20.5   1.9\n```\n:::\n:::\n\n\nWhat is probabibility of temperature being 18 or above? Not mean temperature, the actual temperature.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n#rnorm is vectorized and implicitly loops over mu and sigma\npred_temp<- rnorm(10000, mean=pars_sample$mu, sd=pars_sample$sigma)\n\npar(mfrow=c(1,2))\nhist(pars_sample$mu,30, main = 'probability distribution of mean temperature')\nhist(pred_temp,30, main = 'probability distribution of tempeture' )\n```\n\n::: {.cell-output-display}\n![](3_Statistics_files/figure-html/unnamed-chunk-18-1.png){fig-align='left' width=672}\n:::\n\n```{.r .cell-code}\nmean(pred_temp>=18)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7366\n```\n:::\n:::\n\n\n### Automatisation - BEST package\n\nThe Bayesian model behind BEST assumes that the generative model for the data is a t-distribution; a more flexible distribution than the normal distribution as it assumes that data points might be outliers to some degree. This makes BEST's estimate of the mean difference robust to outliers in the data.\n\nThe t-distribution is just like the normal distribution, a generative model with a mean and a standard deviation that generates heap shaped data. The difference is that the t-distribution has an extra parameter, sometimes called the degrees-of-freedom parameter, that governs how likely the t-distribution is to generate outliers far from its center.\n\nAnother way in which BEST is different is that BEST uses a so-called Markov chain Monte Carlo method to fit the model. Markov chain Monte Carlo, or MCMC for short, returns a table of samples from the posterior, we can work with the output just like before.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# The IQ of zombies on a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)\n\n# Calculate the mean difference in IQ between the two groups\nmean(iq_brains) - mean(iq_regular)\n\n# Fit the BEST model to the data from both groups\nlibrary(BEST)\nlibrary(rjags)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\n\n# Plot the model result\nplot(best_posterior)\n```\n:::\n\n\nAssume that a super smart mutant zombie (IQ = 150) got into the iq_regular group by mistake. This might mess up the results as you and your colleagues really were interested in how diet affects normal zombies.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# The IQ of zombies given a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, \n                150) # <- Mutant zombie\n\n# Modify the data above and calculate the difference in means\nmean(iq_brains) - mean(iq_regular)\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\n# Fit the BEST model to the modified data and plot the result\nlibrary(BEST)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\nplot(best_posterior)\n```\n:::\n\n\n### Conclusions\n\nBayes allows you to tweak, change and tinker with the model to better fit the data analytical problem you have. But a last reason to use Bayes is because it is optimal, kind of. It can be shown, theoretically, that no other method learns as efficiently from data as Bayesian inference.\n\nIn above examples I show what Bayesian model is about: \\* I describe my expectations of proportion_clicks as uniform distribution (prior) \\* Then i describe a generative model which will be responsible for generating views based on proportion_clicks - the second source of variability. For this aim I use two diffrent distribution - binomial and poison - depending on specifity of exercise. \\* I was able to say which add wass better, more, I was able to say which add was better in probability way.\n\n## Bayesian Statistics - Intermediate\n\n### Likelihood\n\nOn the example of poll. Imagine I am taking part in election to local goverment. Based on many historical election poles I can count on 45% of votes. Votes chances are approximate by bheta function.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ndf<-data.frame(sample=seq(0,1,0.01),\n               density=dbeta(x=seq(0,1,0.01),shape1=45,shape2=55))\ndf %>% ggplot(aes(x=sample,y=density))+\n  geom_line()+\n  ggtitle(\"Density function\")\n```\n:::\n\n\nLets imagine that i receive 60% of votes in ellection pole. I can assume that binomial distribution is well suited for generative model responsible for how many votes I am geting. Then I may ask myself: \\*\\*How probable would be obtaining such a results (60%) of votes under different succes_rate (paramter of Binomial distribution).\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ndf<-data.frame(likelihood=dbinom(x=6,size=10,prob=seq(0,1,0.1)), \n               parameter_p=seq(0,1,0.1))\n\ndf %>% ggplot(aes(x=parameter_p,y=likelihood))+\n  geom_line()+\n  ggtitle(\"Likelihood distribution over different succes_rate parameters\")\n```\n:::\n\n\nThe likelihood function summarizes the likelihood of observing polling data X under different values of the underlying support parameter p. Thus, the likelihood is a function of p that depends upon the observed data X\n\n### Posterior\n\nSince I've got the prior & likelihood:\n\n-   prior: let say based on the historical pole % of votes I can count on is described by betha distribution Betha(45.55) --\\> most probable is geting 45% votes\n\n-   likelihood: is denoting to the most recent data shown above\n\nI can approach now to modeling **posterior model of p** According to Bayes rules posterior is calculating by:\n\n*posterior* = prior \\* likelihood\n\nHowever, in more sophisticated model settings, tidy, closed-form solutions to this formula might not exist. Very loosely speaking, the goal here is to send information out to the JAGS program, which will then design an algorithm to sample from the posterior, based on which I will then simulate the posterior.\n\n#### Compiling rjags model\n\nBuilt from previous polls & election data, my prior model of is a Beta(,) with shape parameters a=45 and b=55. For added insight into **p**, I also polled potential voters. The dependence of X, the number of these voters that support you, on **p** is modeled by the Bin(**n**,**p**) distribution.\n\nIn the completed poll, X=6 of n=10 voters supported you. The next goal is to update my model of in light of these observed polling data! To this end, I will use the rjags package to approximate the posterior model of . This exercise will be break down into the 3 rjags steps: define, compile, simulate.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nlibrary(rjags)\n\n# DEFINE the model\nvote_model <- \"model{\n    # Likelihood model for X\n    X ~ dbin(p, n)\n    \n    # Prior model for p\n    p ~ dbeta(a, b)\n}\"\n\n# COMPILE the model    \nvote_jags <- jags.model(textConnection(vote_model), \n    data = list(a = 45, b = 55, X = 6, n = 10),\n    inits = list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 100))\n\n# SIMULATE the posterior\nvote_sim <- coda.samples(model = vote_jags, variable.names = c(\"p\"), n.iter = 10000)\n\n# PLOT the posterior\nplot(vote_sim, trace = FALSE)\n```\n:::\n\n\nQuiz correct answers: d. Hint: \\`Remember\\`, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable.\n",
    "supporting": [
      "3_Statistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}