---
title: "draft"
date: "2022-11-20"
tags: []
categories: []
draft: TRUE
toc: TRUE
editor: source
---

```{r markdown_parameters, include=FALSE}

#markdown ----
knitr::opts_chunk$set(#fig.width=12, 
                      fig.height=4,
                       out.width = '100%'
                      ) 
knitr::opts_chunk$set(include =TRUE, #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
                      echo = FALSE, #echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
                      warning = FALSE,
                      message =FALSE,
                      collapse=TRUE,
                      error=TRUE
                      )
options(scipen=999)
```

```{r, include=FALSE}
library(dplyr)
```

```{r}
diamonds <- ggplot2::diamonds %>%
  # I am creating binary vairable
  mutate(success=ifelse(cut %in% c('Premium','Ideal'),1,0)) %>% select(-cut) %>%
  # forcing one class to be unrepresented
  mutate(random=runif(nrow(.))) %>% 
  mutate(exclude=ifelse(success==0 & random<0.9,1,0)) %>% filter(exclude!=1)

table(diamonds$success) %>% prop.table()
```


# dataset exploration

# decision tree



```{r}

library(rpart)
library(rpart.plot)
set.seed(123)
fit <- rpart(success ~ .,method  = "class", data = diamonds)
rpart.plot(fit, box.palette="RdBu", shadow.col="gray", nn=TRUE,  digits = 4)
#saveRDS(model, "output/model.rds")

```

```{r}
summary(fit)
```


Behind the scenes rpart() is automatically applying a range of cost complexity (α values to prune the tree). To compare the error for each α value, rpart() performs a 10-fold CV (by default). 



In this example we find diminishing returns after 6 terminal nodes as illustrated in Figure below

```{r}
plotcp(fit)
```

y-axis is the CV error, 
lower x-axis is the cost complexity (α) value, 
upper x-axis is the number of terminal nodes (i.e., tree size = |T|)

You may also notice the dashed line which goes through the point |T|=4. Breiman (1984) suggested that in actual practice, it’s common to instead use the smallest tree within 1 standard error (SE) of the minimum CV error (this is called the 1-SE rule). Thus, we could use a tree with 3 terminal nodes and reasonably expect to experience similar results within a small margin of error.


To illustrate the point of selecting a tree with 6 terminal nodes (or 4 if you go by the 1-SE rule), we can force rpart() to generate a full tree by setting cp = 0 (no penalty results in a fully grown tree). Figure below shows that after 4 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can significantly prune our tree and still achieve minimal expected error.


```{r}
ctrl <- list(cp = 0, xval = 10#, minbucket = 1, maxdepth = 5
             )

fit_no_limit <- rpart(
    formula = success ~ .,
    data    = diamonds,
    method  = "class", 
    control = ctrl
)


plotcp(fit_no_limit)
abline(v = 11, lty = "dashed")
```


So, by default, rpart() is performing some automated tuning, with an optimal subtree of 6 total splits, 6 terminal nodes, and a cross-validated SSE of 0.553.


```{r}
set.seed(123)
fit$cptable
```


## dealing with imbalance
### loss matrix

You can include a loss matrix, changing the relative importance of misclassifying a default as non-default versus a non-default as a default. You want to stress that misclassifying a default as a non-default should be penalized more heavily. 


```{r, eval=FALSE}
parms = list(loss = matrix(c(0, cost_def_as_nondef, cost_nondef_as_def, 0), ncol=2))
```


Doing this, you are constructing a 2x2-matrix with zeroes on the diagonal and changed loss penalties off-diagonal. The default loss matrix is all ones off-diagonal.

penalization that is 20 times bigger when misclassifying an actual default as a non-default.

```{r}
parms = list(loss = matrix(c(0, 20, 1, 0), ncol = 2))
fit_loss_matrix <- rpart(success ~ .,method  = "class", data = diamonds,parms=parms)

rpart.plot(fit_loss_matrix, box.palette="RdBu", shadow.col="gray", nn=TRUE,  digits = 4)
```




## weights & costs
The weights is for rows (e.g. give higher weight to smaller class), the cost is for columns.

weights

    optional case weights.

cost

    a vector of non-negative costs, one for each variable in the model. Defaults to one for all variables. These are scalings to be applied when considering splits, so the improvement on splitting on a variable is divided by its cost in deciding which split to choose.


## bagging

Bootstrapping  can be used to create an _ensemble_ of predictions. Bootstrap aggregating, also called _bagging_, is one of the first ensemble algorithms machine learning practitioners learn and is designed to improve the stability and accuracy of regression and classification algorithms. 
By model averaging, bagging helps to reduce variance and minimize overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.


```{r}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees


# create Ames training data
set.seed(123)
split  <- rsample::initial_split(diamonds, prop = 0.7, strata = "success")
diamonds_train  <- rsample::training(split)
diamonds_test  <- rsample::testing(split)
```



The bagging() function comes from the ipred package and we use nbagg to control how many iterations to include in the bagged model and coob = TRUE indicates to use the OOB error rate. By default, bagging() uses rpart::rpart() for decision tree base learners but other base learners are available. Since bagging just aggregates a base learner, we can tune the base learner parameters as normal. Here, we pass parameters to rpart() with the control parameter and we build deep trees (no pruning) that require just two observations in a node to split. 


```{r}
# make bootstrapping reproducible
set.seed(123)

# train bagged model
model <- bagging(
  formula = success ~ .,
  data = diamonds,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)
```


One thing to note is that typically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more). For the Ames data we see that the error is stabilizing with just over 100 trees so we’ll likely not gain much improvement by simply bagging more trees.


```{r}
ntree <- seq(1, 200, by = 2)
# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)
  # perform bagged model
  model <- ranger::ranger(
  formula = success ~ .,
  data = diamonds,
  num.trees = ntree[i],
  mtry = ncol(diamonds) - 1,
  min.node.size = 1)
  
  
  # get OOB error
  rmse[i] <- sqrt(model$prediction.error)
}

bagging_errors <- data.frame(ntree, rmse)

ggplot(bagging_errors, aes(ntree, rmse)) +
  geom_line() +
  #geom_hline(yintercept = 41019, lty = "dashed", color = "grey50") +
  #annotate("text", x = 100, y = 41385, label = "Best individual pruned tree", vjust = 0, hjust = 0, color = "grey50") +
  #annotate("text", x = 100, y = 26750, label = "Bagged trees", vjust = 0, hjust = 0) +
  ylab("RMSE") +
  xlab("Number of trees")

```


## grid search

%% inspiration https://drsimonj.svbtle.com/grid-search-in-the-tidyverse %%

### Training-Test Split
To help validate our hyperparameter combinations, we’ll split our data into training and test sets (in an 80/20 split):

```{r}
set.seed(245)
n <- nrow(diamonds)
train_rows <- sample(seq(n), size = .8 * n)
train <- diamonds[ train_rows, ]
test  <- diamonds[-train_rows, ]
```


### Create the Grid 


Step one for grid search is to define our hyperparameter combinations. Say we want to test a few values for minsplit and maxdepth. I like to setup the grid of their combinations in a tidy data frame with a list and cross_d as follows:


```{r}
library(tidyverse)
gs <- list(minsplit = c(2, 5, 10),
           cp=0,
           maxdepth = c(1, 3, 8)) %>% 
  cross_d()
```


Note that the list names are the names of the hyperparameters that we want to adjust in our model function.

### Create a model function

We’ll be iterating down the gs data frame to use the hyperparameter values in a rpart model. The easiest way to handle this is to define a function that accepts a row of our data frame values and passes them correctly to our model. Here’s what I’ll use:


```{r}
mod <- function(...) {
  rpart(success~., data = train,method='class', control = rpart.control(...))
}
```


### Fit the models
Now, to fit our models, use pmap to iterate down the values. The following is iterating through each row of our gs data frame, plugging the hyperparameter values for that row into our model.


```{r}
gs <- gs %>% mutate(fit = pmap(gs, mod))
```


### Obtain accuracy


Next, let’s assess the performance of each fit on our test data. To handle this efficiently, let’s write another small function:

```{r}
compute_accuracy <- function(fit, test_features, test_labels) {
  predicted <- predict(fit, test_features, type = "class")
  mean(predicted == test_labels)
}

```

Now apply this to each fit:

```{r}
test_features <- test %>% select(-success)
test_labels   <- test$success

gs_acc <- gs %>%
  mutate(test_accuracy = map_dbl(fit, compute_accuracy,
                                 test_features, test_labels))
gs_acc


rpart.plot(gs$fit[[1]], box.palette="RdBu", shadow.col="gray", nn=TRUE,  digits = 4)

```



## bootstraping


https://rapidsurveys.io/learn/statistics/bootstrap/

```{r}
df_bootstraping <- rsample::bootstraps(diamonds, times = 10)
df_bootstraping$splits[1]

library(purrr)
library(modeldata)
data(wa_churn)

set.seed(13)
resample1 <- rsample::bootstraps(wa_churn, times = 3)
map_dbl(
  resample1$splits,
  function(x) {
    dat <- as.data.frame(x)$churn
    mean(dat == "Yes")
  }
)
resample1$splits

wa_churn[c(1,1,1),]
```


## random Forest


```{r}
# Helper packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics

# Modeling packages
library(ranger)   # a c++ implementation of random forest 
library(h2o)      # a java-based implementation of random forest
```


# CV

perform CV directly within certain ML functions:


```{r}
library(h2o)
h2o.init()

ames <- AmesHousing::make_ames()
ames.h2o <- h2o::as.h2o(ames)

h2o.cv <- h2o.glm(
  x = 'Lot_Area', 
  y = 'Lot_Frontage', 
  training_frame = ames.h2o,
  nfolds = 10  # perform 10-fold CV
)
```


Or externally as in the below chunk5. When applying it externally to an ML algorithm as below, we’ll need a process to apply the ML model to each resample, which we’ll also cover.


```{r}
rsample::vfold_cv(ames, v = 10)
```

