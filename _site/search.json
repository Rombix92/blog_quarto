[
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html",
    "title": "Grow of Metropolis",
    "section": "",
    "text": "Aims:\n\nFIlter out the data localisation where people are supposed to live in (cities; villages etc)\nSelect cities which are metropolis based on population on external source\nWrite a script which will ascribe metropolis to a city. Additionally model a process of consecutive cities which would be ascribed to the city bearing in mind that city will grow thanks to this process.\n\nData Source\nDocumentation"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "title": "Grow of Metropolis",
    "section": "Preparing environment",
    "text": "Preparing environment\nSetting default markdown option responsible of code chunk behaviour.\nFirstly I choose prefered python environment on which I have installed useful libraries.\n\n\nShow the code\nlibrary(reticulate)\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\")\nreticulate::py_config()\n## python:         /Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\n## libpython:      /Users/lrabalski1/miniforge3/envs/everyday_use/lib/libpython3.8.dylib\n## pythonhome:     /Users/lrabalski1/miniforge3/envs/everyday_use:/Users/lrabalski1/miniforge3/envs/everyday_use\n## version:        3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17)  [Clang 11.1.0 ]\n## numpy:          /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/numpy\n## numpy_version:  1.21.4\n## \n## NOTE: Python version was forced by RETICULATE_PYTHON\n\nmyenvs=conda_list()\nenvname=myenvs$name[4]\nuse_condaenv(envname, required = TRUE)\n\n\nBellow I present two function:\n\nradius - function which based on city population is calculating a radius within which city is able to absorb cities from this range\n_calcualate_metrocity_impact - calculate impact on metrocity on given city\n\n\n\nShow the code\ndef radius(population):\n    METRO_CITY_POPULATION_CONSTANT = -1/1443000\n    MIN_METRO_CITY_RADIUS = 10\n    MAX_METRO_CITY_RADIUS = 100 - MIN_METRO_CITY_RADIUS\n    return MIN_METRO_CITY_RADIUS + MAX_METRO_CITY_RADIUS * (1 - np.exp(METRO_CITY_POPULATION_CONSTANT *  population))\n\ndef _calcualate_metrocity_impact(max_radius, distance_to_metro_city):\n    METRO_CITY_POWER_CONSTANT = -1.4\n    impact = np.exp(METRO_CITY_POWER_CONSTANT  * distance_to_metro_city / max_radius)\n    return impact\n\n\nFunction responsible for calculating distances between 2 points on earth surface.\n\n\nShow the code\n#https://towardsdatascience.com/heres-how-to-calculate-distance-between-2-geolocations-in-python-93ecab5bbba4\ndef haversine_distance_code(lat1, lon1, lat2, lon2):\n    r = 6371\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2 - lat1)\n    delta_lambda = np.radians(lon2 - lon1)\n    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n    return np.round(res, 2)\n\n\n\n\nShow the code\ndf= pd.read_csv(file_path_name, sep=\"\\t\", \n                names=['geonameid','name','asciiname','alternatenames','latitude','longitude','feature class','feature code','country code','cc2','admin1 code','admin2 code','admin3 code','admin4 code','population','elevation','dem','timezone','modification date',])\n\n\nDataset readme states that column feature classes contains level information of:\nA: country, state, region,…\nH: stream, lake, …\nL: parks,area, …\nP: city, village,…\nR: road, railroad\nS: spot, building, farm\nT: mountain,hill,rock,…\nU: undersea\nV: forest,heath,…\nWe will be interested on object of level P, and maybe A.\n\n\nShow the code\nimport requests\n\nurl = 'http://www.geonames.org/export/codes.html'\nhtml = requests.get(url).content\ndf_list = pd.read_html(html)\ndf_legend = df_list[-1]\ndf_legend = df_legend.rename(columns={df_legend.columns[0]: 'feature code',\n                                     df_legend.columns[1]: 'short  descr',\n                                     df_legend.columns[2]: 'long descr'})\ndf_legend = pd.merge(df[['feature code','feature class']].drop_duplicates(),df_legend, on='feature code')\ndf_legend\n##     feature code  ...                                         long descr\n## 0           PPLQ  ...                                                NaN\n## 1            STM  ...  a body of running water moving to a lower leve...\n## 2           HLLS  ...  rounded elevations of limited extent rising ab...\n## 3            CNL  ...                          an artificial watercourse\n## 4            PPL  ...  a city, town, village, or other agglomeration ...\n## ..           ...  ...                                                ...\n## 186          BCN  ...                 a fixed artificial navigation mark\n## 187         HSEC  ...  a large house, mansion, or chateau, on a large...\n## 188          RES  ...  a tract of public land reserved for future use...\n## 189         STNR  ...  a facility for producing and transmitting info...\n## 190         BLDA  ...  a building containing several individual apart...\n## \n## [191 rows x 4 columns]\n\n\n\n\nShow the code\ndf = df[df['feature class'].isin(['P','A'])]\ndf_check = pd.merge(df,df_legend, on=['feature code','feature class'])\n\n# sorting by the biggest objects I can see that those are cities\ndf_check[df_check['feature class']=='P'].sort_values('population', ascending=False).head(5)\n\n# administrative object located in object of level P\ndf_check[df_check['feature class']=='A'].sort_values('population', ascending=False).head(5)\n\n#z tej tabeli wynika, ze PPLX to sekcje zaludnionych miejsc, sa to ulice, dzielnice, wiec wykluczam, sa czescia miast\ndf_check[['feature class','feature code', 'short  descr']].drop_duplicates()\n\n#finalnie musze skupic sie na na obu klasach, jednoczesnie usuwajac duplikaty\ndf = df[(df['feature class'].isin(['P'])) & \n        (df.population != 0) & \n        ~(df['feature code'].isin(['PPLX']))].drop_duplicates('name')\n\n\ndf.index.name = 'city_id'\ndf.reset_index(inplace=True)\n\n\n\n\nShow the code\ndf.groupby(['feature class','feature code']).agg({'population': ['mean', 'min', 'max']})\n##                               population                  \n##                                     mean      min      max\n## feature class feature code                                \n## P             PPL           3.238169e+03        5   244969\n##               PPLA          3.652322e+05   118433   768755\n##               PPLA2         3.543081e+04     5696   226794\n##               PPLA3         5.619329e+03      110   248125\n##               PPLC          1.702139e+06  1702139  1702139\n##               PPLF          1.750000e+02      175      175"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "title": "Grow of Metropolis",
    "section": "Metropolis in Poland",
    "text": "Metropolis in Poland\nwikipedia Warszawa, Katowice, Kraków, Łódź, Trójmiasto, Poznań, Wrocław, Bydgoszcz, Szczecin, Lublin.\n\n\nShow the code\ndf_metropolie = df[df.name.isin(\n    ['Warsaw','Katowice','Kraków','Łódź',\n     'Gdańsk','Gdynia',#'Trójmiasto',\n     'Poznań','Wrocław','Bydgoszcz','Szczecin','Lublin'])][\n    ['city_id','name','population','latitude','longitude']]\ndf_metropolie['iteration']=0 \n#df_metropolie['radius'] = radius(df_metropolie['population'])\ndf_metropolie=df_metropolie.add_suffix('_metro')\ndf_metropolie\n##       city_id_metro name_metro  ...  longitude_metro  iteration_metro\n## 188            3191     Warsaw  ...         21.01178                0\n## 917           12873     Lublin  ...         22.56667                0\n## 1734          25287    Wrocław  ...         17.03333                0\n## 1916          27732   Szczecin  ...         14.55302                0\n## 2279          32047     Poznań  ...         16.92993                0\n## 2654          36976       Łódź  ...         19.47395                0\n## 2774          38634     Kraków  ...         19.93658                0\n## 2889          40299   Katowice  ...         19.02754                0\n## 3089          43232     Gdynia  ...         18.53188                0\n## 3090          43241     Gdańsk  ...         18.64912                0\n## 3298          45802  Bydgoszcz  ...         18.00762                0\n## \n## [11 rows x 6 columns]"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "title": "Grow of Metropolis",
    "section": "metropolis absorption algorithm",
    "text": "metropolis absorption algorithm\n\nInstruction\n\nstworze id kolumne z indeksem\nzlacze tabele z metropoliami i wszystkimi miastami im do tej pory przypisanymi, wylicze zagregowana ludnosc oraz promien metropoli\ncroos joinuje do kazdego miasta bez przypisanej metropolii tabele z metropolia\nwylicze odleglosc miejscowosci od metropoli i pozbede sie tych wierszy ktore sa poza promieniem\ndla pozostalych miejscowosci wylicze moc metropolii\nzrobie slice max groupujac po id miejscowosci pozostawiajc metropolie wchlaniajaca - tak powstanie tabela incrementalna do ktorej potem bede rbindowal nastepne tego typu tabele\nw obu tabelach powstanie tabele z indeksem mowiacy o n-iteracji z jakiej pochodzi przypisanie miejscowosci do metropolii oraz stan populacji\nwszystko zamkne w lupie while ktory bedzie wykonywany tak dlugo jak zostanie odnotowany przyrost w tabeli incrementalnej\n\n\n\nShow the code\ndf_cities = df[['city_id','name','population','latitude','longitude']]\ndf_cities = df_cities.loc[~df_cities.city_id.isin(df_metropolie.city_id_metro)]\ndf_cities.head(5)\n##    city_id      name  population  latitude  longitude\n## 0       13  Prędocin         536  51.14791   21.32704\n## 1       16     Poraj         266  50.89962   23.99191\n## 2       37    Żyrzyn        1400  51.49918   22.09170\n## 3       41  Żyrardów       41179  52.04880   20.44599\n## 4       42   Żyraków        1400  50.08545   21.39622\n\n\n\n\nwlasciwy algorytm\n\n\nShow the code\ndf_miasta_w_puli =df_cities\ncolumn_names = ['city_id','name','population'] +df_metropolie.columns.values.tolist()\ndf_miasta_wchloniete=pd.DataFrame(columns=column_names)\nstart = True\niteration =0\n\n\n# start funkcji\nwhile start == True:\n    df_metropolie_powiekszone=df_metropolie.append(df_miasta_wchloniete, ignore_index=True)\n    df_metropolie_powiekszone.population = df_metropolie_powiekszone.population.combine_first(df_metropolie_powiekszone.population_metro)\n    \n    df_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n        ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n        {'population':['sum']}).reset_index()\n    df_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\n    df_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n    df_miasta_w_puli['key'] = 1\n    df_metropolie_powiekszone_popul['key'] = 1\n    df_x = pd.merge(df_miasta_w_puli, df_metropolie_powiekszone_popul, on='key', suffixes=('','_y')).drop(\"key\", 1)\n    #calculating distance between two coordinates \n    #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n    distances_km = []\n    for row in df_x.itertuples():\n        distances_km.append(\n            haversine_distance_code( row.latitude, row.longitude ,row.latitude_metro, row.longitude_metro)\n        )\n    df_x['distance_km'] = distances_km\n    df_x = df_x[df_x.radius >= df_x.distance_km]\n    df_x['impact'] = _calcualate_metrocity_impact(df_x.radius,df_x.distance_km)\n    #stwierdzam do ktorej finalnie metropoli miejscowosci zostaje zaliczon\n    idx = df_x.groupby(['name','population'])['impact'].transform(max) == df_x['impact']\n    df_x = df_x[idx]\n    iteration+= 1\n    df_x['iteration_metro']=iteration\n    pre_rows_num=df_miasta_wchloniete.shape[0]\n    df_miasta_wchloniete=df_miasta_wchloniete.append(\n        df_x[column_names], ignore_index=True)\n    #pozbywam sie miast juz wchlonietych\n    indx = df_miasta_w_puli.city_id.isin(df_miasta_wchloniete.city_id)\n    df_miasta_w_puli = df_miasta_w_puli[~indx]\n    if pre_rows_num == df_miasta_wchloniete.shape[0]:\n        start = False\n## <string>:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n## <string>:10: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ndf_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n    ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n    {'population':['sum']}).reset_index()\ndf_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\ndf_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n\n\n\n\nShow the code\n#finalne populacje metropoli\ndf_metropolie_powiekszone_popul.head(5)\n\n#przypisanie miast do metropoli wraz numerem iteracji\n##    city_id_metro name_metro  ...  population     radius\n## 0           3191     Warsaw  ...     5167905  97.494601\n## 1          12873     Lublin  ...      531712  37.739146\n## 2          25287    Wrocław  ...     1038518  56.178876\n## 3          27732   Szczecin  ...      589846  40.197589\n## 4          32047     Poznań  ...     1116528  58.484992\n## \n## [5 rows x 7 columns]\ndf_miasta_wchloniete.head(5)\n##   city_id           name  ... longitude_metro iteration_metro\n## 0      41       Żyrardów  ...        21.01178               1\n## 1     189       Zręczyce  ...        19.93658               1\n## 2     215       Żoliborz  ...        21.01178               1\n## 3     291          Złota  ...        21.01178               1\n## 4     339  Zielonki-Wieś  ...        21.01178               1\n## \n## [5 rows x 9 columns]"
  },
  {
    "objectID": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "href": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "A Monte Carlo simulation is a type of computational algorithm that estimates the probability of occurrence of an undeterminable event due to the involvement of random variables. The algorithm relies on repeated random sampling in an attempt to determine the probability. This means simulating an event with random inputs a large number of times to obtain your estimation. You can determine other factors as well, and we will see that in the example. Monte Carlo simulations can be utilized in a broad range of fields spanning from economics, gambling, engineering, energy, and anything in-between. So, no matter what career field you are in, it’s an excellent thing to know about.\n#The Dice Game\nOur simple game will involve two six-sided dice. In order to win, the player needs to roll the same number on both dice. A six-sided die has six possible outcomes (1, 2, 3, 4, 5, and 6). With two dice, there is now 36 possible outcomes (1 and 1, 1 and 2, 1 and 3, etc., or 6 x 6 = 36 possibilities). In this game, the house has more opportunities to win (30 outcomes vs. the player’s 6 outcomes), meaning the house has the quite the advantage.\nLet’s say our player starts with a balance of $1,000 and is prepared to lose it all, so they bet $1 on every roll (meaning both dice are rolled) and decide to play 1,000 rolls. Because the house is so generous, they offer to payout 4 times the player’s bet when the player wins. For example, if the player wins the first roll, their balance increases by $4, and they end the round with a balance of $1,004. If they miraculously went on a 1,000 roll win-streak, they could go home with $5,000. If they lost every round, they could go home with nothing. Not a bad risk-reward ratio… or maybe it is.\n\n\nShow the code\n# Importing Packages\nimport matplotlib.pyplot as plt\nimport random\n\n\n\n\nLet’s define a function that will randomize an integer from 1 to 6 for both dice (simulating a roll). The function will also compare the two dice to see if they are the same. The function will return a Boolean variable, same_num, to store if the rolls are the same or not. We will use this value later to determine actions in our code.\n\n\nShow the code\n# Creating Roll Dice Function\ndef roll_dice():\n    die_1 = random.randint(1, 6)\n    die_2 = random.randint(1, 6)\n    # Determining if the dice are the same number\n    if die_1 == die_2:\n        same_num = True\n    else:\n        same_num = False\n    return same_num\n\n\nThese are initialized as lists and will be updated at the end of each game.\n\n\nShow the code\n# Inputs\nnum_simulations = 100\nmax_num_rolls = 1000\nbet = 1\n\n# Tracking\nwin_probability = []\nend_balance = []\n\n\n\n\nShow the code\n# Creating Figure for Simulation Balances\nfig = plt.figure()\nplt.title(\"Monte Carlo Dice Game [\" + str(num_simulations) + \"simulations]\")\nplt.xlabel(\"Roll Number\")\nplt.ylabel(\"Balance [$]\")\nplt.xlim([0, max_num_rolls])\n\n\n(0.0, 1000.0)\n\n\nOnce the number of rolls hits 1,000, we can calculate the player’s win probability as the number of wins divided by the total number of rolls. We can also store the ending balance for the completed game in the tracking variable end_balance. Finally, we can plot the num_rolls and balance variables to add a line to the figure we defined earlier.\n\n\n\n\n\nShow the code\n# For loop to run for the number of simulations desired\nfor i in range(num_simulations):\n    balance = [1000]\n    num_rolls = [0]\n    num_wins = 0    # Run until the player has rolled 1,000 times\n    while num_rolls[-1] < max_num_rolls:\n        same = roll_dice()        # Result if the dice are the same number\n        if same:\n            balance.append(balance[-1] + 4 * bet)\n            num_wins += 1\n        # Result if the dice are different numbers\n        else:\n            balance.append(balance[-1] - bet)\n        num_rolls.append(num_rolls[-1] + 1)# Store tracking variables and add line to figure\n    win_probability.append(num_wins/num_rolls[-1])\n    end_balance.append(balance[-1])\n    plt.plot(num_rolls, balance)\n\n\n\n\n\nThe last step is displaying meaningful data from our tracking variables. We can display our figure (shown below) that we created in our for loop. Also, we can calculate and display (shown below) our overall win probability and ending balance by averaging our win_probability and end_balance lists.\n\n\nShow the code\n# Averaging win probability and end balance\noverall_win_probability = sum(win_probability)/len(win_probability)\noverall_end_balance = sum(end_balance)/len(end_balance)# Displaying the averages\nprint(\"Average win probability after \" + str(num_simulations) + \"runs: \" + str(overall_win_probability))\n\n\nAverage win probability after 100runs: 0.16936999999999997\n\n\nShow the code\nprint(\"Average ending balance after \" + str(num_simulations) + \"runs: $\" + str(overall_end_balance))\n\n\nAverage ending balance after 100runs: $846.85\n\n\n\n\n\nThe most important part of any Monte Carlo simulation (or any analysis for that matter) is drawing conclusions from the results. From our figure, we can determine that the player rarely makes a profit after 1,000 rolls. In fact, the average ending balance of our 10,000 simulations is $833.66 (your results may be slightly different due to randomization). So, even though the house was “generous” in paying out 4 times our bet when the player won, the house still came out on top.\nWe also notice that our win probability is about 0.1667, or approximately 1/6. Let’s think about why that might be. Returning back to one of the earlier paragraphs, we noted that the player had 6 outcomes in which they could win. We also noted there are 36 possible rolls. Using these two numbers, we would expect that the player would win 6 out of 36 rolls, or 1/6 rolls, which matches our Monte Carlo prediction. Pretty cool!"
  },
  {
    "objectID": "posts/2022-10-29-graph-dataset/artykuł.html",
    "href": "posts/2022-10-29-graph-dataset/artykuł.html",
    "title": "DGL graph datastructure",
    "section": "",
    "text": "Very simple example below\n\n\nShow the code\n# Each value of the dictionary is a list of edge tuples.\n# Nodes are integer IDs starting from zero. Nodes IDs of different types have\n# separate countings.\nimport torch\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nimport dgl\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nratings = dgl.heterograph(\n    {('user', '+1', 'movie') : [(0, 0), (0, 1), (1, 0)],\n     ('user', '-1', 'movie') : [(2, 1)]})\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nratings\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ratings' is not defined\n\n\n\n\nIn order to save graph as object, very convenient method is to use pickle.\n\n\nShow the code\nimport pickle\n\noutput_file = 'builded_graph.pkl'\ndirectory = 'input/'\nwith open(directory + output_file, 'rb') as f:\n    g = pickle.load(f)\n\n\n#loading saved graph\nwith open(directory  + output_file, 'rb') as f:\n    sp_matrix = pickle.load(f)\n\n\nTo create a more realistic heterograph let’s use the ACM dataset. ## Graphs Dataset can be downloaded from here. It’s stored in mat (matrix) object. Within which we can find object/matrices stored in a compresed sparsed format. More about further.\n\n\nShow the code\nimport scipy.io\nimport urllib.request\n\ndata = scipy.io.loadmat(data_file_path+'ACM.mat')\n\n\n\n\n\nMany different ways to store sparsed matrices may be find in scipy documentation.\nFor us most important will be csr_matrix and csc_matrix.\nCSC format is almost identical, except that values are indexed first by column with a column-major order. Usually, the CSC is used when there are more rows than columns. On the contrary, the CSR works better for a ‘wide’ format. So, her is taking CSR as an example here.\nBelow short example how sparsed matrices can be handle with scipy package.\n\n\nShow the code\nimport numpy as np\nfrom scipy.sparse import csr_matrix, csc_matrix\narr = np.array([[0, 0, 0], [0, 0, 1], [1, 2, 0]])\n\narr_csr = csr_matrix(arr)\narr_csc = csc_matrix(arr)\n\nprint(type(arr_csr))\n## <class 'scipy.sparse.csr.csr_matrix'>\nprint(type(arr_csc))\n\n# `CSC` format is almost identical, except that values are indexed first by column with a column-major order. Usually, the `CSC` is used when there are more rows than columns. On the contrary, the `CSR` works better for a ‘wide’ format. So, her is taking CSR as an example here\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint(arr_csr)\n##   (1, 2) 1\n##   (2, 0) 1\n##   (2, 1) 2\nprint(arr_csc)\n\n# however in order to get access ti those indexes need to use method to_coo.\n##   (2, 0) 1\n##   (2, 1) 2\n##   (1, 2) 1\narr_csr.tocoo().row\n## array([1, 2, 2], dtype=int32)\narr_csr.tocoo().col\n\n#Viewing stored data (not the zero items) with the data property\n## array([2, 0, 1], dtype=int32)\nprint(arr_csr.data)\n## [1 1 2]\nprint(arr_csc.data)\n\n#Counting nonzeros with the count_nonzero() method:\n## [1 2 1]\nprint(arr_csr.count_nonzero())\n## 3\nprint(arr_csc.count_nonzero)\n## <bound method _data_matrix.count_nonzero of <3x3 sparse matrix of type '<class 'numpy.int64'>'\n##  with 3 stored elements in Compressed Sparse Column format>>\nprint(arr_csr.toarray())\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\nprint(arr_csc.todense())\n\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\n\n\n\n\n\n\n\n\n\nShow the code\nimport scipy.sparse as sp\n\nsp_matrix = data['PvsA']\nprint(type(sp_matrix))\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint('#Papers:', sp_matrix.shape[0])\n## #Papers: 12499\nprint('#Authors:',sp_matrix.shape[1])\n## #Authors: 17431\nprint('#Links:', sp_matrix.nnz)\n\n# ways of populating graph with coo_matrix\n## #Links: 37055\npp_g = dgl.bipartite_from_scipy(sp_matrix, utype='paper', etype='written-by', vtype='author')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\npp_g.is_homogeneous\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.number_of_nodes())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.number_of_edges())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.successors(3))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Node types:', pp_g.ntypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Edge types:', pp_g.etypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Canonical edge types:', pp_g.canonical_etypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\n\n\nIn order to visualize the interactions (edges) between nodes let use following function.\n\n\nShow the code\nimport pygraphviz as pgv\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pygraphviz'\ndef plot_graph(nxg, plot_name):\n    ag = pgv.AGraph(strict=False, directed=True)\n    for u, v, k in nxg.edges(keys=True):\n        ag.add_edge(u, v, label=k)\n    ag.layout('dot')\n    ag.draw(plot_name+'.png')\n\n\n\n\nShow the code\nplot_graph(nxg=pp_g.metagraph(),plot_name='simple_graph')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\n\n\n\n\n\n\nUsing ACM dataset\n\n\nShow the code\nimport torch\n\n# Unfortunately following code no longer works\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : data['PvsA'],\n        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n        ('paper', 'citing', 'paper') : data['PvsP'],\n        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n        ('paper', 'is-about', 'subject') : data['PvsL'],\n        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n    })\n\n# we need to a little bit tweak the code the get the same result as above.\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : \n          (torch.tensor(data['PvsA'].tocoo().col),torch.tensor(data['PvsA'].tocoo().row )),\n         ('author', 'writing', 'paper') : \n           (torch.tensor(data['PvsA'].tocoo().row) ,torch.tensor(data['PvsA'].tocoo().col)),\n        ('paper', 'citing', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().col),torch.tensor(data['PvsP'].tocoo().row )),\n        ('paper', 'cited', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().row) ,torch.tensor(data['PvsP'].tocoo().col)),\n        ('paper', 'is-about', 'subject') : \n          (torch.tensor(data['PvsL'].tocoo().col),torch.tensor(data['PvsL'].tocoo().row )),\n        ('subject', 'has', 'paper') : \n          (torch.tensor(data['PvsL'].tocoo().row) ,torch.tensor(data['PvsL'].tocoo().col))\n    })\n  \n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nplot_graph(nxg=G.metagraph(),plot_name='more_complicated_graph')\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'G' is not defined\n\n\n\n\n\n\n\nOn github repository we can find a method allowing for building graphs using pandas dataphrame.\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.insert(1, repo_directory)\nfrom builder import PandasGraphBuilder\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'builder'\nusers=pd.DataFrame(data=range(1,101), columns=['user_id'])\nproducts=pd.DataFrame(data=range(1,50), columns=['product_id'])\ninteractions=pd.DataFrame(data={\n  'user_id': np.random.choice(users.user_id,1000,replace=True),\n  'product_id' :np.random.choice(products.product_id,1000,replace=True)}\n  )\n\n\n\n\ngraph_builder = PandasGraphBuilder()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'PandasGraphBuilder' is not defined\ngraph_builder.add_entities(users, 'user_id', 'user')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_entities(products, 'product_id', 'product')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_binary_relations(interactions, 'user_id','product_id', 'interaction')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_binary_relations(interactions, 'product_id','user_id', 'interaction-by')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\nprint('starting graph building')\n## starting graph building\ng = graph_builder.build()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\nplot_graph(nxg=g.metagraph(),plot_name='pandas_graph')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'g' is not defined"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "List of all articles can be found below",
    "section": "",
    "text": "R lib: data.table\n\n\n\n\n\n\n\ndata.table\n\n\n\n\nR data.table\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nR_Markdown\n\n\n\n\n\n\n\nRmarkdown\n\n\n\n\nPresentation of R Markdown functionality\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nWater sensors\n\n\n\n\n\n\n\nPython\n\n\nTime Series\n\n\nanomaly detection\n\n\n\n\nPresentation of Time Series exploration, anomaly detection, and predictions techniques\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nDescribing statistical methods\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Tests\n\n\n\n\n\n\n\nUnit testing\n\n\n\n\n\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - forecast\n\n\n\n\n\n\n\nTime Series\n\n\nforecast\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - missing data imputation & Smoothing\n\n\n\n\n\n\n\nTime Series\n\n\nSmoothing\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nGrow of Metropolis\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n  \n\n\n\n\nDGL graph datastructure\n\n\n\n\n\n\n\nDGL\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Łukasz Rąbalski blog",
    "section": "",
    "text": "Hi! My name is Łukasz Rąbalski.\nI hope you will find content served here interesting and useful.\nWhat you may find here:\n- Python / R / SQL algorithm\n- Application showcases of statistical / algorithmical method aimed toward solving real life problems.\n- Explanation of mathematical foundation lying behind above mentioned methods\nEach article has attached Category and one or more tags which makes it easier to find useful content.\nThis page was developed with awesome R package blogdown 👍"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html",
    "title": "Water sensors",
    "section": "",
    "text": "##### libraries\nIn the attached file you will find data containing two water sensors: water level and water velocity.\nColumn named “level” refers to raw level data and column named “velocity” refers to raw velocity data. Columns “final_level” and “final_velocity” refer to data that was manually corrected or removed because of malfunctions of sensors or other reasons. You have three tasks here:"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#missing-data",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#missing-data",
    "title": "Water sensors",
    "section": "1.1 missing data",
    "text": "1.1 missing data\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport math \ndf = pd.read_csv('/Users/lrabalski1/Desktop/prv/data/water_sensors.csv', parse_dates=[\"time\"])\nprint('min time value: ',df.time.min(),'\\n max time value: ',df.time.max())\n## min time value:  2019-01-01 00:00:00 \n##  max time value:  2021-05-31 23:55:00\ndf=pd.DataFrame(\n  pd.date_range('2019-01-01 00:00:00','2021-05-31 23:55:00',freq='5T'),\n  columns=['time']\n  ).merge(df, how='left', on=['time'])\ndf.index=df.time\n\n\nprint('NA within level_final: ',sum(df.level_final.isna()),'\\nNA within velocity_final: ', sum(df.velocity_final.isna()))\n\n## NA within level_final:  13725 \n## NA within velocity_final:  13725\ndf['level_data_malfunction']=(df.level.isna()) | (df.level!=df.level_final)\ndf['velocity_data_malfunction']=(df.velocity.isna()) | (df.velocity!=df.velocity_final)\n\n\n# create a list of our conditions\nconditions_level = [\n    (df['level_data_malfunction'] ==False),\n    (df['level_data_malfunction'] ==True) & ( df['level_final'].isna()),\n    (df['level_data_malfunction'] ==True) & ( ~df['level_final'].isna())\n    ]\nvalues_level = [np.nan,'sensors_malfunction','manual_correction']\nconditions_velocity = [\n    (df['velocity_data_malfunction'] ==False),\n    (df['velocity_data_malfunction'] ==True) & ( df['velocity_final'].isna()),\n    (df['velocity_data_malfunction'] ==True) & ( ~df['velocity_final'].isna())\n    ]\nvalues_velocity = [np.nan,'sensors_malfunction','manual_correction']\ndf['velocity_reason_of_correction'] = np.select(conditions_velocity, values_velocity)\ndf['level_reason_of_correction'] = np.select(conditions_level, values_level)\nprint('\\n\\n')\n\n\nMissing data in both variable (velocity and level), which can be explained as sensors malfunction, is observed in 13725 data points. For both variable missing data appears in the same datapoint. Probably sensors are responsible for measuring both measures.\nAdditionally in case of level measure we can observe 204 datapoint where data were manually corrected.\n\n\nShow the code\nprint(pd.crosstab(df['velocity_reason_of_correction'],df['level_reason_of_correction']))\n## level_reason_of_correction     manual_correction     nan  sensors_malfunction\n## velocity_reason_of_correction                                                \n## nan                                          204  240087                    0\n## sensors_malfunction                            0       0                13725"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#patterns",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#patterns",
    "title": "Water sensors",
    "section": "1.2 patterns",
    "text": "1.2 patterns\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\ndf['time'] = pd.to_datetime(df['time'])\ndf_t=df.copy()\ndf_t.index = df_t['time']\ndf_t=df_t.drop(['time','level_data_malfunction', 'velocity_data_malfunction','velocity_reason_of_correction', 'level_reason_of_correction'], axis=1)\n\ncheck=df_t.sort_values('level_final', ascending=False)\n\n\nFrom the chart it can bee seen that:\n\nthere is rather no trend within each timeseries\nin level data one big anomaly can be observed in the middle of a period\n\n\n\nShow the code\n\n# downsampling\ndf_M = df_t.resample(\"D\").mean()\n\nstyles1 = ['b-','r-']\nplt.clf()\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True)\ndf_M[['level_final']].plot(ax=axes[0],style=styles1)\ndf_M[['velocity_final']].plot(ax=axes[1],style=styles1)\nplt.show()\n\n\n\n\n\nShow the code\ncheck=df_M.sort_values('level_final', ascending=False)\n\n\n\n1.2.1 stationarity\nAs it can be seen on the both plots out time series seeme to be stationary, rolling mean and SD is stable within time.\n\n\nShow the code\n\nrolling_mean = df_M.rolling(7).mean()\nrolling_std = df_M.rolling(7).std()\n\n\n#f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 1, 1) \n\nplt.plot(df_M['level_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['level_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['level_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Level Data\")\nplt.xticks([])\n## ([], [])\nplt.legend(loc=\"best\")\n\nfig.add_subplot(2, 1, 2) \nplt.plot(df_M['velocity_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['velocity_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['velocity_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Velocity Data\")\nplt.legend(loc=\"best\")\nplt.xticks([])\n## ([], [])\nplt.show()\n\n\n\n\n\nStiationarity of a data is support by the fact that p-value of Dickey–Fuller test is lower than 5 percent (null hypothesis it claiming that time series is non-stationary) and the test statistic is lower than the critical value. We can also draw these conclusions from inspecting the data.\n\n\nShow the code\nfrom statsmodels.tsa.stattools import adfuller\nadft_velocity = adfuller(df_M.dropna(subset=['velocity_final'])['velocity_final'],autolag=\"AIC\")\nadft_level = adfuller(df_M.dropna(subset=['level_final'])['level_final'],autolag=\"AIC\")\n\nadft=adft_level\noutput_level = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nadft=adft_velocity\noutput_velocity = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nresults = pd.DataFrame(output_velocity.join(output_level, lsuffix='_velocity', rsuffix='_level'))\nresults\n##    Values_velocity  ...                 Metric_level\n## 0        -4.758844  ...              Test Statistics\n## 1         0.000065  ...                      p-value\n## 2         8.000000  ...             No. of lags used\n## 3       841.000000  ...  Number of observations used\n## 4        -3.438149  ...          critical value (1%)\n## 5        -2.864983  ...          critical value (5%)\n## 6        -2.568603  ...         critical value (10%)\n## \n## [7 rows x 4 columns]\n\n\n\n\nShow the code\ntabela(py$results)\n\n\n\n\n\n\n\n\n\n1.2.2 autocorelation\nWe can see that both variable are weakly autocorelated, however velocity data better. Reason for bad autocorelation in level data measured this way, may have fact of big anomaly within dataset.\n\n\nShow the code\ndef my_function(df,col, array):\n  dicts = {}\n  for idx, x in enumerate(array):\n    dicts[x] = df[col].autocorr(x)\n  return dicts\nprint('autocorelation of level data :')\n## autocorelation of level data :\nmy_function(df_M,'level_final',array=[1,7,14,30,365])\n## {1: 0.6187899970924978, 7: 0.16030304961078023, 14: 0.1660215318228444, 30: 0.08793288098871045, 365: 0.016746891095633046}\nprint('\\n autocorelation of velocity data :')\n## \n##  autocorelation of velocity data :\nmy_function(df_M,'velocity_final',array=[1,7,14,30,365])\n## {1: 0.822156599237431, 7: 0.4759130585597128, 14: 0.3678199431634758, 30: 0.15582363183139622, 365: -0.11352389683675475}\n\n\n\n\n1.2.3 data imputation\nI will choose best method based on longest period upon which I have full data.\nMissing data is spread along all time range.\n\n\nShow the code\n\ndf_na = df_t.copy()\ndf_na['time']=df_na.index\ndf_na = df_na.reset_index(drop=True)\ndf_na['date']=df_na['time'].dt.date\ndf_na['na_velocity_final'] = df_na.velocity_final.isna()\ndf_na['na_level_final'] = df_na.level_final.isna()\ndates_with_na = df_na.groupby('date')[['na_level_final','na_velocity_final']].sum().reset_index()\ndates_with_na['any_na'] = dates_with_na.sum(axis=1)\n## <string>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\ndates_with_na[dates_with_na.any_na > 0]\n##            date  na_level_final  na_velocity_final  any_na\n## 116  2019-04-27              58                 58     116\n## 117  2019-04-28             119                119     238\n## 129  2019-05-10              64                 64     128\n## 142  2019-05-23              38                 38      76\n## 163  2019-06-13               4                  4       8\n## ..          ...             ...                ...     ...\n## 854  2021-05-04               1                  1       2\n## 855  2021-05-05               3                  3       6\n## 856  2021-05-06               2                  2       4\n## 869  2021-05-19               9                  9      18\n## 870  2021-05-20               9                  9      18\n## \n## [105 rows x 4 columns]\nplt.clf()\nfig = plt.figure()\nplt.plot(dates_with_na.date, dates_with_na.any_na)\n\nplt.show()\n\n\n\n\n\nThe longest chain of missing data is of 5476 consecutive data points (5 seconds * 5476 = 7.6 hours) .\n\n\nShow the code\nlongest_consecutive_na_chain = (~df_na['na_level_final']).cumsum().value_counts()\nlongest_consecutive_na_chain = longest_consecutive_na_chain[longest_consecutive_na_chain!=1]-1\n\n\ndf_na.index = df_na['time']\nlongest_concecutive_full_data_chain  = (df_na['na_level_final']).cumsum()\nlongest_concecutive_full_data_chain_counts = longest_concecutive_full_data_chain.value_counts()\n\nlongest_concecutive_full_data_chain_counts=longest_concecutive_full_data_chain_counts[longest_concecutive_full_data_chain_counts!=1]-1\n\nselected_chain = longest_concecutive_full_data_chain[longest_concecutive_full_data_chain==longest_concecutive_full_data_chain_counts.index[0]]\n\n\nAny automatic technic od missing data imputation like:\n\nMean, median and mode imputation\nForward and backward filling\nlinear / nearest imputation\n\ncould behave bad in such a kind o missing pattern (5476 consecutive data points is missing)\nThats why I will prepare two different imputation schema, check their accuracy for given dataset and choose better one:\n\nMethod 1: fill the missing data with the averaged value of given second of year. For seconds from january to may i will have maximum 3 data points (2019;2020;2021); for rest maximum of 2 data points.\nMethod 2:fill data with previous day data point with similar daytime\n\n\n\nShow the code\n\n\n# preparing dataset for Method 1\ndf_fill = df_t.copy()\ndf_fill['time']=df_fill.index\n#df_fill = df_fill.reset_index(drop=True)\ndf_fill['time_agg'] = pd.DatetimeIndex(df_fill.time).strftime(\"%m-%d %H:%M:%S\")\ndf_time_agg = df_fill.groupby('time_agg').agg(\n  velocity_final_mean=('velocity_final','mean'),\n  level_final_mean=('level_final','mean'),\n  velocity_mean=('velocity','mean'),\n  level_mean=('level','mean')\n).reset_index()\n\n\n#'after agregation to day of year I still have 54 missing value, which mostly come from one chain of missing data. I will replace them with data from previous day.')\n#df_time_agg.isna().sum()\n\ndf_time_agg = df_time_agg.join(df_time_agg.shift(24*12), rsuffix='_lag_1_day')\ndf_time_agg.velocity_final_mean = df_time_agg.velocity_final_mean.fillna(df_time_agg.velocity_final_mean_lag_1_day)\ndf_time_agg.velocity_mean = df_time_agg.velocity_mean.fillna(df_time_agg.velocity_mean_lag_1_day)\ndf_time_agg.level_final_mean = df_time_agg.level_final_mean.fillna(df_time_agg.level_final_mean_lag_1_day)\ndf_time_agg.level_mean = df_time_agg.level_mean.fillna(df_time_agg.level_mean_lag_1_day)\ndf_time_agg = df_time_agg[df_time_agg.columns.drop(list(df_time_agg.filter(regex='_lag_1_day')))]\n\n\n# testing two method\n## preparing test data\n\ndf_test = df_fill.loc[selected_chain.index].copy()\ndf_test['velocity_final_missing'] = df_test.velocity_final\ndf_test['level_final_missing'] = df_test.level_final\nrng = np.random.RandomState(42)\nrandom_indices = rng.choice(range(len(df_test)), size=round(len(df_test)/50))\ndf_test = df_test.reset_index(drop=True)\ndf_test.loc[random_indices, ['velocity_final_missing','level_final_missing']] = np.nan\n\n\n## fillig missing data\n### time_agg method - Method 1\ndf_test = df_test.merge(df_time_agg, how='left', on =['time_agg'])\ndf_test.index = df_test['time']\ndf_test['velocity_final_fill_method_1'] = df_test.velocity_final_missing.fillna(df_test.velocity_final_mean)\ndf_test['level_final_fill_method_1'] = df_test.level_final_missing.fillna(df_test.level_final_mean)\n\n### laged value - Method 2\ncols= ['velocity_final_fill_method_2','level_final_fill_method_2']\ntime_shifts = np.array([-24*12,-24*12+1,-24*12-1,-24*12+2,-24*12-2,-24*12+3,-24*12-3,-24*12*2,-24*12*2+1,-24*12*2-1,-24*12*2+2,-24*12*2+2,-24*12*2+3,-24*12*2-3])\n\ndf_test['velocity_final_fill_method_2'] = df_test.velocity_final_missing\ndf_test['level_final_fill_method_2'] = df_test.level_final_missing\n\ni=-1\nwhile df_test[['velocity_final_fill_method_2','level_final_fill_method_2']].isna().sum().sum() >0:\n  i=+1\n  time_shifts = [x+1 for x in time_shifts]\n  for col,time_shift in  ((x, y) for x in cols for y in time_shifts):\n    #print (col,time_shift)\n    df_test[col] = df_test[col].fillna(df_test[col].shift(time_shift))\n\n\n\n\n\n\n# testing difference\nlevel_dist_1 = abs(df_test.level_final_fill_method_2-df_test.level_final)\nlevel_dist_2 = abs(df_test.level_final_fill_method_1-df_test.level_final)\n\nvelocity_dist_1 = abs(df_test.velocity_final_fill_method_2-df_test.velocity_final)\nvelocity_dist_2 = abs(df_test.velocity_final_fill_method_1-df_test.velocity_final)\n\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 2, 1)\n\nplt.plot(level_dist_1, color=\"red\", label=\"Level - Method 1\")\nplt.plot(level_dist_2, color=\"black\", label = \"Level - Method 2\")\n#plt.title(\"Absolute Error for level missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 2)\nplt.bar(['Level - method 1','Level - method 2'], [level_dist_1.sum(),level_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 3)\nplt.plot(velocity_dist_1, color=\"red\", label=\"Velocity - Method 1\")\nplt.plot(velocity_dist_2, color=\"black\", label = \"Velocity - Method 2\")\n#plt.title(\"Absolute Error for velocity missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 4)\nplt.bar(['Velocity - method 1','Velocity - method 2'], [velocity_dist_1.sum(),velocity_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n1.2.4 periodic decomposition\nBoth dataset show quite strong daily seasonality (level higher then velocity) Both dataset show weak weekly seasonality (again level higher then velocity) There is no observable trend in both variables.\n\n\nShow the code\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\n\n\n# daily seasonality\ndf_D = df_fill_final\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','velocity_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n\n\n\n\n\nShow the code\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','level_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n\n# weakly seasonality\n\n\n\n\n\nShow the code\ndf_D = df_fill_final.resample(\"D\").mean()\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','velocity_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()\n\n\n\n\n\nShow the code\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','level_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#simple-solution",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#simple-solution",
    "title": "Water sensors",
    "section": "2.1 simple solution",
    "text": "2.1 simple solution\n\nHaving the knowledge of what was corrected … prepare simple solution to automatically find suspicious or corrupted data\n\nBelow I present two periods within which data correction of level variable took place. It shows that there is no more then 600 level value, while manuall correction reported that there were higher values to observe. Probably sensors are not able to make measurement above this level.\nAccording to this my suggestion would be:\n\n🚨 when the measurement of level will hit 600 sound the alarm 🚨\n\nThe data correction process which took place within 2019-06-13 was substle in size. Hard to be understooded without any extra information.\n\n\nShow the code\na = df.loc[df['level_reason_of_correction'] == 'manual_correction', ['level']] #anomaly\n\n\nsel_dates = (df.index>a.index.min()) & (df.index < a.index.max())\nsel_dates_1 = np.isin(df.index.date,list(set(a.index.date))[0:2])\nsel_dates_2 = np.isin(df.index.date,list(set(a.index.date))[2:])\n\na_1 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_1==True), ['level']] #anomaly\n\n\na_2 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_2==True), ['level']] #anomaly\n\ncheck = df.loc[sel_dates_2,['level','level_final']]\n\nplt.clf()\nfig = plt.figure()\n\nfig.add_subplot(2, 1, 1) \nplt.plot( df.loc[sel_dates_1,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_1,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_1.index,a_1['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18078., 18109., 18140., 18170., 18201., 18231., 18262.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\n\nfig.add_subplot(2, 1, 2) \nplt.plot( df.loc[sel_dates_2,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_2,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_2.index,a_2['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18272.   , 18272.125, 18272.25 , 18272.375, 18272.5  , 18272.625,\n##        18272.75 , 18272.875, 18273.   ]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\nfig.tight_layout()\nplt.show();"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#statistical-method",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#statistical-method",
    "title": "Water sensors",
    "section": "2.2 Statistical method",
    "text": "2.2 Statistical method\n\nHaving the knowledge of statistical methods prepare simple solution to automatically find suspicious or corrupted data.\n\nIn order to detect anomalies in dataset I used IsolationForest algorithm which identifies anomalies by isolating outliers in the data.\nThe contamination parameter defines a rough estimate of the percentage of the outliers in our dataset. Based on what i know about size of manual correction within my dataset I assigned contamination to be 0.1% in our case.\n\n\nShow the code\nrandom_state = np.random.RandomState(42)\noutliers_fraction = float(round(np.mean(df['level_reason_of_correction']=='manual_correction'),ndigits=3))\n\n\nscaler = StandardScaler()\nnp_scaled_velocity = scaler.fit_transform(df_fill_final.velocity_fill.values.reshape(-1, 1))\nnp_scaled_level = scaler.fit_transform(df_fill_final.level_fill.values.reshape(-1, 1))\n\ndata_velocity = pd.DataFrame(np_scaled_velocity)\ndata_level = pd.DataFrame(np_scaled_level)\n\n#train isolation forest\nmodel_velocity =  IsolationForest(contamination=outliers_fraction+0.01,random_state=random_state)\nmodel_level =  IsolationForest(contamination=outliers_fraction,random_state=random_state)\n\nmodel_velocity.fit(data_velocity)\n## IsolationForest(contamination=0.011,\n##                 random_state=RandomState(MT19937) at 0x16A366940)\nmodel_level.fit(data_level)\n## IsolationForest(contamination=0.001,\n##                 random_state=RandomState(MT19937) at 0x16A366940)\ndf_fill_final['velocity_anomaly'] = model_velocity.predict(data_velocity)\ndf_fill_final['level_anomaly'] = model_level.predict(data_level)\n\n\n# checking correctness\ndf.index = df['time']\ndf_fill_final = df_fill_final.join(df['level_reason_of_correction'])\n\n\n# visualization\n\na = df_fill_final.loc[df_fill_final['velocity_anomaly'] == -1, ['velocity_fill']] #anomaly\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(a.index,a['velocity_fill'], color='red', label = 'Anomaly')\nplt.scatter(df_fill_final.index, df_fill_final['velocity_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.legend()\nplt.show();\n\n\n\n\n\nShow the code\na = df_fill_final.loc[(df_fill_final['level_anomaly'] == -1), ['level_fill']] #anomaly\na_true = df_fill_final.loc[(df_fill_final['level_reason_of_correction'] == 'manual_correction') , ['level_fill']] #anomaly\n\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(df_fill_final.index, df_fill_final['level_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.scatter(a.index,a['level_fill'], color='red', label = 'Anomaly_IsolationForest',alpha=0.5, marker='*')\nplt.scatter(a_true.index,a_true['level_fill'], color='blue', label = 'bad data',alpha=0.5,marker='o',s=20)\nplt.legend()\nplt.show();\n\n\n\n\n\nShow the code\ndf_fill_final['anomaly_detected'] = (df_fill_final['level_anomaly'] == -1)\ndf_fill_final['anomaly_true'] = df_fill_final['level_reason_of_correction'] == 'manual_correction'\n\n\nIsolationForest managed to detect all manually coreccted data and additionally assign this label to 50 extra data points.\n\n\nShow the code\nprint(pd.crosstab(df_fill_final['anomaly_detected'] ,df_fill_final['anomaly_true'] ))\n## anomaly_true       False  True\n## anomaly_detected              \n## False             253762     0\n## True                  50   204"
  }
]