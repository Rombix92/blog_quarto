[
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html",
    "title": "Grow of Metropolis",
    "section": "",
    "text": "Aims:\n\nFIlter out the data localisation where people are supposed to live in (cities; villages etc)\nSelect cities which are metropolis based on population on external source\nWrite a script which will ascribe metropolis to a city. Additionally model a process of consecutive cities which would be ascribed to the city bearing in mind that city will grow thanks to this process.\n\nData Source\nDocumentation"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "title": "Grow of Metropolis",
    "section": "Preparing environment",
    "text": "Preparing environment\nSetting default markdown option responsible of code chunk behaviour.\nFirstly I choose prefered python environment on which I have installed useful libraries.\n\n\nShow the code\nlibrary(reticulate)\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\")\nreticulate::py_config()\n## python:         /Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\n## libpython:      /Users/lrabalski1/miniforge3/envs/everyday_use/lib/libpython3.8.dylib\n## pythonhome:     /Users/lrabalski1/miniforge3/envs/everyday_use:/Users/lrabalski1/miniforge3/envs/everyday_use\n## version:        3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17)  [Clang 11.1.0 ]\n## numpy:          /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/numpy\n## numpy_version:  1.21.4\n## \n## NOTE: Python version was forced by RETICULATE_PYTHON\n\nmyenvs=conda_list()\nenvname=myenvs$name[4]\nuse_condaenv(envname, required = TRUE)\n\n\nBellow I present two function:\n\nradius - function which based on city population is calculating a radius within which city is able to absorb cities from this range\n_calcualate_metrocity_impact - calculate impact on metrocity on given city\n\n\n\nShow the code\ndef radius(population):\n    METRO_CITY_POPULATION_CONSTANT = -1/1443000\n    MIN_METRO_CITY_RADIUS = 10\n    MAX_METRO_CITY_RADIUS = 100 - MIN_METRO_CITY_RADIUS\n    return MIN_METRO_CITY_RADIUS + MAX_METRO_CITY_RADIUS * (1 - np.exp(METRO_CITY_POPULATION_CONSTANT *  population))\n\ndef _calcualate_metrocity_impact(max_radius, distance_to_metro_city):\n    METRO_CITY_POWER_CONSTANT = -1.4\n    impact = np.exp(METRO_CITY_POWER_CONSTANT  * distance_to_metro_city / max_radius)\n    return impact\n\n\nFunction responsible for calculating distances between 2 points on earth surface.\n\n\nShow the code\n#https://towardsdatascience.com/heres-how-to-calculate-distance-between-2-geolocations-in-python-93ecab5bbba4\ndef haversine_distance_code(lat1, lon1, lat2, lon2):\n    r = 6371\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2 - lat1)\n    delta_lambda = np.radians(lon2 - lon1)\n    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n    return np.round(res, 2)\n\n\n\n\nShow the code\ndf= pd.read_csv(file_path_name, sep=\"\\t\", \n                names=['geonameid','name','asciiname','alternatenames','latitude','longitude','feature class','feature code','country code','cc2','admin1 code','admin2 code','admin3 code','admin4 code','population','elevation','dem','timezone','modification date',])\n\n\nDataset readme states that column feature classes contains level information of:\nA: country, state, region,…\nH: stream, lake, …\nL: parks,area, …\nP: city, village,…\nR: road, railroad\nS: spot, building, farm\nT: mountain,hill,rock,…\nU: undersea\nV: forest,heath,…\nWe will be interested on object of level P, and maybe A.\n\n\nShow the code\nimport requests\n\nurl = 'http://www.geonames.org/export/codes.html'\nhtml = requests.get(url).content\ndf_list = pd.read_html(html)\ndf_legend = df_list[-1]\ndf_legend = df_legend.rename(columns={df_legend.columns[0]: 'feature code',\n                                     df_legend.columns[1]: 'short  descr',\n                                     df_legend.columns[2]: 'long descr'})\ndf_legend = pd.merge(df[['feature code','feature class']].drop_duplicates(),df_legend, on='feature code')\ndf_legend\n##     feature code  ...                                         long descr\n## 0           PPLQ  ...                                                NaN\n## 1            STM  ...  a body of running water moving to a lower leve...\n## 2           HLLS  ...  rounded elevations of limited extent rising ab...\n## 3            CNL  ...                          an artificial watercourse\n## 4            PPL  ...  a city, town, village, or other agglomeration ...\n## ..           ...  ...                                                ...\n## 186          BCN  ...                 a fixed artificial navigation mark\n## 187         HSEC  ...  a large house, mansion, or chateau, on a large...\n## 188          RES  ...  a tract of public land reserved for future use...\n## 189         STNR  ...  a facility for producing and transmitting info...\n## 190         BLDA  ...  a building containing several individual apart...\n## \n## [191 rows x 4 columns]\n\n\n\n\nShow the code\ndf = df[df['feature class'].isin(['P','A'])]\ndf_check = pd.merge(df,df_legend, on=['feature code','feature class'])\n\n# sorting by the biggest objects I can see that those are cities\ndf_check[df_check['feature class']=='P'].sort_values('population', ascending=False).head(5)\n\n# administrative object located in object of level P\ndf_check[df_check['feature class']=='A'].sort_values('population', ascending=False).head(5)\n\n#z tej tabeli wynika, ze PPLX to sekcje zaludnionych miejsc, sa to ulice, dzielnice, wiec wykluczam, sa czescia miast\ndf_check[['feature class','feature code', 'short  descr']].drop_duplicates()\n\n#finalnie musze skupic sie na na obu klasach, jednoczesnie usuwajac duplikaty\ndf = df[(df['feature class'].isin(['P'])) & \n        (df.population != 0) & \n        ~(df['feature code'].isin(['PPLX']))].drop_duplicates('name')\n\n\ndf.index.name = 'city_id'\ndf.reset_index(inplace=True)\n\n\n\n\nShow the code\ndf.groupby(['feature class','feature code']).agg({'population': ['mean', 'min', 'max']})\n##                               population                  \n##                                     mean      min      max\n## feature class feature code                                \n## P             PPL           3.238169e+03        5   244969\n##               PPLA          3.652322e+05   118433   768755\n##               PPLA2         3.543081e+04     5696   226794\n##               PPLA3         5.619329e+03      110   248125\n##               PPLC          1.702139e+06  1702139  1702139\n##               PPLF          1.750000e+02      175      175"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "title": "Grow of Metropolis",
    "section": "Metropolis in Poland",
    "text": "Metropolis in Poland\nwikipedia Warszawa, Katowice, Kraków, Łódź, Trójmiasto, Poznań, Wrocław, Bydgoszcz, Szczecin, Lublin.\n\n\nShow the code\ndf_metropolie = df[df.name.isin(\n    ['Warsaw','Katowice','Kraków','Łódź',\n     'Gdańsk','Gdynia',#'Trójmiasto',\n     'Poznań','Wrocław','Bydgoszcz','Szczecin','Lublin'])][\n    ['city_id','name','population','latitude','longitude']]\ndf_metropolie['iteration']=0 \n#df_metropolie['radius'] = radius(df_metropolie['population'])\ndf_metropolie=df_metropolie.add_suffix('_metro')\ndf_metropolie\n##       city_id_metro name_metro  ...  longitude_metro  iteration_metro\n## 188            3191     Warsaw  ...         21.01178                0\n## 917           12873     Lublin  ...         22.56667                0\n## 1734          25287    Wrocław  ...         17.03333                0\n## 1916          27732   Szczecin  ...         14.55302                0\n## 2279          32047     Poznań  ...         16.92993                0\n## 2654          36976       Łódź  ...         19.47395                0\n## 2774          38634     Kraków  ...         19.93658                0\n## 2889          40299   Katowice  ...         19.02754                0\n## 3089          43232     Gdynia  ...         18.53188                0\n## 3090          43241     Gdańsk  ...         18.64912                0\n## 3298          45802  Bydgoszcz  ...         18.00762                0\n## \n## [11 rows x 6 columns]"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "title": "Grow of Metropolis",
    "section": "metropolis absorption algorithm",
    "text": "metropolis absorption algorithm\n\nInstruction\n\nstworze id kolumne z indeksem\nzlacze tabele z metropoliami i wszystkimi miastami im do tej pory przypisanymi, wylicze zagregowana ludnosc oraz promien metropoli\ncroos joinuje do kazdego miasta bez przypisanej metropolii tabele z metropolia\nwylicze odleglosc miejscowosci od metropoli i pozbede sie tych wierszy ktore sa poza promieniem\ndla pozostalych miejscowosci wylicze moc metropolii\nzrobie slice max groupujac po id miejscowosci pozostawiajc metropolie wchlaniajaca - tak powstanie tabela incrementalna do ktorej potem bede rbindowal nastepne tego typu tabele\nw obu tabelach powstanie tabele z indeksem mowiacy o n-iteracji z jakiej pochodzi przypisanie miejscowosci do metropolii oraz stan populacji\nwszystko zamkne w lupie while ktory bedzie wykonywany tak dlugo jak zostanie odnotowany przyrost w tabeli incrementalnej\n\n\n\nShow the code\ndf_cities = df[['city_id','name','population','latitude','longitude']]\ndf_cities = df_cities.loc[~df_cities.city_id.isin(df_metropolie.city_id_metro)]\ndf_cities.head(5)\n##    city_id      name  population  latitude  longitude\n## 0       13  Prędocin         536  51.14791   21.32704\n## 1       16     Poraj         266  50.89962   23.99191\n## 2       37    Żyrzyn        1400  51.49918   22.09170\n## 3       41  Żyrardów       41179  52.04880   20.44599\n## 4       42   Żyraków        1400  50.08545   21.39622\n\n\n\n\nwlasciwy algorytm\n\n\nShow the code\ndf_miasta_w_puli =df_cities\ncolumn_names = ['city_id','name','population'] +df_metropolie.columns.values.tolist()\ndf_miasta_wchloniete=pd.DataFrame(columns=column_names)\nstart = True\niteration =0\n\n\n# start funkcji\nwhile start == True:\n    df_metropolie_powiekszone=df_metropolie.append(df_miasta_wchloniete, ignore_index=True)\n    df_metropolie_powiekszone.population = df_metropolie_powiekszone.population.combine_first(df_metropolie_powiekszone.population_metro)\n    \n    df_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n        ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n        {'population':['sum']}).reset_index()\n    df_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\n    df_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n    df_miasta_w_puli['key'] = 1\n    df_metropolie_powiekszone_popul['key'] = 1\n    df_x = pd.merge(df_miasta_w_puli, df_metropolie_powiekszone_popul, on='key', suffixes=('','_y')).drop(\"key\", 1)\n    #calculating distance between two coordinates \n    #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n    distances_km = []\n    for row in df_x.itertuples():\n        distances_km.append(\n            haversine_distance_code( row.latitude, row.longitude ,row.latitude_metro, row.longitude_metro)\n        )\n    df_x['distance_km'] = distances_km\n    df_x = df_x[df_x.radius >= df_x.distance_km]\n    df_x['impact'] = _calcualate_metrocity_impact(df_x.radius,df_x.distance_km)\n    #stwierdzam do ktorej finalnie metropoli miejscowosci zostaje zaliczon\n    idx = df_x.groupby(['name','population'])['impact'].transform(max) == df_x['impact']\n    df_x = df_x[idx]\n    iteration+= 1\n    df_x['iteration_metro']=iteration\n    pre_rows_num=df_miasta_wchloniete.shape[0]\n    df_miasta_wchloniete=df_miasta_wchloniete.append(\n        df_x[column_names], ignore_index=True)\n    #pozbywam sie miast juz wchlonietych\n    indx = df_miasta_w_puli.city_id.isin(df_miasta_wchloniete.city_id)\n    df_miasta_w_puli = df_miasta_w_puli[~indx]\n    if pre_rows_num == df_miasta_wchloniete.shape[0]:\n        start = False\n## <string>:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n## <string>:10: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ndf_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n    ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n    {'population':['sum']}).reset_index()\ndf_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\ndf_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n\n\n\n\nShow the code\n#finalne populacje metropoli\ndf_metropolie_powiekszone_popul.head(5)\n\n#przypisanie miast do metropoli wraz numerem iteracji\n##    city_id_metro name_metro  ...  population     radius\n## 0           3191     Warsaw  ...     5167905  97.494601\n## 1          12873     Lublin  ...      531712  37.739146\n## 2          25287    Wrocław  ...     1038518  56.178876\n## 3          27732   Szczecin  ...      589846  40.197589\n## 4          32047     Poznań  ...     1116528  58.484992\n## \n## [5 rows x 7 columns]\ndf_miasta_wchloniete.head(5)\n##   city_id           name  ... longitude_metro iteration_metro\n## 0      41       Żyrardów  ...        21.01178               1\n## 1     189       Zręczyce  ...        19.93658               1\n## 2     215       Żoliborz  ...        21.01178               1\n## 3     291          Złota  ...        21.01178               1\n## 4     339  Zielonki-Wieś  ...        21.01178               1\n## \n## [5 rows x 9 columns]"
  },
  {
    "objectID": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "href": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "A Monte Carlo simulation is a type of computational algorithm that estimates the probability of occurrence of an undeterminable event due to the involvement of random variables. The algorithm relies on repeated random sampling in an attempt to determine the probability. This means simulating an event with random inputs a large number of times to obtain your estimation. You can determine other factors as well, and we will see that in the example. Monte Carlo simulations can be utilized in a broad range of fields spanning from economics, gambling, engineering, energy, and anything in-between. So, no matter what career field you are in, it’s an excellent thing to know about.\n#The Dice Game\nOur simple game will involve two six-sided dice. In order to win, the player needs to roll the same number on both dice. A six-sided die has six possible outcomes (1, 2, 3, 4, 5, and 6). With two dice, there is now 36 possible outcomes (1 and 1, 1 and 2, 1 and 3, etc., or 6 x 6 = 36 possibilities). In this game, the house has more opportunities to win (30 outcomes vs. the player’s 6 outcomes), meaning the house has the quite the advantage.\nLet’s say our player starts with a balance of $1,000 and is prepared to lose it all, so they bet $1 on every roll (meaning both dice are rolled) and decide to play 1,000 rolls. Because the house is so generous, they offer to payout 4 times the player’s bet when the player wins. For example, if the player wins the first roll, their balance increases by $4, and they end the round with a balance of $1,004. If they miraculously went on a 1,000 roll win-streak, they could go home with $5,000. If they lost every round, they could go home with nothing. Not a bad risk-reward ratio… or maybe it is.\n\n\nShow the code\n# Importing Packages\nimport matplotlib.pyplot as plt\nimport random\n\n\n\n\nLet’s define a function that will randomize an integer from 1 to 6 for both dice (simulating a roll). The function will also compare the two dice to see if they are the same. The function will return a Boolean variable, same_num, to store if the rolls are the same or not. We will use this value later to determine actions in our code.\n\n\nShow the code\n# Creating Roll Dice Function\ndef roll_dice():\n    die_1 = random.randint(1, 6)\n    die_2 = random.randint(1, 6)\n    # Determining if the dice are the same number\n    if die_1 == die_2:\n        same_num = True\n    else:\n        same_num = False\n    return same_num\n\n\nThese are initialized as lists and will be updated at the end of each game.\n\n\nShow the code\n# Inputs\nnum_simulations = 100\nmax_num_rolls = 1000\nbet = 1\n\n# Tracking\nwin_probability = []\nend_balance = []\n\n\n\n\nShow the code\n# Creating Figure for Simulation Balances\nfig = plt.figure()\nplt.title(\"Monte Carlo Dice Game [\" + str(num_simulations) + \"simulations]\")\nplt.xlabel(\"Roll Number\")\nplt.ylabel(\"Balance [$]\")\nplt.xlim([0, max_num_rolls])\n\n\n(0.0, 1000.0)\n\n\nOnce the number of rolls hits 1,000, we can calculate the player’s win probability as the number of wins divided by the total number of rolls. We can also store the ending balance for the completed game in the tracking variable end_balance. Finally, we can plot the num_rolls and balance variables to add a line to the figure we defined earlier.\n\n\n\n\n\nShow the code\n# For loop to run for the number of simulations desired\nfor i in range(num_simulations):\n    balance = [1000]\n    num_rolls = [0]\n    num_wins = 0    # Run until the player has rolled 1,000 times\n    while num_rolls[-1] < max_num_rolls:\n        same = roll_dice()        # Result if the dice are the same number\n        if same:\n            balance.append(balance[-1] + 4 * bet)\n            num_wins += 1\n        # Result if the dice are different numbers\n        else:\n            balance.append(balance[-1] - bet)\n        num_rolls.append(num_rolls[-1] + 1)# Store tracking variables and add line to figure\n    win_probability.append(num_wins/num_rolls[-1])\n    end_balance.append(balance[-1])\n    plt.plot(num_rolls, balance)\n\n\n\n\n\nThe last step is displaying meaningful data from our tracking variables. We can display our figure (shown below) that we created in our for loop. Also, we can calculate and display (shown below) our overall win probability and ending balance by averaging our win_probability and end_balance lists.\n\n\nShow the code\n# Averaging win probability and end balance\noverall_win_probability = sum(win_probability)/len(win_probability)\noverall_end_balance = sum(end_balance)/len(end_balance)# Displaying the averages\nprint(\"Average win probability after \" + str(num_simulations) + \"runs: \" + str(overall_win_probability))\n\n\nAverage win probability after 100runs: 0.16936999999999997\n\n\nShow the code\nprint(\"Average ending balance after \" + str(num_simulations) + \"runs: $\" + str(overall_end_balance))\n\n\nAverage ending balance after 100runs: $846.85\n\n\n\n\n\nThe most important part of any Monte Carlo simulation (or any analysis for that matter) is drawing conclusions from the results. From our figure, we can determine that the player rarely makes a profit after 1,000 rolls. In fact, the average ending balance of our 10,000 simulations is $833.66 (your results may be slightly different due to randomization). So, even though the house was “generous” in paying out 4 times our bet when the player won, the house still came out on top.\nWe also notice that our win probability is about 0.1667, or approximately 1/6. Let’s think about why that might be. Returning back to one of the earlier paragraphs, we noted that the player had 6 outcomes in which they could win. We also noted there are 36 possible rolls. Using these two numbers, we would expect that the player would win 6 out of 36 rolls, or 1/6 rolls, which matches our Monte Carlo prediction. Pretty cool!"
  },
  {
    "objectID": "posts/2022-10-29-graph-dataset/artykuł.html",
    "href": "posts/2022-10-29-graph-dataset/artykuł.html",
    "title": "DGL graph datastructure",
    "section": "",
    "text": "Very simple example below\n\n\nShow the code\n# Each value of the dictionary is a list of edge tuples.\n# Nodes are integer IDs starting from zero. Nodes IDs of different types have\n# separate countings.\nimport torch\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nimport dgl\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nratings = dgl.heterograph(\n    {('user', '+1', 'movie') : [(0, 0), (0, 1), (1, 0)],\n     ('user', '-1', 'movie') : [(2, 1)]})\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nratings\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ratings' is not defined\n\n\n\n\nIn order to save graph as object, very convenient method is to use pickle.\n\n\nShow the code\nimport pickle\n\noutput_file = 'builded_graph.pkl'\ndirectory = 'input/'\nwith open(directory + output_file, 'rb') as f:\n    g = pickle.load(f)\n\n\n#loading saved graph\nwith open(directory  + output_file, 'rb') as f:\n    sp_matrix = pickle.load(f)\n\n\nTo create a more realistic heterograph let’s use the ACM dataset. ## Graphs Dataset can be downloaded from here. It’s stored in mat (matrix) object. Within which we can find object/matrices stored in a compresed sparsed format. More about further.\n\n\nShow the code\nimport scipy.io\nimport urllib.request\n\ndata = scipy.io.loadmat(data_file_path+'ACM.mat')\n\n\n\n\n\nMany different ways to store sparsed matrices may be find in scipy documentation.\nFor us most important will be csr_matrix and csc_matrix.\nCSC format is almost identical, except that values are indexed first by column with a column-major order. Usually, the CSC is used when there are more rows than columns. On the contrary, the CSR works better for a ‘wide’ format. So, her is taking CSR as an example here.\nBelow short example how sparsed matrices can be handle with scipy package.\n\n\nShow the code\nimport numpy as np\nfrom scipy.sparse import csr_matrix, csc_matrix\narr = np.array([[0, 0, 0], [0, 0, 1], [1, 2, 0]])\n\narr_csr = csr_matrix(arr)\narr_csc = csc_matrix(arr)\n\nprint(type(arr_csr))\n## <class 'scipy.sparse.csr.csr_matrix'>\nprint(type(arr_csc))\n\n# `CSC` format is almost identical, except that values are indexed first by column with a column-major order. Usually, the `CSC` is used when there are more rows than columns. On the contrary, the `CSR` works better for a ‘wide’ format. So, her is taking CSR as an example here\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint(arr_csr)\n##   (1, 2) 1\n##   (2, 0) 1\n##   (2, 1) 2\nprint(arr_csc)\n\n# however in order to get access ti those indexes need to use method to_coo.\n##   (2, 0) 1\n##   (2, 1) 2\n##   (1, 2) 1\narr_csr.tocoo().row\n## array([1, 2, 2], dtype=int32)\narr_csr.tocoo().col\n\n#Viewing stored data (not the zero items) with the data property\n## array([2, 0, 1], dtype=int32)\nprint(arr_csr.data)\n## [1 1 2]\nprint(arr_csc.data)\n\n#Counting nonzeros with the count_nonzero() method:\n## [1 2 1]\nprint(arr_csr.count_nonzero())\n## 3\nprint(arr_csc.count_nonzero)\n## <bound method _data_matrix.count_nonzero of <3x3 sparse matrix of type '<class 'numpy.int64'>'\n##  with 3 stored elements in Compressed Sparse Column format>>\nprint(arr_csr.toarray())\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\nprint(arr_csc.todense())\n\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\n\n\n\n\n\n\n\n\n\nShow the code\nimport scipy.sparse as sp\n\nsp_matrix = data['PvsA']\nprint(type(sp_matrix))\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint('#Papers:', sp_matrix.shape[0])\n## #Papers: 12499\nprint('#Authors:',sp_matrix.shape[1])\n## #Authors: 17431\nprint('#Links:', sp_matrix.nnz)\n\n# ways of populating graph with coo_matrix\n## #Links: 37055\npp_g = dgl.bipartite_from_scipy(sp_matrix, utype='paper', etype='written-by', vtype='author')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\npp_g.is_homogeneous\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.number_of_nodes())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.number_of_edges())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint(pp_g.successors(3))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Node types:', pp_g.ntypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Edge types:', pp_g.etypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\nprint('Canonical edge types:', pp_g.canonical_etypes)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\n\n\nIn order to visualize the interactions (edges) between nodes let use following function.\n\n\nShow the code\nimport pygraphviz as pgv\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pygraphviz'\ndef plot_graph(nxg, plot_name):\n    ag = pgv.AGraph(strict=False, directed=True)\n    for u, v, k in nxg.edges(keys=True):\n        ag.add_edge(u, v, label=k)\n    ag.layout('dot')\n    ag.draw(plot_name+'.png')\n\n\n\n\nShow the code\nplot_graph(nxg=pp_g.metagraph(),plot_name='simple_graph')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pp_g' is not defined\n\n\n\n\n\n\nUsing ACM dataset\n\n\nShow the code\nimport torch\n\n# Unfortunately following code no longer works\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'torch'\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : data['PvsA'],\n        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n        ('paper', 'citing', 'paper') : data['PvsP'],\n        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n        ('paper', 'is-about', 'subject') : data['PvsL'],\n        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n    })\n\n# we need to a little bit tweak the code the get the same result as above.\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : \n          (torch.tensor(data['PvsA'].tocoo().col),torch.tensor(data['PvsA'].tocoo().row )),\n         ('author', 'writing', 'paper') : \n           (torch.tensor(data['PvsA'].tocoo().row) ,torch.tensor(data['PvsA'].tocoo().col)),\n        ('paper', 'citing', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().col),torch.tensor(data['PvsP'].tocoo().row )),\n        ('paper', 'cited', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().row) ,torch.tensor(data['PvsP'].tocoo().col)),\n        ('paper', 'is-about', 'subject') : \n          (torch.tensor(data['PvsL'].tocoo().col),torch.tensor(data['PvsL'].tocoo().row )),\n        ('subject', 'has', 'paper') : \n          (torch.tensor(data['PvsL'].tocoo().row) ,torch.tensor(data['PvsL'].tocoo().col))\n    })\n  \n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'dgl' is not defined\nplot_graph(nxg=G.metagraph(),plot_name='more_complicated_graph')\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'G' is not defined\n\n\n\n\n\n\n\nOn github repository we can find a method allowing for building graphs using pandas dataphrame.\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.insert(1, repo_directory)\nfrom builder import PandasGraphBuilder\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'builder'\nusers=pd.DataFrame(data=range(1,101), columns=['user_id'])\nproducts=pd.DataFrame(data=range(1,50), columns=['product_id'])\ninteractions=pd.DataFrame(data={\n  'user_id': np.random.choice(users.user_id,1000,replace=True),\n  'product_id' :np.random.choice(products.product_id,1000,replace=True)}\n  )\n\n\n\n\ngraph_builder = PandasGraphBuilder()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'PandasGraphBuilder' is not defined\ngraph_builder.add_entities(users, 'user_id', 'user')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_entities(products, 'product_id', 'product')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_binary_relations(interactions, 'user_id','product_id', 'interaction')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\ngraph_builder.add_binary_relations(interactions, 'product_id','user_id', 'interaction-by')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\nprint('starting graph building')\n## starting graph building\ng = graph_builder.build()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'graph_builder' is not defined\nplot_graph(nxg=g.metagraph(),plot_name='pandas_graph')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'g' is not defined"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "List of all articles can be found below",
    "section": "",
    "text": "R lib: data.table\n\n\n\n\n\n\n\ndata.table\n\n\n\n\nR data.table\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nNLP - multiple methods and concepts\n\n\n\n\n\n\n\nPython\n\n\nNLP\n\n\nTensorflow\n\n\n\n\nNotebook presenting several concept denoting to NLP like: tockenization, creating and saving embeddings, using pretrained embeding, visualizing embedding, sequncial and bag of words approach to NLP models.\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR_Markdown\n\n\n\n\n\n\n\nRmarkdown\n\n\n\n\nPresentation of R Markdown functionality\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nWater sensors\n\n\n\n\n\n\n\nPython\n\n\nTime Series\n\n\nanomaly detection\n\n\n\n\nPresentation of Time Series exploration, anomaly detection, and predictions techniques\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nDescribing statistical methods\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Tests\n\n\n\n\n\n\n\nUnit testing\n\n\n\n\n\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - forecast\n\n\n\n\n\n\n\nTime Series\n\n\nforecast\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - missing data imputation & Smoothing\n\n\n\n\n\n\n\nTime Series\n\n\nSmoothing\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nGrow of Metropolis\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPI connections\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAPI\n\n\n\n\nPresentation how to connect to API with token authenthification using R and Python\n\n\n\n\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n  \n\n\n\n\nDGL graph datastructure\n\n\n\n\n\n\n\nDGL\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Łukasz Rąbalski blog",
    "section": "",
    "text": "Hi! My name is Łukasz Rąbalski.\nI hope you will find content served here interesting and useful.\nWhat you may find here:\n- Python / R / SQL algorithm\n- Application showcases of statistical / algorithmical method aimed toward solving real life problems.\n- Explanation of mathematical foundation lying behind above mentioned methods\nEach article has attached Category and one or more tags which makes it easier to find useful content.\nThis page was developed with awesome R package blogdown 👍"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html",
    "title": "Water sensors",
    "section": "",
    "text": "##### libraries\nIn the attached file you will find data containing two water sensors: water level and water velocity.\nColumn named “level” refers to raw level data and column named “velocity” refers to raw velocity data. Columns “final_level” and “final_velocity” refer to data that was manually corrected or removed because of malfunctions of sensors or other reasons. You have three tasks here:"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#missing-data",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#missing-data",
    "title": "Water sensors",
    "section": "1.1 missing data",
    "text": "1.1 missing data\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport math \ndf = pd.read_csv('/Users/lrabalski1/Desktop/prv/data/water_sensors.csv', parse_dates=[\"time\"])\nprint('min time value: ',df.time.min(),'\\n max time value: ',df.time.max())\n## min time value:  2019-01-01 00:00:00 \n##  max time value:  2021-05-31 23:55:00\ndf=pd.DataFrame(\n  pd.date_range('2019-01-01 00:00:00','2021-05-31 23:55:00',freq='5T'),\n  columns=['time']\n  ).merge(df, how='left', on=['time'])\ndf.index=df.time\n\n\nprint('NA within level_final: ',sum(df.level_final.isna()),'\\nNA within velocity_final: ', sum(df.velocity_final.isna()))\n\n## NA within level_final:  13725 \n## NA within velocity_final:  13725\ndf['level_data_malfunction']=(df.level.isna()) | (df.level!=df.level_final)\ndf['velocity_data_malfunction']=(df.velocity.isna()) | (df.velocity!=df.velocity_final)\n\n\n# create a list of our conditions\nconditions_level = [\n    (df['level_data_malfunction'] ==False),\n    (df['level_data_malfunction'] ==True) & ( df['level_final'].isna()),\n    (df['level_data_malfunction'] ==True) & ( ~df['level_final'].isna())\n    ]\nvalues_level = [np.nan,'sensors_malfunction','manual_correction']\nconditions_velocity = [\n    (df['velocity_data_malfunction'] ==False),\n    (df['velocity_data_malfunction'] ==True) & ( df['velocity_final'].isna()),\n    (df['velocity_data_malfunction'] ==True) & ( ~df['velocity_final'].isna())\n    ]\nvalues_velocity = [np.nan,'sensors_malfunction','manual_correction']\ndf['velocity_reason_of_correction'] = np.select(conditions_velocity, values_velocity)\ndf['level_reason_of_correction'] = np.select(conditions_level, values_level)\nprint('\\n\\n')\n\n\nMissing data in both variable (velocity and level), which can be explained as sensors malfunction, is observed in 13725 data points. For both variable missing data appears in the same datapoint. Probably sensors are responsible for measuring both measures.\nAdditionally in case of level measure we can observe 204 datapoint where data were manually corrected.\n\n\nShow the code\nprint(pd.crosstab(df['velocity_reason_of_correction'],df['level_reason_of_correction']))\n## level_reason_of_correction     manual_correction     nan  sensors_malfunction\n## velocity_reason_of_correction                                                \n## nan                                          204  240087                    0\n## sensors_malfunction                            0       0                13725"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#patterns",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#patterns",
    "title": "Water sensors",
    "section": "1.2 patterns",
    "text": "1.2 patterns\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\ndf['time'] = pd.to_datetime(df['time'])\ndf_t=df.copy()\ndf_t.index = df_t['time']\ndf_t=df_t.drop(['time','level_data_malfunction', 'velocity_data_malfunction','velocity_reason_of_correction', 'level_reason_of_correction'], axis=1)\n\ncheck=df_t.sort_values('level_final', ascending=False)\n\n\nFrom the chart it can bee seen that:\n\nthere is rather no trend within each timeseries\nin level data one big anomaly can be observed in the middle of a period\n\n\n\nShow the code\n\n# downsampling\ndf_M = df_t.resample(\"D\").mean()\n\nstyles1 = ['b-','r-']\nplt.clf()\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True)\ndf_M[['level_final']].plot(ax=axes[0],style=styles1)\ndf_M[['velocity_final']].plot(ax=axes[1],style=styles1)\nplt.show()\n\n\n\n\n\nShow the code\ncheck=df_M.sort_values('level_final', ascending=False)\n\n\n\n1.2.1 stationarity\nAs it can be seen on the both plots out time series seeme to be stationary, rolling mean and SD is stable within time.\n\n\nShow the code\n\nrolling_mean = df_M.rolling(7).mean()\nrolling_std = df_M.rolling(7).std()\n\n\n#f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 1, 1) \n\nplt.plot(df_M['level_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['level_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['level_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Level Data\")\nplt.xticks([])\n## ([], [])\nplt.legend(loc=\"best\")\n\nfig.add_subplot(2, 1, 2) \nplt.plot(df_M['velocity_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['velocity_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['velocity_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Velocity Data\")\nplt.legend(loc=\"best\")\nplt.xticks([])\n## ([], [])\nplt.show()\n\n\n\n\n\nStiationarity of a data is support by the fact that p-value of Dickey–Fuller test is lower than 5 percent (null hypothesis it claiming that time series is non-stationary) and the test statistic is lower than the critical value. We can also draw these conclusions from inspecting the data.\n\n\nShow the code\nfrom statsmodels.tsa.stattools import adfuller\nadft_velocity = adfuller(df_M.dropna(subset=['velocity_final'])['velocity_final'],autolag=\"AIC\")\nadft_level = adfuller(df_M.dropna(subset=['level_final'])['level_final'],autolag=\"AIC\")\n\nadft=adft_level\noutput_level = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nadft=adft_velocity\noutput_velocity = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nresults = pd.DataFrame(output_velocity.join(output_level, lsuffix='_velocity', rsuffix='_level'))\nresults\n##    Values_velocity  ...                 Metric_level\n## 0        -4.758844  ...              Test Statistics\n## 1         0.000065  ...                      p-value\n## 2         8.000000  ...             No. of lags used\n## 3       841.000000  ...  Number of observations used\n## 4        -3.438149  ...          critical value (1%)\n## 5        -2.864983  ...          critical value (5%)\n## 6        -2.568603  ...         critical value (10%)\n## \n## [7 rows x 4 columns]\n\n\n\n\nShow the code\ntabela(py$results)\n\n\n\n\n\n\n\n\n\n1.2.2 autocorelation\nWe can see that both variable are weakly autocorelated, however velocity data better. Reason for bad autocorelation in level data measured this way, may have fact of big anomaly within dataset.\n\n\nShow the code\ndef my_function(df,col, array):\n  dicts = {}\n  for idx, x in enumerate(array):\n    dicts[x] = df[col].autocorr(x)\n  return dicts\nprint('autocorelation of level data :')\n## autocorelation of level data :\nmy_function(df_M,'level_final',array=[1,7,14,30,365])\n## {1: 0.6187899970924978, 7: 0.16030304961078023, 14: 0.1660215318228444, 30: 0.08793288098871045, 365: 0.016746891095633046}\nprint('\\n autocorelation of velocity data :')\n## \n##  autocorelation of velocity data :\nmy_function(df_M,'velocity_final',array=[1,7,14,30,365])\n## {1: 0.822156599237431, 7: 0.4759130585597128, 14: 0.3678199431634758, 30: 0.15582363183139622, 365: -0.11352389683675475}\n\n\n\n\n1.2.3 data imputation\nI will choose best method based on longest period upon which I have full data.\nMissing data is spread along all time range.\n\n\nShow the code\n\ndf_na = df_t.copy()\ndf_na['time']=df_na.index\ndf_na = df_na.reset_index(drop=True)\ndf_na['date']=df_na['time'].dt.date\ndf_na['na_velocity_final'] = df_na.velocity_final.isna()\ndf_na['na_level_final'] = df_na.level_final.isna()\ndates_with_na = df_na.groupby('date')[['na_level_final','na_velocity_final']].sum().reset_index()\ndates_with_na['any_na'] = dates_with_na.sum(axis=1)\n## <string>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\ndates_with_na[dates_with_na.any_na > 0]\n##            date  na_level_final  na_velocity_final  any_na\n## 116  2019-04-27              58                 58     116\n## 117  2019-04-28             119                119     238\n## 129  2019-05-10              64                 64     128\n## 142  2019-05-23              38                 38      76\n## 163  2019-06-13               4                  4       8\n## ..          ...             ...                ...     ...\n## 854  2021-05-04               1                  1       2\n## 855  2021-05-05               3                  3       6\n## 856  2021-05-06               2                  2       4\n## 869  2021-05-19               9                  9      18\n## 870  2021-05-20               9                  9      18\n## \n## [105 rows x 4 columns]\nplt.clf()\nfig = plt.figure()\nplt.plot(dates_with_na.date, dates_with_na.any_na)\n\nplt.show()\n\n\n\n\n\nThe longest chain of missing data is of 5476 consecutive data points (5 seconds * 5476 = 7.6 hours) .\n\n\nShow the code\nlongest_consecutive_na_chain = (~df_na['na_level_final']).cumsum().value_counts()\nlongest_consecutive_na_chain = longest_consecutive_na_chain[longest_consecutive_na_chain!=1]-1\n\n\ndf_na.index = df_na['time']\nlongest_concecutive_full_data_chain  = (df_na['na_level_final']).cumsum()\nlongest_concecutive_full_data_chain_counts = longest_concecutive_full_data_chain.value_counts()\n\nlongest_concecutive_full_data_chain_counts=longest_concecutive_full_data_chain_counts[longest_concecutive_full_data_chain_counts!=1]-1\n\nselected_chain = longest_concecutive_full_data_chain[longest_concecutive_full_data_chain==longest_concecutive_full_data_chain_counts.index[0]]\n\n\nAny automatic technic od missing data imputation like:\n\nMean, median and mode imputation\nForward and backward filling\nlinear / nearest imputation\n\ncould behave bad in such a kind o missing pattern (5476 consecutive data points is missing)\nThats why I will prepare two different imputation schema, check their accuracy for given dataset and choose better one:\n\nMethod 1: fill the missing data with the averaged value of given second of year. For seconds from january to may i will have maximum 3 data points (2019;2020;2021); for rest maximum of 2 data points.\nMethod 2:fill data with previous day data point with similar daytime\n\n\n\nShow the code\n\n\n# preparing dataset for Method 1\ndf_fill = df_t.copy()\ndf_fill['time']=df_fill.index\n#df_fill = df_fill.reset_index(drop=True)\ndf_fill['time_agg'] = pd.DatetimeIndex(df_fill.time).strftime(\"%m-%d %H:%M:%S\")\ndf_time_agg = df_fill.groupby('time_agg').agg(\n  velocity_final_mean=('velocity_final','mean'),\n  level_final_mean=('level_final','mean'),\n  velocity_mean=('velocity','mean'),\n  level_mean=('level','mean')\n).reset_index()\n\n\n#'after agregation to day of year I still have 54 missing value, which mostly come from one chain of missing data. I will replace them with data from previous day.')\n#df_time_agg.isna().sum()\n\ndf_time_agg = df_time_agg.join(df_time_agg.shift(24*12), rsuffix='_lag_1_day')\ndf_time_agg.velocity_final_mean = df_time_agg.velocity_final_mean.fillna(df_time_agg.velocity_final_mean_lag_1_day)\ndf_time_agg.velocity_mean = df_time_agg.velocity_mean.fillna(df_time_agg.velocity_mean_lag_1_day)\ndf_time_agg.level_final_mean = df_time_agg.level_final_mean.fillna(df_time_agg.level_final_mean_lag_1_day)\ndf_time_agg.level_mean = df_time_agg.level_mean.fillna(df_time_agg.level_mean_lag_1_day)\ndf_time_agg = df_time_agg[df_time_agg.columns.drop(list(df_time_agg.filter(regex='_lag_1_day')))]\n\n\n# testing two method\n## preparing test data\n\ndf_test = df_fill.loc[selected_chain.index].copy()\ndf_test['velocity_final_missing'] = df_test.velocity_final\ndf_test['level_final_missing'] = df_test.level_final\nrng = np.random.RandomState(42)\nrandom_indices = rng.choice(range(len(df_test)), size=round(len(df_test)/50))\ndf_test = df_test.reset_index(drop=True)\ndf_test.loc[random_indices, ['velocity_final_missing','level_final_missing']] = np.nan\n\n\n## fillig missing data\n### time_agg method - Method 1\ndf_test = df_test.merge(df_time_agg, how='left', on =['time_agg'])\ndf_test.index = df_test['time']\ndf_test['velocity_final_fill_method_1'] = df_test.velocity_final_missing.fillna(df_test.velocity_final_mean)\ndf_test['level_final_fill_method_1'] = df_test.level_final_missing.fillna(df_test.level_final_mean)\n\n### laged value - Method 2\ncols= ['velocity_final_fill_method_2','level_final_fill_method_2']\ntime_shifts = np.array([-24*12,-24*12+1,-24*12-1,-24*12+2,-24*12-2,-24*12+3,-24*12-3,-24*12*2,-24*12*2+1,-24*12*2-1,-24*12*2+2,-24*12*2+2,-24*12*2+3,-24*12*2-3])\n\ndf_test['velocity_final_fill_method_2'] = df_test.velocity_final_missing\ndf_test['level_final_fill_method_2'] = df_test.level_final_missing\n\ni=-1\nwhile df_test[['velocity_final_fill_method_2','level_final_fill_method_2']].isna().sum().sum() >0:\n  i=+1\n  time_shifts = [x+1 for x in time_shifts]\n  for col,time_shift in  ((x, y) for x in cols for y in time_shifts):\n    #print (col,time_shift)\n    df_test[col] = df_test[col].fillna(df_test[col].shift(time_shift))\n\n\n\n\n\n\n# testing difference\nlevel_dist_1 = abs(df_test.level_final_fill_method_2-df_test.level_final)\nlevel_dist_2 = abs(df_test.level_final_fill_method_1-df_test.level_final)\n\nvelocity_dist_1 = abs(df_test.velocity_final_fill_method_2-df_test.velocity_final)\nvelocity_dist_2 = abs(df_test.velocity_final_fill_method_1-df_test.velocity_final)\n\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 2, 1)\n\nplt.plot(level_dist_1, color=\"red\", label=\"Level - Method 1\")\nplt.plot(level_dist_2, color=\"black\", label = \"Level - Method 2\")\n#plt.title(\"Absolute Error for level missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 2)\nplt.bar(['Level - method 1','Level - method 2'], [level_dist_1.sum(),level_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 3)\nplt.plot(velocity_dist_1, color=\"red\", label=\"Velocity - Method 1\")\nplt.plot(velocity_dist_2, color=\"black\", label = \"Velocity - Method 2\")\n#plt.title(\"Absolute Error for velocity missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 4)\nplt.bar(['Velocity - method 1','Velocity - method 2'], [velocity_dist_1.sum(),velocity_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n1.2.4 periodic decomposition\nBoth dataset show quite strong daily seasonality (level higher then velocity) Both dataset show weak weekly seasonality (again level higher then velocity) There is no observable trend in both variables.\n\n\nShow the code\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\n\n\n# daily seasonality\ndf_D = df_fill_final\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','velocity_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n\n\n\n\n\nShow the code\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','level_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n\n# weakly seasonality\n\n\n\n\n\nShow the code\ndf_D = df_fill_final.resample(\"D\").mean()\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','velocity_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()\n\n\n\n\n\nShow the code\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','level_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#simple-solution",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#simple-solution",
    "title": "Water sensors",
    "section": "2.1 simple solution",
    "text": "2.1 simple solution\n\nHaving the knowledge of what was corrected … prepare simple solution to automatically find suspicious or corrupted data\n\nBelow I present two periods within which data correction of level variable took place. It shows that there is no more then 600 level value, while manuall correction reported that there were higher values to observe. Probably sensors are not able to make measurement above this level.\nAccording to this my suggestion would be:\n\n🚨 when the measurement of level will hit 600 sound the alarm 🚨\n\nThe data correction process which took place within 2019-06-13 was substle in size. Hard to be understooded without any extra information.\n\n\nShow the code\na = df.loc[df['level_reason_of_correction'] == 'manual_correction', ['level']] #anomaly\n\n\nsel_dates = (df.index>a.index.min()) & (df.index < a.index.max())\nsel_dates_1 = np.isin(df.index.date,list(set(a.index.date))[0:2])\nsel_dates_2 = np.isin(df.index.date,list(set(a.index.date))[2:])\n\na_1 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_1==True), ['level']] #anomaly\n\n\na_2 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_2==True), ['level']] #anomaly\n\ncheck = df.loc[sel_dates_2,['level','level_final']]\n\nplt.clf()\nfig = plt.figure()\n\nfig.add_subplot(2, 1, 1) \nplt.plot( df.loc[sel_dates_1,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_1,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_1.index,a_1['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18078., 18109., 18140., 18170., 18201., 18231., 18262.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\n\nfig.add_subplot(2, 1, 2) \nplt.plot( df.loc[sel_dates_2,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_2,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_2.index,a_2['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18272.   , 18272.125, 18272.25 , 18272.375, 18272.5  , 18272.625,\n##        18272.75 , 18272.875, 18273.   ]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\nfig.tight_layout()\nplt.show();"
  },
  {
    "objectID": "posts/2023-02-05 water-sensors/0_analiza.html#statistical-method",
    "href": "posts/2023-02-05 water-sensors/0_analiza.html#statistical-method",
    "title": "Water sensors",
    "section": "2.2 Statistical method",
    "text": "2.2 Statistical method\n\nHaving the knowledge of statistical methods prepare simple solution to automatically find suspicious or corrupted data.\n\nIn order to detect anomalies in dataset I used IsolationForest algorithm which identifies anomalies by isolating outliers in the data.\nThe contamination parameter defines a rough estimate of the percentage of the outliers in our dataset. Based on what i know about size of manual correction within my dataset I assigned contamination to be 0.1% in our case.\n\n\nShow the code\nrandom_state = np.random.RandomState(42)\noutliers_fraction = float(round(np.mean(df['level_reason_of_correction']=='manual_correction'),ndigits=3))\n\n\nscaler = StandardScaler()\nnp_scaled_velocity = scaler.fit_transform(df_fill_final.velocity_fill.values.reshape(-1, 1))\nnp_scaled_level = scaler.fit_transform(df_fill_final.level_fill.values.reshape(-1, 1))\n\ndata_velocity = pd.DataFrame(np_scaled_velocity)\ndata_level = pd.DataFrame(np_scaled_level)\n\n#train isolation forest\nmodel_velocity =  IsolationForest(contamination=outliers_fraction+0.01,random_state=random_state)\nmodel_level =  IsolationForest(contamination=outliers_fraction,random_state=random_state)\n\nmodel_velocity.fit(data_velocity)\n## IsolationForest(contamination=0.011,\n##                 random_state=RandomState(MT19937) at 0x2923E0940)\nmodel_level.fit(data_level)\n## IsolationForest(contamination=0.001,\n##                 random_state=RandomState(MT19937) at 0x2923E0940)\ndf_fill_final['velocity_anomaly'] = model_velocity.predict(data_velocity)\ndf_fill_final['level_anomaly'] = model_level.predict(data_level)\n\n\n# checking correctness\ndf.index = df['time']\ndf_fill_final = df_fill_final.join(df['level_reason_of_correction'])\n\n\n# visualization\n\na = df_fill_final.loc[df_fill_final['velocity_anomaly'] == -1, ['velocity_fill']] #anomaly\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(a.index,a['velocity_fill'], color='red', label = 'Anomaly')\nplt.scatter(df_fill_final.index, df_fill_final['velocity_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.legend()\nplt.show();\n\n\n\n\n\nShow the code\na = df_fill_final.loc[(df_fill_final['level_anomaly'] == -1), ['level_fill']] #anomaly\na_true = df_fill_final.loc[(df_fill_final['level_reason_of_correction'] == 'manual_correction') , ['level_fill']] #anomaly\n\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(df_fill_final.index, df_fill_final['level_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.scatter(a.index,a['level_fill'], color='red', label = 'Anomaly_IsolationForest',alpha=0.5, marker='*')\nplt.scatter(a_true.index,a_true['level_fill'], color='blue', label = 'bad data',alpha=0.5,marker='o',s=20)\nplt.legend()\nplt.show();\n\n\n\n\n\nShow the code\ndf_fill_final['anomaly_detected'] = (df_fill_final['level_anomaly'] == -1)\ndf_fill_final['anomaly_true'] = df_fill_final['level_reason_of_correction'] == 'manual_correction'\n\n\nIsolationForest managed to detect all manually coreccted data and additionally assign this label to 50 extra data points.\n\n\nShow the code\nprint(pd.crosstab(df_fill_final['anomaly_detected'] ,df_fill_final['anomaly_true'] ))\n## anomaly_true       False  True\n## anomaly_detected              \n## False             253762     0\n## True                  50   204"
  },
  {
    "objectID": "posts/1_api/index.html",
    "href": "posts/1_api/index.html",
    "title": "API connections",
    "section": "",
    "text": "Show the code\nlibrary(reticulate)\nmyenvs=conda_list()\nenvname=myenvs$name[4]\nuse_condaenv(envname, required = TRUE)\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\")\nreticulate::py_config()"
  },
  {
    "objectID": "posts/1_api/index.html#post-method",
    "href": "posts/1_api/index.html#post-method",
    "title": "API connections",
    "section": "1.1 POST method",
    "text": "1.1 POST method\nKnowledge based on\ndocumentation of httr\ntutorial how to authorized connections with token\nHTTP statuses\nBelow I present how to validly connect to API and receive output using POST method. I am showing different ways of how json could be prepared out of which only 1 is valid and allows for receiving expected results.\n\n\nShow the code\nlibrary(\"httr\")\nlibrary(\"jsonlite\")\n\n\n\nb_json_not_valid<-paste0(\"{'model':'agent','content':'/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz','sender': 'januszeky'}\")\n\nb_list=list(model= \"agent\",content = \"/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz\",sender= \"januszeky\")\nb_temp <- unbox(data.frame(b_list))\nb_jons <- toJSON(b_temp, pretty = TRUE)\n\n\nr<- httr::POST(url,\n                add_headers(Authorization = auth),\n                #content_type_json(),\n                content_type(\"application/json\"),\n                body = b_json_not_valid,\n                encode = \"json\"\n               )\nprint('json like below will result with \"Internal Server Error\"')\n\n\n[1] \"json like below will result with \\\"Internal Server Error\\\"\"\n\n\nShow the code\nprint(class(b_json_not_valid))\n\n\n[1] \"character\"\n\n\nShow the code\nprint(b_json_not_valid)\n\n\n[1] \"{'model':'agent','content':'/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz','sender': 'januszeky'}\"\n\n\nShow the code\nhttr::content(r) # to extract content from the response\n\n\n$error\n$error$message\n[1] \"Internal Server Error\"\n\n\nShow the code\nr<- httr::POST(url,\n                add_headers(Authorization = auth),\n                #content_type_json(),\n                content_type(\"application/json\"),\n                body = toJSON(b_list),\n                encode = \"json\"\n               )\n\nprint('json like below will more informative error, suggesting that argument should be string not list\"')\n\n\n[1] \"json like below will more informative error, suggesting that argument should be string not list\\\"\"\n\n\nShow the code\nprint(class(toJSON(b_list)))\n\n\n[1] \"json\"\n\n\nShow the code\nprint(toJSON(b_list))\n\n\n{\"model\":[\"agent\"],\"content\":[\"/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz\"],\"sender\":[\"januszeky\"]} \n\n\nShow the code\nhttr::content(r)\n\n\n$error\n$error$errors\n$error$errors$model\n$error$errors$model[[1]]\n[1] \"This value should be of type string.\"\n\n\n$error$errors$content\n$error$errors$content[[1]]\n[1] \"This value should be of type string.\"\n\n\n$error$errors$sender\n$error$errors$sender[[1]]\n[1] \"This value should be of type string.\"\n\n\n\n$error$message\n[1] \"Request is not valid\"\n\n\nShow the code\ntictoc::tic()\nr <- httr::POST(url,\n           add_headers(Authorization = auth),\n           content_type(\"application/json\"),\n           body = b_jons,\n           content_type_json()\n           #encode = \"json\"\n           )\ntictoc::toc()\n\n\n3.338 sec elapsed\n\n\nShow the code\nprint('json like below will finally give positive result')\n\n\n[1] \"json like below will finally give positive result\"\n\n\nShow the code\nprint(class(b_jons))\n\n\n[1] \"json\"\n\n\nShow the code\nprint(b_jons)\n\n\n{\n    \"model\": \"agent\",\n    \"content\": \"/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz\",\n    \"sender\": \"januszeky\"\n  } \n\n\nShow the code\nhttr::content(r)\n\n\n$content\n[1] \" putin dsfdfds mail od januszeky\"\n\n\nShow the code\nstop_for_status(r) #stop_for_status so that the function stops when there is an error.\n\n\nABOVE last working methods is substitute for following HTTP request form:\n\n\nShow the code\nPOST url\nContent-Type: application/json\nAuthorization: token_code\n\n{\n\"model\": \"agent\",\n\"content\" : \"/\\r\\n//\\r\\n//\\r\\n/putin/\\r\\n/dsfdfds pozdrawiam Janusz\",\n\"sender\": \"januszeky\"\n}\n>> path_to_save_file"
  },
  {
    "objectID": "posts/1_api/index.html#free-api-to-play-with",
    "href": "posts/1_api/index.html#free-api-to-play-with",
    "title": "API connections",
    "section": "1.2 Free API to play with",
    "text": "1.2 Free API to play with\nIf someone wants\nurl\n\n\nShow the code\nr <- GET(\n  \"http://api.stackexchange.com\",\n  path = \"questions\",\n  query = list(\n    site = \"stackoverflow.com\",\n    tagged = \"r\"\n  )\n)\n\n\nstop_for_status(r)\n\n# Automatically parse the json output\nquestions <- content(r)\n# questions$items[[1]]$title\n# questions$items[[1]]$link"
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html",
    "href": "posts/2022-06-02 statistics/3_Statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "inspirations: url url\n\n\n\n\n\nSurvived\nindependent\n\n\n\n\n0\n1.945910\n\n\n1\n4.262680\n\n\n1\n2.079442\n\n\n1\n3.970292\n\n\n0\n2.079442\n\n\n0\n2.079442\n\n\n\n\n\n\nTable 1. Summary statistics for logistic regression model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.6623408\n0.2493139\n-10.678668\n0\n\n\nindependent\n0.7429505\n0.0798121\n9.308746\n0\n\n\n\n\n\n\n\n\n\n\nHere’s the equation of a logistic regression model with 1 predictor X:\n\\(log(\\frac{P}{1-P})=\\beta_0 + \\beta_1X\\)\nWhere: P is the probability of having the outcome \\(\\frac{P}{1-P}\\) is the odds of the outcome. Left side called log odds or logit and P / (1-P) is the odds of the outcome. \\(\\beta_0\\) called intercept\n\n\n\nTo solve for the probability P, we exponentiate both sides of the equation above to get:\nEquation 1.: \\(\\frac{P}{1-P}= e^{\\beta_0 + \\beta_1X}\\)\nWith this equation, we can calculate the probability P for any given value of X\n\n\n\nWhen X = 0 the interpretation of intercept become simple using Equation 1. \\(P=\\frac{e^{\\beta_0}}{(1 + e^{\\beta_0})}\\) :\n\nIf the intercept has a negative sign: then the probability of having the outcome will be < 0.5.\nIf the intercept has a positive sign: then the probability of having the outcome will be > 0.5.\nIf the intercept is equal to zero: then the probability of having the outcome will be exactly 0.5.\n\nSo in case of my log model\n\n\nShow the code\nprint(coef(model)[1])\n\n\n(Intercept) \n  -2.662341 \n\n\nShow the code\nprob_of_survival<-round(exp(coef(model)[1])/(1+exp(coef(model)[1])),2)\nprint(paste0('according to model probability of surving the trip if the independent  variable was = 0 is: ',prob_of_survival))\n\n\n[1] \"according to model probability of surving the trip if the independent  variable was = 0 is: 0.07\"\n\n\n\n\n\nRemember, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable.\n\n\nShow the code\n#Tak przemnozone wspolczynniki interpretujemy nastepujaco:\n#  o ile % wzrosnie odds wystapienia zdarzenia jezeli wzrosnie nam wartosc predyktora o 1\nexp(coef(model))\n\n\n(Intercept) independent \n 0.06978468  2.10212868 \n\n\nPonizej w sposob matematyczny pokazuje ze to wlasnie oznacza interpretacja wzrostu parametra stajacego przy predyktorze.\n\n\nShow the code\ndf_aug <- augment(model, type.predict = \"response\") # without response argument, the fitted value will be on log-odds scale\n\np1 = df_aug$.fitted[df_aug$independent==1][1]\np2 = df_aug$.fitted[df_aug$independent==2][1]\n\nx <- round(p2/(1-p2)/(p1/(1-p1)),5)\n\n# i sprawdzenie czy dobrze rozumiem zależnosc\nx1<-round(exp(coef(model))['independent'],5)\nx1==x\n\n\nindependent \n         NA \n\n\nProb for independent= 1 was equal to NA while for independent = 2 was equal to NA. The odds increase by NA. The same what model results suggests -> 2.10213.\nQuiz\nThe fitted coefficient from the medical school logistic regression model is 5.45. The exponential of this is 233.73.\nDonald’s GPA is 2.9, and thus the model predicts that the probability of him getting into medical school is 3.26%. The odds of Donald getting into medical school are 0.0337, or—phrased in gambling terms—29.6:1. If Donald hacks the school’s registrar and changes his GPA to 3.9, then which of the following statements is FALSE:\nPossible Answers\n\nHis expected odds of getting into medical school improve to 7.8833 (or about 9:8).\nHis expected probability of getting into medical school improves to 88.7%.\nHis expected log-odds of getting into medical school improve by 5.45.\nHis expected probability of getting into medical school improves to 7.9%.\n\nCorrect answers on the bottom of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf_aug %>% mutate(Survived_hat=round(.fitted)) %>%\n  select(Survived, Survived_hat) %>% table\n\n\n        Survived_hat\nSurvived   0   1\n       0 470  75\n       1 219 123\n\n\nShow the code\n#Out of sample predictions\nDiCaprio<-data.frame(independent=1)\naugment(model, newdata = DiCaprio, type.predict = 'response')\n\n\n# A tibble: 1 × 2\n  independent .fitted\n        <dbl>   <dbl>\n1           1   0.128\n\n\n\n\n\nIn order to run stepwise regression you should use library SignifReg.\nSignifReg selects only significant predictors according to a designated criterion. More to be find url\nMethod based on AIC criterion not basing on significance of predictors to be found here url\n\n\nShow the code\nlibrary(SignifReg)\ndep_var_temp = 'Survived'\n\nfullmodel = glm(paste0(dep_var_temp,\" ~ .\"), family = 'binomial', data = df_titanic %>% select(-Name)) # model with all variables\nnullmodel = glm(paste0(dep_var_temp,\" ~ 1\"), family = 'binomial', data = df_titanic) # model with the intercept only\n  \nscope = list(lower=formula(nullmodel),upper=formula(fullmodel))\nfit1 <- nullmodel\nselect.fit = SignifReg(fit1, scope = scope, direction = \"forward\", trace = FALSE)\n\n\nbroom::tidy(select.fit) %>% \n  mutate(dep_var=dep_var_temp) %>% rename(indep_var=term) %>%  dplyr::select(dep_var,indep_var, everything()) %>%\n  mutate_if(is.numeric, round, digits=2) %>% rename(t.statistic=statistic) %>%\n  mutate(`exp_(estimate)`=ifelse(indep_var!='(Intercept)',exp(estimate),NA)) %>%\n  relocate(`exp_(estimate)`,.after=estimate)\n\n\n# A tibble: 6 × 7\n  dep_var  indep_var               estimate exp_(estim…¹ std.e…² t.sta…³ p.value\n  <chr>    <chr>                      <dbl>        <dbl>   <dbl>   <dbl>   <dbl>\n1 Survived (Intercept)                 4.03      NA         0.82    4.91    0   \n2 Survived Sexmale                    -2.67       0.0693    0.2   -13.5     0   \n3 Survived Pclass                     -1.01       0.364     0.17   -6.03    0   \n4 Survived Age                        -0.04       0.961     0.01   -5.65    0   \n5 Survived Siblings.Spouses.Aboard    -0.53       0.589     0.12   -4.41    0   \n6 Survived independent                 0.33       1.39      0.14    2.31    0.02\n# … with abbreviated variable names ¹​`exp_(estimate)`, ²​std.error, ³​t.statistic"
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---introduction",
    "href": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---introduction",
    "title": "Statistics",
    "section": "2 Bayesian Statistics - Introduction",
    "text": "2 Bayesian Statistics - Introduction\n\n2.1 Introduction\nThe role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update these probability distributions to reflect what has been learned from data.\nLet say I want to set an advertisement on social media. They claim, adds on their surface has 10% of clicks. I a bit sceptical and asses probable efectivnes may range between 0 and 0.20. I assume that binomial model will imitate process generating visitors. Binomial model is my generative model then.\n\n\nShow the code\nn_samples <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_samples, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)\n\npar(mfrow=c(1,2))\n# Visualize proportion clicks\nhist(proportion_clicks)\n# Visualize n_visitors\nhist(n_visitors)\n\n\n\n\n\n\n\n\n\nBelow I present joint distribution over both the underlying proportion of clicks and how many visitors I would get.\n\n\n\n\n\n\n\n\n\nI ran my ad campaign, and 13 people clicked and visited your site when the ad was shown a 100 times. I would now like to use this new information to update the Bayesian model. The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data.\n\n\nShow the code\n# Create the posterior data frame\nposterior <- prior[prior$n_visitors == 13, ]\n\n# Visualize posterior proportion clicks - below I condition the joint distribution - of prior distribution of proportion_clicks and distribution of n_visitors \nhist(posterior$proportion_clicks)\n\n\n\n\n\n\n\n\n\nNow we want to use this updated proportion_clicks to predict how many visitors we would get if we reran the ad campaign.\n\n\nShow the code\n# Assign posterior to a new variable called prior\nprior <- posterior\n\n# Take a look at the first rows in prior\nhead(prior)\n\n\n   proportion_clicks n_visitors\n10         0.1973504         13\n22         0.1555724         13\n66         0.1213712         13\n68         0.1366464         13\n93         0.1340447         13\n96         0.1325997         13\n\n\nShow the code\n# Replace prior$n_visitors with a new sample and visualize the result\nn_samples <-  nrow(prior)\nn_ads_shown <- 100\nprior$n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = prior$proportion_clicks)\nhist(prior$n_visitors)\n\n\n\n\n\n\n\n\n\n\n\n2.2 Priors\n\n2.2.1 Beta distribution\nThe Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you’ll explore how the two parameters of the Beta distribution determine its shape.\nSo the larger the shape parameters are, the more concentrated the beta distribution becomes.\n\n\nShow the code\n# Explore using the rbeta function\nbeta_1 <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)\nbeta_2 <- rbeta(n = 1000000, shape1 = 100, shape2 = 100)\nbeta_3 <- rbeta(n = 1000000, shape1 = 100, shape2 = 20)\nbeta_4 <- rbeta(n = 1000000, shape1 = 5, shape2 = 95)\n\n\n\npar(mfrow=c(2,2))\nhist(beta_1, breaks=seq(0,1,0.02), main = \"shape1 = 1, shape2 = 1\")\nhist(beta_2, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 100\")\nhist(beta_3, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 20\")\nhist(beta_4, breaks=seq(0,1,0.02), main = \"shape1 = 5, shape2 = 95\")\n\n\n\n\n\n\n\n\n\nThe 4th graphs represents the best following setence: Most ads get clicked on 5% of the time, but for some ads it is as low as 2% and for others as high as 8%.\n\n\n\n2.3 Contrasts and comparison\nLet say, I initialize also text add campaign, get 6 visitors out of 100 views and now I want to compare which one video or text add is more cost effective.\n\n\nShow the code\n# Define parameters\nn_draws <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_draws, size = n_ads_shown, \n                     prob = proportion_clicks)\nprior <- data.frame(proportion_clicks, n_visitors)\n\n# Create the posteriors for video and text ads\nposterior_video <- prior[prior$n_visitors == 13, ]\nposterior_text <- prior[prior$n_visitors == 6, ]\n\n# Visualize the posteriors\nhist(posterior_video$proportion_clicks, xlim = c(0, 0.25))\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior_text$proportion_clicks, xlim = c(0, 0.25))\n\n\n\n\n\n\n\n\n\nShow the code\nposterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000],\n                        text_prop = posterior_text$proportion_click[1:4000])\n\n# Calculate the posterior difference: video_prop - text_prop\nposterior$prop_diff <- posterior$video_prop - posterior$text_prop \n\n# Visualize prop_diff\nhist(posterior$prop_diff)\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate the median of prop_diff\nmedian(posterior$prop_diff)\n\n\n[1] 0.0647875\n\n\nShow the code\n# Calculate the proportion\nmean(posterior$prop_diff > 0.0)\n\n\n[1] 0.9475\n\n\nShow the code\n#Different adds have differnt costs then:\nvisitor_spend <- 2.53\nvideo_cost <- 0.25\ntext_cost <- 0.05\n\n# Add the column posterior$video_profit\nposterior$video_profit <- posterior$video_prop * visitor_spend - video_cost\n\n# Add the column posterior$text_profit\nposterior$text_profit <- posterior$text_prop * visitor_spend - text_cost\n\n# Visualize the video_profit and text_profit columns\nhist(posterior$video_profit)\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior$text_profit)\n\n\n\n\n\n\n\n\n\nShow the code\n# Add the column posterior$profit_diff\nposterior$profit_diff <- posterior$video_profit - posterior$text_profit\n\n# Visualize posterior$profit_diff\nhist(posterior$profit_diff)\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate a \"best guess\" for the difference in profits\nmedian(posterior$profit_diff)\n\n\n[1] -0.03608763\n\n\nShow the code\n# Calculate the probability that text ads are better than video ads\nmean(posterior$profit_diff < 0)\n\n\n[1] 0.64175\n\n\nShow the code\n#So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose the text ads is better.\n\n\n\n2.3.1 Changeing Generative model\nCompany has changed the way how they price adds. Now they take money just for full day of exposition. Binomial model, which approximate participation of succes in all trials (click in all views) is no longer valid. For new scenario. Poison distribution is now needed.\nThe Poison distribution takes only one parameter which is the mean number of events per time unit\nIn R you can simulate from a Poisson distribution using rpois where lambda is the average number of occurrences:\n\n\nShow the code\n# Change the model according to instructions\nn_draws <- 100000\nmean_clicks <- runif(n_draws, min = 0, max = 80) #this is my prior\nn_visitors <- rpois(n = n_draws, mean_clicks)\n\nprior <- data.frame(mean_clicks, n_visitors)\nposterior <- prior[prior$n_visitors == 19, ]\n\nhist(prior$mean_clicks)\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior$mean_clicks)\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Dealing with 2 parameter model\n\n\nShow the code\n#  the temperatures of Sweden water in 21 th of June in few following year\ntemp <- c(19,23,20,17,23)\n# Defining the parameter grid - here are are my priors about the posible values of parameters of distribution\npars <- expand.grid(mu = seq(8,30, by = 0.5), \n                    sigma = seq(0.1, 10, by= 0.3))\n# Defining and calculating the prior density for each parameter combination\npars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)\npars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)\npars$prior <- pars$mu_prior * pars$sigma_prior\n# Calculating the likelihood for each parameter combination\nfor(i in 1:nrow(pars)) {\n  likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])\n  pars$likelihood[i] <- prod(likelihoods)\n}\n# Calculate the probability of each parameter combination\npars$probability <- pars$likelihood * pars$prior\npars$probability <- pars$probability / sum(pars$probability )\n\nlibrary(lattice)\nlevelplot(probability ~ mu * sigma, data = pars)\n\n\n\n\n\n\n\n\n\nWhat’s likely the average water temperature for this lake on 20th of Julys, and what’s the probability the water temperature is going to be 18 or more on the next 20th?\nRight now the posterior probability distribution is represented as a data frame with one row per parameter combination with the corresponding probability.\n\n\nShow the code\nhead(pars)\n\n\n    mu sigma   mu_prior sigma_prior       prior likelihood probability\n1  8.0   0.1 0.01079819         0.1 0.001079819          0           0\n2  8.5   0.1 0.01312316         0.1 0.001312316          0           0\n3  9.0   0.1 0.01579003         0.1 0.001579003          0           0\n4  9.5   0.1 0.01880982         0.1 0.001880982          0           0\n5 10.0   0.1 0.02218417         0.1 0.002218417          0           0\n6 10.5   0.1 0.02590352         0.1 0.002590352          0           0\n\n\nBut my questions are much easier to answer if the posterior is represented as a large number of samples, like in earlier chapters. So, let’s draw a sample from this posterior.\n\n\nShow the code\nsample_indices <- sample(1:nrow(pars), size=10000, replace=TRUE, prob=pars$probability)\npars_sample <- pars[sample_indices,c(\"mu\",\"sigma\")]\nhead(pars_sample)\n\n\n      mu sigma\n252 21.0   1.6\n793 21.5   5.2\n519 19.5   3.4\n518 19.0   3.4\n341 20.5   2.2\n296 20.5   1.9\n\n\nWhat is probabibility of temperature being 18 or above? Not mean temperature, the actual temperature.\n\n\nShow the code\n#rnorm is vectorized and implicitly loops over mu and sigma\npred_temp<- rnorm(10000, mean=pars_sample$mu, sd=pars_sample$sigma)\n\npar(mfrow=c(1,2))\nhist(pars_sample$mu,30, main = 'probability distribution of mean temperature')\nhist(pred_temp,30, main = 'probability distribution of tempeture' )\n\n\n\n\n\n\n\n\n\nShow the code\nmean(pred_temp>=18)\n\n\n[1] 0.7366\n\n\n\n\n2.5 Automatisation - BEST package\nThe Bayesian model behind BEST assumes that the generative model for the data is a t-distribution; a more flexible distribution than the normal distribution as it assumes that data points might be outliers to some degree. This makes BEST’s estimate of the mean difference robust to outliers in the data.\nThe t-distribution is just like the normal distribution, a generative model with a mean and a standard deviation that generates heap shaped data. The difference is that the t-distribution has an extra parameter, sometimes called the degrees-of-freedom parameter, that governs how likely the t-distribution is to generate outliers far from its center.\nAnother way in which BEST is different is that BEST uses a so-called Markov chain Monte Carlo method to fit the model. Markov chain Monte Carlo, or MCMC for short, returns a table of samples from the posterior, we can work with the output just like before.\n\n\nShow the code\n# The IQ of zombies on a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)\n\n# Calculate the mean difference in IQ between the two groups\nmean(iq_brains) - mean(iq_regular)\n\n# Fit the BEST model to the data from both groups\nlibrary(BEST)\nlibrary(rjags)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\n\n# Plot the model result\nplot(best_posterior)\n\n\nAssume that a super smart mutant zombie (IQ = 150) got into the iq_regular group by mistake. This might mess up the results as you and your colleagues really were interested in how diet affects normal zombies.\n\n\nShow the code\n# The IQ of zombies given a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, \n                150) # <- Mutant zombie\n\n# Modify the data above and calculate the difference in means\nmean(iq_brains) - mean(iq_regular)\n\n\n\n\nShow the code\n# Fit the BEST model to the modified data and plot the result\nlibrary(BEST)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\nplot(best_posterior)\n\n\n\n\n2.6 Conclusions\nBayes allows you to tweak, change and tinker with the model to better fit the data analytical problem you have. But a last reason to use Bayes is because it is optimal, kind of. It can be shown, theoretically, that no other method learns as efficiently from data as Bayesian inference.\nIn above examples I show what Bayesian model is about: * I describe my expectations of proportion_clicks as uniform distribution (prior) * Then i describe a generative model which will be responsible for generating views based on proportion_clicks - the second source of variability. For this aim I use two diffrent distribution - binomial and poison - depending on specifity of exercise. * I was able to say which add wass better, more, I was able to say which add was better in probability way."
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---intermediate",
    "href": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---intermediate",
    "title": "Statistics",
    "section": "3 Bayesian Statistics - Intermediate",
    "text": "3 Bayesian Statistics - Intermediate\n\n3.1 Likelihood\nOn the example of poll. Imagine I am taking part in election to local goverment. Based on many historical election poles I can count on 45% of votes. Votes chances are approximate by bheta function.\n\n\nShow the code\ndf<-data.frame(sample=seq(0,1,0.01),\n               density=dbeta(x=seq(0,1,0.01),shape1=45,shape2=55))\ndf %>% ggplot(aes(x=sample,y=density))+\n  geom_line()+\n  ggtitle(\"Density function\")\n\n\nLets imagine that i receive 60% of votes in ellection pole. I can assume that binomial distribution is well suited for generative model responsible for how many votes I am geting. Then I may ask myself: **How probable would be obtaining such a results (60%) of votes under different succes_rate (paramter of Binomial distribution).\n\n\nShow the code\ndf<-data.frame(likelihood=dbinom(x=6,size=10,prob=seq(0,1,0.1)), \n               parameter_p=seq(0,1,0.1))\n\ndf %>% ggplot(aes(x=parameter_p,y=likelihood))+\n  geom_line()+\n  ggtitle(\"Likelihood distribution over different succes_rate parameters\")\n\n\nThe likelihood function summarizes the likelihood of observing polling data X under different values of the underlying support parameter p. Thus, the likelihood is a function of p that depends upon the observed data X\n\n\n3.2 Posterior\nSince I’ve got the prior & likelihood:\n\nprior: let say based on the historical pole % of votes I can count on is described by betha distribution Betha(45.55) –> most probable is geting 45% votes\nlikelihood: is denoting to the most recent data shown above\n\nI can approach now to modeling posterior model of p According to Bayes rules posterior is calculating by:\nposterior = prior * likelihood\nHowever, in more sophisticated model settings, tidy, closed-form solutions to this formula might not exist. Very loosely speaking, the goal here is to send information out to the JAGS program, which will then design an algorithm to sample from the posterior, based on which I will then simulate the posterior.\n\n3.2.1 Compiling rjags model\nBuilt from previous polls & election data, my prior model of is a Beta(,) with shape parameters a=45 and b=55. For added insight into p, I also polled potential voters. The dependence of X, the number of these voters that support you, on p is modeled by the Bin(n,p) distribution.\nIn the completed poll, X=6 of n=10 voters supported you. The next goal is to update my model of in light of these observed polling data! To this end, I will use the rjags package to approximate the posterior model of . This exercise will be break down into the 3 rjags steps: define, compile, simulate.\n\n\nShow the code\nlibrary(rjags)\n\n# DEFINE the model\nvote_model <- \"model{\n    # Likelihood model for X\n    X ~ dbin(p, n)\n    \n    # Prior model for p\n    p ~ dbeta(a, b)\n}\"\n\n# COMPILE the model    \nvote_jags <- jags.model(textConnection(vote_model), \n    data = list(a = 45, b = 55, X = 6, n = 10),\n    inits = list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 100))\n\n# SIMULATE the posterior\nvote_sim <- coda.samples(model = vote_jags, variable.names = c(\"p\"), n.iter = 10000)\n\n# PLOT the posterior\nplot(vote_sim, trace = FALSE)\n\n\nQuiz correct answers: d. Hint: `Remember`, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable."
  },
  {
    "objectID": "posts/2023-03-01 NLP/NLP.html",
    "href": "posts/2023-03-01 NLP/NLP.html",
    "title": "NLP - multiple methods and concepts",
    "section": "",
    "text": "import pandas as pd\nimport re #regular expression\n#pod maile\nimport datetime\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n#import tensorflow_hub as hub\n#import tensorflow_text\nimport numpy as np\nimport pandas as pd\n\n\n# !pip install matplotlib==3.6.0\n!pip install scikit-learn==1.2.1\n# !pip install tensorflow_text\n\n\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\ntf.config.list_physical_devices('GPU')"
  },
  {
    "objectID": "posts/2023-03-01 NLP/NLP.html#preparing-data",
    "href": "posts/2023-03-01 NLP/NLP.html#preparing-data",
    "title": "NLP - multiple methods and concepts",
    "section": "2 Preparing data",
    "text": "2 Preparing data\n\n2.1 preparing functions\n\n##| tags: []\ndef df_to_dataset_maszyna(dataframe, shuffle=True, batch_size=32):\n    sentencex = dataframe.filter(regex=(\"tresc_maila_cutted_clean\")).tresc_maila_cutted_clean.values\n    categories = dataframe.filter(regex=(\"_KATEGOR$\")).values\n    ds = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            tf.cast(sentencex, tf.string),\n            tf.cast(categories, tf.int32)\n        )\n    ).batch(batch_size)\n    )\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.prefetch(batch_size)\n    return ds  \n\n\n## funkcja czyszczaca maile\ndef preprocess_text(sen):\n\n    # transorm  telephone number into password\n    sentence = re.sub('(?<=\\D[4-9])\\d{8}(?=\\D)', ' dziewiec cyfr ', sen) #zamieniam 9 cyfr na haslo, za wyjatkiem numerow zaczynajaych sie od 1 bo to id_rezer + zastanwiam jedną cyfrę by jak wyrzucam duplikatu kilka powieleniem zostało w bazie (max 6 (4-9))\n    sentence = re.sub('\\d{3} \\d{3} \\d{3}', 'dziewiec cyfr ', sentence)\n    sentence = re.sub('\\d{3}[\\s-]\\d{3}[\\s-]\\d{3}', 'dziewiec cyfr ', sentence)\n    sentence = re.sub('\\d{2} \\d{3} \\d{2} \\d{2}', 'dziewiec cyfr ', sentence)\n    #numer konta\n    sentence = re.sub('\\d{26}', 'dwadziescia szesc cyfr', sentence)\n    #transform date into password\n    sentence = re.sub('\\s\\d{2}[.-]\\d{2}\\s', 'data', sentence)\n    sentence = re.sub('\\d{1,4}[.-]\\d{1,2}[.-]\\d{1,4}', 'data', sentence)\n    # Remove numbers\n    sentence = re.sub('\\d', ' ', sentence)\n    # Remove punctuations \n    sentence = re.sub('[-!_#\"*?:;,.><+=\\\\\\)(\\/]', \" \", sentence)\n    sentence = re.sub('&nbsp', ' ', sentence)\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z](?=(\\s+)|($))\", ' ', sentence)\n    sentence = re.sub(r\"^[a-zA-Z](?=(\\s+)|($))\", ' ', sentence)\n    # Removing specific week ending sentence\n    sentence = re.sub(r\" użytkownik \", ' ', sentence)\n    # Removing specific week ending sentence\n    sentence = re.sub(r\"( pon )|( wt )|( śr )|( czw )|( czwartek )|( pt )|(fri)|( sob )|( niedz )\", ' ', sentence)\n    # Removing specific month ending sentence\n    sentence = re.sub(r\"( st )|( lut )|( mar )|( kw )|( cze )|( lip )| (lipca)|( sierp )|( wrz )|( paź )|( lis )|( gru )\", ' ', sentence)\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n    return sentence\n\n\n\n##funkcja tworzaca kolumne z maile do uczenia\n### tabele musi zawierac nastepujace kolumny\n#### tresc_maila_cutted\n#### client_email\ndef przetworzenie_kolumny_z_mailem(df_start):\n  #wczytuje plik z r\n  df_start = pd.DataFrame(df_start)\n  X = []\n  sentences = list(df_start[\"tresc_maila_cutted\"])\n  for i in range(len(sentences)):\n    #print(i)\n    X.append(preprocess_text(sentences[i]))\n  #przetworzanie info o mailu\n  X_email = []\n  #emaile_do = list(df_start[\"email_do\"])\n  if typ=='agent':\n    client_email = list(df_start[\"client_email\"])\n    print('x')\n    for i in range(len(client_email)):\n      #print(i)\n      #' od ' + re.sub('@.*', '', client_email[i]) + \n      #+ ' mail do ' + re.sub('@.*', '', emaile_do[i]) + ' do domeny ' + re.sub('.*@', '', emaile_do[i])\n      email_text = ' mail od ' + re.sub('.*@', '', client_email[i])\n      X_email.append(email_text)\n  \n  df_start_clean = df_start\n  #wstawiam nowa kolumne na drugim miejscu\n  df_start_clean.insert(1, \"tresc_maila_cutted_clean\", X, True)\n  print('x2')\n  if typ=='agent':\n      df_start_clean.insert(2, \"email_tekst\", X_email, True)\n  print('x3')\n  #usuwanie zduplikowanych wartosci\n  #df_start_clean = df_start_clean.drop_duplicates(subset=['tresc_maila_cutted_clean'])\n  \n  \n  df_start_clean['tresc_maila_cutted_clean'].replace('', np.nan, inplace=True)\n  df_start_clean['tresc_maila_cutted_clean'].replace(' ', np.nan, inplace=True)\n  df_start_clean.dropna(subset=['tresc_maila_cutted_clean'], inplace=True)\n  df_start_clean=pd.DataFrame(df_start_clean)\n  if typ=='agent':\n      df_start_clean['tresc_maila_cutted_clean'] = df_start_clean['tresc_maila_cutted_clean'] + df_start_clean['email_tekst']\n  df_start_clean=pd.DataFrame(df_start_clean)\n  #df_start_clean['tresc_maila_cutted_clean'][0]\n  #df_start_clean.info()\n  #df_start_clean.nunique()\n  #print(df_start_clean.head(4))\n  return df_start_clean\n\n\n\ndef predykcja(typ,nr_modelu, df):\n  EXPORT_PATH = \"/home/lrabalski/Text Classification/USE/model_\"+typ+\"/\"+str(nr_modelu)\n  print(EXPORT_PATH)\n  model = tf.keras.models.load_model(EXPORT_PATH)\n  # df_maile_do_predykcji=df_maile_do_predykcji.reset_index()\n  # df_maile_do_predykcji.head\n  df.reset_index(drop=True,inplace=True)\n  df_maile_pred=pd.DataFrame(model.predict(\n    df[['tresc_maila_cutted_clean']].values\n    ))\n  df_maile_pred_all=pd.concat([df,df_maile_pred], axis=1#,ignore_index=True\n  )\n  return df_maile_pred_all\n\n\n##| tags: []\ntyp=  'client' #   'agent' #  \nsave_dir ='/data/lrabalski/DOP/call_back_models/'\n\n\n\n2.2 preparing dataframe\n\n##| tags: []\ndf=pd.read_csv('https://raw.githubusercontent.com/NavePnow/Google-BERT-on-fake_or_real-news-dataset/master/data/fake_or_real_news.csv',\n              skiprows=1,\n              names=['title','text','label','title_vectors']).drop('title_vectors',axis=1).reset_index()\n\n\n##| tags: []\n#### convert label into binary text\n\n\n##| tags: []\ndf.head()\nfor i in range(len(df)):\n    if df.loc[i, 'label'] == \"REAL\": #REAL equal 0\n        df.loc[i, 'label'] = 0\n    elif df.loc[i, 'label'] == \"FAKE\": #FAKE equal 1\n        df.loc[i, 'label'] = 1\n    if df.loc[i, 'text'] == \"\":\n        df = df.drop([i])\n\ndf['label'] = pd.Categorical(df['label'])\ndf['label'] = df.label.cat.codes\ndf['client_email']='xxx@xx.pl'\n\n\n##| tags: []\ndf = df.rename(columns={\"text\":\"tresc_maila_cutted\", \"label\": \"final__KATEGOR\"})\n\n\n##| tags: []\ndf_maile = df\n\n\n\n2.3 preparing datasets\n\n##| tags: []\ndf_start_clean = przetworzenie_kolumny_z_mailem(df_maile)\n\n\ntrain_df, test_df = train_test_split(df_start_clean, test_size=0.15,random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2,random_state=42) \n\nprint(\"Rozmiary datasetów\")\nprint(f\"Zbiór uczący {len(train_df)}\")\nprint(f\"Zbiór walidacyjny {len(val_df)}\")\nprint(f\"Zbiór testowy {len(test_df)}\")\n\n\ntrain_ds = df_to_dataset_maszyna(train_df, shuffle=True)\nval_ds = df_to_dataset_maszyna(val_df, shuffle=True)\ntest_ds = df_to_dataset_maszyna(test_df, shuffle=True)\n\n\n##| tags: []\ntmp_ds = df_to_dataset_maszyna(train_df, shuffle=True, batch_size=1)\nfor i in tmp_ds.shuffle(len(train_df)).take(2):\n    print(list(i))\n\n\n\n2.4 general parametrization\n\n##| tags: []\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\n\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nvocab = text_vectorization.get_vocabulary()\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\n\n\n2.5 assesing models\n\n2.5.1 learning rate\n\n##| tags: []\nhistory_lists[0]\n\n\n##| tags: []\nhistory_lists=list()\nfiles_list=list()\nfor (dir_path, dir_names, file_names) in os.walk(save_dir):\n    dir_names = [dir_path + '/' + x for x in  file_names if bool(re.match(r'history.pkl', x))]\n    history_lists = history_lists + dir_names   \n\n\n##| tags: []\nimport pickle\ndf_model_accuracy = pd.DataFrame(columns=['model','val_accuracy'])\n\nfor i in range(len(history_lists)):\n    with open(history_lists[i], 'rb') as f:\n        data = pickle.load(f)\n    model=re.search(r'(?<=call_back_models/)[A-Za-z|_]+', history_lists[i]).group()\n    acc=max(data['val_accuracy'])\n    df_model_accuracy = df_model_accuracy.append(pd.DataFrame({'model':model, 'val_accuracy':acc} , index=[0]))\n\ndf_model_accuracy\n\n\n\n2.5.2 visualizing learning rate\n\n## visualization of loss\nimport matplotlib.pyplot as plt \n\nplt.clf()\nhistory_dict = history.history \nloss_values = history_dict[\"loss\"] \nval_loss_values = history_dict[\"val_loss\"] \nepochs = range(1, len(loss_values) + 1) \nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\") \nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") \nplt.title(\"Training and validation loss\") \nplt.xlabel(\"Epochs\") \nplt.ylabel(\"Loss\") \nplt.legend() \nplt.show() \n\n\n\n2.5.3 accuracy on test data\n\n## accuracy on test data\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\n##model = keras.models.load_model(\"embeddings_bidir_gru.keras\") \nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\nmodel.summary()"
  },
  {
    "objectID": "posts/2023-03-01 NLP/NLP.html#models",
    "href": "posts/2023-03-01 NLP/NLP.html#models",
    "title": "NLP - multiple methods and concepts",
    "section": "3 Models",
    "text": "3 Models\n\n3.1 Encoding\nWord embeddings are vector representations of words thatmap human language into a structured geometric space. Word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors)\nthe vectors obtained through * one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (the same dimensionality as the number of words in the vocabulary), * word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors)\n\n3.1.1 learned word embedding\n\n3.1.1.1 parameters\n\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\nimport os\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\nsave_dir+model_name\n\n\n\n3.1.1.2 learning\n\n##| tags: []\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\n\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nvocab = text_vectorization.get_vocabulary()\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\n\n##| tags: []\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = layers.Embedding(input_dim=max_tokens, output_dim=256, name=\"embedding\")(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n  \ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/\"+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(int_train_ds, validation_data=int_val_ds, epochs=20\n                   , callbacks=callbacks\n                   )\n\n\n\n3.1.1.3 saving history\n\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)\n            \n## with open(save_dir+model_name +'/history.pkl', 'rb') as f:\n##     data = pickle.load(f)\n\n\n\n3.1.1.4 saving embeddings\nhttps://www.tensorflow.org/text/guide/word_embeddings\n\n##| tags: []\n\n## savings for visualisation\nimport io\n\nweights = model.get_layer('embedding').get_weights()[0]\nvocab = text_vectorization.get_vocabulary()\n\nprint(weights.shape)\nprint(len(vocab))\n\nout_v = io.open(save_dir+model_name +'/vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open(save_dir+model_name +'/metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0 or not bool(re.match(r'[a-zA-z]', word)):\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()\n\n\n##len(pd.read_csv(save_dir+model_name +'/vectors.tsv',encoding='UTF-8')) == len(pd.read_csv(save_dir+model_name +'/metadata.tsv',encoding='UTF-8'))\n\n## to observe \n##http://projector.tensorflow.org/\n\n\n##| tags: []\nvocab[4]\nweights[4]\n\n\n\n3.1.1.5 visualizing embeddings\nhttps://www.tensorflow.org/tensorboard/tensorboard_projector_plugin https://projector.tensorflow.org/\n\n\n\n3.1.2 fine tunning\n\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmodel = tf.keras.models.load_model(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\")\n\n\n##| tags: []\ntry:\n    initial_epochs = history.epoch[-1]\nexcept NameError:\n    initial_epochs = 0\ninitial_epochs=0\n\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\nfine_tune_epochs = 20\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\nhistory_fine = model.fit(int_train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=initial_epochs,\n                         validation_data=int_val_ds, \n                         callbacks=callbacks\n                        )\n\n\n##| tags: []\nx = model.get_layer('embedding').get_weights()[0:2]\n\n\n##| tags: []\nimport io\n\nweights = model.get_layer('embedding').get_weights()[0]\nvocab = text_vectorization.get_vocabulary()\n\nprint(weights.shape)\nprint(len(vocab))\n\nout_v = io.open(save_dir+model_name +'/vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open(save_dir+model_name +'/metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0 or not bool(re.match(r'[a-zA-z]', word)):\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()\n\n\n\n3.1.3 PRETRAINED WORD EMBEDDINGS\nThe rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect that the features you need are fairly generic\n\n3.1.3.1 my_embeddings\n\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmetadata = pd.read_csv(save_dir+model_name+'/metadata.tsv',encoding='UTF-8', header=0,names=['word'])\nvectors = pd.read_csv(save_dir+model_name+'/vectors.tsv',encoding='UTF-8', header=0,names=['vector'])\n\n\n##| tags: []\nimport numpy\nembeddings_index = {} \nfor index, word in enumerate(metadata.word):\n    coefs = numpy.fromstring(vectors.vector[index], \"f\", sep=\"\\t\")\n    embeddings_index[word] = coefs\n\nembedding_dim = coefs.shape[0]\nembedding_dim\n\n##embeddings_index\n\n\n##| tags: []\n## Next, let’s build an embedding matrix that you can load into an Embedding layer. \n##It must be a matrix of shape (max_words, embedding_dim), where each entry i contains \n##the embedding_dim-dimensional vector for the word of index i in the reference word index \n##(built during tokenization).\n\n\n##| tags: []\nvocabulary = text_vectorization.get_vocabulary()             \nword_index = dict(zip(vocabulary, range(len(vocabulary))))   \n \nembedding_matrix = np.zeros((max_tokens, embedding_dim))     \nfor word, i in word_index.items():\n    if i < max_tokens:\n        embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:                         \n        embedding_matrix[i] = embedding_vector   \n\nembedding_layer = layers.Embedding(\n    max_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=True\n)\n\n\n##| tags: []\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = embedding_layer(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n  \n\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=3)\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\n\n##| tags: []\nlist(int_val_ds)\n\n\n\n3.1.3.2 USE\n\n3.1.3.2.1 DENSE\n\n##| tags: []\nmodel_name = 'USE_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n##| tags: []\ntext_input = tf.keras.Input(shape=(), name=\"sentence\", dtype=tf.string)\ntext_embed = tfh.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\", \n                            name=\"text_embedding\")(text_input)\ndense_relu_64 = tf.keras.layers.Dense(256, activation=\"relu\")(text_embed)    \ndense_relu_64_2 = tf.keras.layers.Dense(256, activation=\"relu\")(dense_relu_64)  \nout  =  tf.keras.layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(dense_relu_64_2)\nmodel = tf.keras.Model(inputs=text_input, outputs=out)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n\n\n##| tags: []\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(train_ds, \n                    validation_data=val_ds, \n                    epochs=20\n                   , callbacks=callbacks\n                   )\n\n\n##| tags: []\nmodel_name = 'USE_embeddings_bidir_lstm'\nimport pickle\n\n## with open(save_dir+model_name +'/history.pkl', 'wb') as f:\n##             pickle.dump(history.history, f)\n            \nwith open(save_dir+model_name +'/history.pkl', 'rb') as f:\n    data = pickle.load(f)\n\n\n\n3.1.3.2.2 LSTM\n\n##| tags: []\nmodel_name = 'USE_LSTM'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n##| tags: []\ntext_input = tf.keras.Input(shape=(), name=\"sentence\", dtype=tf.string)\ntext_embed = tfh.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\", \n                            name=\"text_embedding\")(text_input)\nx = layers.Bidirectional(layers.LSTM(32))(text_embed)   \nx = layers.Dropout(0.5)(x) \noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)    \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\n\nmodel.summary()\n\n\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(train_ds, \n                    validation_data=val_ds, \n                    epochs=20\n                   , callbacks=callbacks\n                   )\n\n\n\n\n\n\n3.2 Sequence\nWhat if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what sequence models are about.\n\n3.2.1 LSTM\n\nmodel_name = 'raw_sequence_bidir_lstm'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n##| tags: []\n##Preparing integer sequence datasets\nfrom tensorflow.keras import layers\n\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n##**A sequence model built on one-hot encoded vector sequences**\n\n\n##| tags: []\nlist(int_train_ds.take(1))[0][0][0]\n##The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary).\n\n\n##| tags: []\nfrom tensorflow.keras import layers\n\nmax_length = 20\nmax_tokens = 50\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\nsentence = [ \"I write, erase, rewrite, Erase again, and then,A poppy blooms.\" ]\n\ntext_vectorization.adapt(sentence)\nvocabulary=text_vectorization.get_vocabulary()\nsentence = text_vectorization(sentence)\n\nimport tensorflow as tf\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = tf.one_hot(inputs, depth=max_tokens)\nmodel = keras.Model(inputs, embedded)\n\npredict=model.predict(sentence)\npredict.shape\n\n\n##| tags: []\nimport tensorflow as tf\ninputs = keras.Input(shape=(None,), dtype=\"int64\")    \nembedded = tf.one_hot(inputs, depth=max_tokens)       \nx = layers.Bidirectional(layers.LSTM(32))(embedded)   \nx = layers.Dropout(0.5)(x) \noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)    \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n\n##| tags: []\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nmodel.fit(int_train_ds, \n          validation_data=int_val_ds, \n          epochs=2,\n          callbacks=callbacks)\n\n\n3.2.1.1 Wnioski\nA first observation: this model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000) (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review.\n\n\n\n3.2.2 Transformers\n\n##| tags: []\nimport os\nmodel_name = \"full_transformer_encoder\"\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n##| tags: []\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):  \n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(                          \n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)              \n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n  \n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions                        \n \n    def compute_mask(self, inputs, mask=None):                             \n        return tf.math.not_equal(inputs, 0)                                \n \n    def get_config(self):                                                  \n        config = super().get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config\n    \n\n  \nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim                         \n        self.dense_dim = dense_dim                         \n        self.num_heads = num_heads                         \n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n    def call(self, inputs, mask=None):                    \n        if mask is not None:                              \n            mask = mask[:, tf.newaxis, :]                 \n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n  \n    def get_config(self):                                 \n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\n\n##| tags: []\nvocab_size = 20000 \nsequence_length = 600 \nembed_dim = 256 \nnum_heads = 2 \ndense_dim = 32 \n  \ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)   \nx = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n\n##| tags: []\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                    save_best_only=True)\n] \nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)\n\n\n##| tags: []\nmodel_name = 'full_transformer_encoder'\nmodel = tf.keras.models.load_model(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                  custom_objects={\"TransformerEncoder\": TransformerEncoder,\n                    \"PositionalEmbedding\": PositionalEmbedding})\n\n\n##| tags: []\ntry:\n    initial_epochs = history.epoch[-1]\nexcept NameError:\n    initial_epochs = 0\ninitial_epochs=0\n\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\nfine_tune_epochs = 2\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\nhistory_fine = model.fit(int_train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=initial_epochs,\n                         validation_data=int_val_ds, \n                         callbacks=callbacks\n                        )\n\n\n##| tags: []\nhistory_fine.history\n\n\n\n3.2.3 BERT\n\n##| tags: []\nbert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n\n\n##| tags: []\nbert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n\n\n##| tags: []\ntext_test = ['this is such an amazing movie!']\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n\n\n##| tags: []\nbert_model = hub.KerasLayer(tfhub_handle_encoder)\n\n\n##| tags: []\nbert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\n\n\n\n\n3.3 Bag of words\n\n3.3.1 Unigram\n\n##| tags: []\nimport os\nmodel_name = 'Unigram_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n3.3.1.1 text vectorization\n\n##| tags: []\nfrom tensorflow.keras.layers import TextVectorization\n\ntext_vectorization = TextVectorization(\n    max_tokens=20000,  #Limit the vocabulary to the 20,000 most frequent words                    \n    output_mode=\"multi_hot\",   #Encode the output tokens as multi-hot binary vectors.                     \n)\ntext_vectorization.adapt(text_only_train_ds)\n\n##Prepare processed versions of our training, validation, and test dataset. Make sure to specify num_parallel_calls to leverage multiple CPU cores\n\nbinary_1gram_train_ds = train_ds.map(               \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)                                                   \nbinary_1gram_val_ds = val_ds.map(                   \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)                           \nbinary_1gram_test_ds = test_ds.map(                 \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)\n\n\nfor inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break\n\n\n3.3.1.1.1 playground\n\n##| tags: []\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\ntext_vectorization.adapt(text_only_train_ds)\n\nvocabulary = text_vectorization.get_vocabulary()\n\n\n##| tags: []\nvocabulary[0:5]\n\n\n##| tags: []\ntest_sentence = \"do do na na nie proszę proszę\" \nencoded_sentence = text_vectorization(test_sentence)\nencoded_sentence[0:100]\n\n\n##| tags: []\ninverse_vocab = dict(enumerate(vocabulary))\ninverse_vocab[1]\n\n\n##| tags: []\ndecoded_sentence = \" \".join(inverse_vocab[int(i)] for i, check in enumerate(encoded_sentence.numpy()) if check==1)\ndecoded_sentence\n\n\n\n\n3.3.1.2 building model\n\n##| tags: []\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n  \ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = keras.Input(shape=(max_tokens,))\n    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"rmsprop\",\n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model\n\n\n##| tags: []\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\n##We call cache() on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory.                  \nhistory=model.fit(binary_1gram_train_ds.cache(), \n          validation_data=binary_1gram_val_ds.cache(),     \n          epochs=100,\n          callbacks=callbacks)\n\n\n##| tags: []\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)\n\n\n\n\n3.3.2 BIGRAMS WITH TF-IDF ENCODING\n\n##TF-IDF stands for “term frequency, inverse document frequency.”\n\n\n##| tags: []\nmodel_name = 'bigram_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\n\n3.3.2.1 text vectorization\n\n##| tags: []\ntext_vectorization = TextVectorization(\n    ngrams=3,\n    max_tokens=20000, \n    #output_mode=\"count\" #counting how many times each word or N-gram occurs\n    output_mode=\"tf_idf\"\n)\n\ntext_vectorization.adapt(text_only_train_ds)   \n\n\n##| tags: []\nvocabulary = text_vectorization.get_vocabulary()\nvocabulary\n\n\n##| tags: []\ntrain_ds\n\n\n##| tags: []\ntfidf_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\n\n##| tags: []\nmodel_name = 'bigram_dense'\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nmodel.fit(tfidf_2gram_train_ds.cache(),\n          validation_data=tfidf_2gram_val_ds.cache(),\n          epochs=100,\n          callbacks=callbacks)\n\n\n##| tags: []\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)"
  }
]