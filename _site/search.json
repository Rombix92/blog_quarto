[
  {
    "objectID": "posts/2022-11-05 data.table/R_data.table.html",
    "href": "posts/2022-11-05 data.table/R_data.table.html",
    "title": "R lib: data.table",
    "section": "",
    "text": "source\n\n\nsuppose you have a table of product sales and a table of commercials. You might want to associate each product sale with the most recent commercial that aired prior to the sale. In this case, you cannot do a basic join between the sales table and the commercials table because each sale was not tracked with a CommercialId attribute. Instead, you need to generate a mapping between sales and commercials based on logic involving their dates.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(data.table)\n\n\n\n\nShow the code\nsales <- data.table(\n  SaleId = c(\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"),\n  SaleDate = as.Date(c(\"2014-2-20\", \"2014-5-1\", \"2014-6-15\", \"2014-7-1\", \"2014-12-31\"))\n)\n\ncommercials <- data.table(\n  CommercialId = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n  CommercialDate = as.Date(c(\"2014-1-1\", \"2014-4-1\", \"2014-7-1\", \"2014-9-15\"))\n)\n\nsales[, RollDate := SaleDate]\ncommercials[, RollDate := CommercialDate]\n\nsetkey(sales, \"RollDate\")\nsetkey(commercials, \"RollDate\")\n\n\ndata.table is associating each commercial with the most recent sale prior to the commercial date (and including the commercial date). In other words, the most recent sale prior to each commercial is said to “roll forward”, and the SaleDate is mapped to the CommercialDate. Notice that sale S4 was the most recent sale prior to commercial C3 and C4, so S4 appears twice in the resultant table.\nTHIS HAVE NO logical SENSE. go forward\n\n\nShow the code\nsales[commercials, roll = TRUE]\n\n\n   SaleId   SaleDate   RollDate CommercialId CommercialDate\n1:   <NA>       <NA> 2014-01-01           C1     2014-01-01\n2:     S1 2014-02-20 2014-04-01           C2     2014-04-01\n3:     S4 2014-07-01 2014-07-01           C3     2014-07-01\n4:     S4 2014-07-01 2014-09-15           C4     2014-09-15\n\n\nHere data.table is associating each sale with the most recent commercial prior to (or including) the SaleDate. This is the solution to our originally stated problem.\n\n\nShow the code\ncommercials[sales, roll = TRUE]\n\n\n   CommercialId CommercialDate   RollDate SaleId   SaleDate\n1:           C1     2014-01-01 2014-02-20     S1 2014-02-20\n2:           C2     2014-04-01 2014-05-01     S2 2014-05-01\n3:           C2     2014-04-01 2014-06-15     S3 2014-06-15\n4:           C3     2014-07-01 2014-07-01     S4 2014-07-01\n5:           C4     2014-09-15 2014-12-31     S5 2014-12-31\n\n\n\ndata.table also supports backward rolling joins by setting roll = -Inf.\n\n\nShow the code\ncommercials[sales, roll = -Inf]\n\n\n   CommercialId CommercialDate   RollDate SaleId   SaleDate\n1:           C2     2014-04-01 2014-02-20     S1 2014-02-20\n2:           C3     2014-07-01 2014-05-01     S2 2014-05-01\n3:           C3     2014-07-01 2014-06-15     S3 2014-06-15\n4:           C3     2014-07-01 2014-07-01     S4 2014-07-01\n5:         <NA>           <NA> 2014-12-31     S5 2014-12-31"
  },
  {
    "objectID": "posts/2022-06-01 rmarkdown/index.html",
    "href": "posts/2022-06-01 rmarkdown/index.html",
    "title": "R_Markdown",
    "section": "",
    "text": "Markdown\n\nTutorial for markdown: https://bookdown.org/yihui/rmarkdown/\n\nBelow some useful examples:\nknitr is what runs each code chunk and knits the document. Knitr will use this option as default for each cchunk in the document when the file is knit. Followiw opts_chunk$dollar_set() we can add the options that we want to set globally to the parentheses before the echo = TRUE argument\n\ninclude - code & results appears in the result\necho - code appear in the knit file\neval - evaluate code in a code chunk\ncollapse - split code and any text output into multiple blocks or include in a single block in the final report\nwarning - display warning\nmessage - display message like from loading packages\nerror - stop kniting file when the error will occure (if false file will knit anyway)\n\nAt the begining of the markdown document I can find YAML header.\nThe YAML header controls the look and feel of your document. At the very least, your R markdown document should contain the following YAML header sandwiched between two sets of ---:\n\n\nShow the code\n    ---\n    title: \"Your document title\"\n    author: \"ES 218\"\n    output:\n      html_document: default\n    ---"
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html",
    "href": "posts/2022-06-02 statistics/3_Statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Markdown Tutorial\n\n\nQuiz correct answers: d. Hint: Remember, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable.\n\n\n\n\n\nSurvived\nFare\nFare_log\n\n\n\n\n0\n7.2500\n2\n\n\n1\n71.2833\n4\n\n\n1\n7.9250\n2\n\n\n1\n53.1000\n4\n\n\n0\n8.0500\n2\n\n\n0\n8.4583\n2\n\n\n\n\n\nWyliczanie modelu logistycznego.\n\n\nShow the code\nmodel <- glm(data=df_titanic, Survived ~ Fare_log, family = 'binomial')\n\ntidy(model)  %>% kable(caption='Table 1. Summary statistics for logistic regression model')\n\n\n\nTable 1. Summary statistics for logistic regression model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.3337286\n0.2452271\n-9.516601\n0\n\n\nFare_log\n0.6442428\n0.0792358\n8.130706\n0\n\n\n\n\n\n\n\nShow the code\ncoef(model)\n\n\n(Intercept)    Fare_log \n -2.3337286   0.6442428 \n\n\nShow the code\n#Tak przemnozone wspolczynniki interpretujemy nastepujaco:\n#  o ile % wzrosnie odds wystapienia zdarzenia jezeli wzrosnie nam wartosc predyktora o 1\n\nexp(coef(model))\n\n\n(Intercept)    Fare_log \n 0.09693365  1.90454431 \n\n\nPonizej w sposob matematyczny pokazuje ze to wlasnie oznacza interpretacja wzrostu parametra stajacego przy predyktorze.\n\n\nShow the code\ndf_aug <- augment(model, type.predict = \"response\") # without response argument, the fitted value will be on log-odds scale\n\np3 = df_aug$.fitted[df_aug$Fare_log==3][1]\np2 = df_aug$.fitted[df_aug$Fare_log==2][1]\n\nx <- round(p3/(1-p3)/(p2/(1-p2)),5)\n\n# i sprawdzenie czy dobrze rozumiem zależnosc\nx1<-round(exp(coef(model))['Fare_log'],5)\nx1==x\n\n\nFare_log \n    TRUE \n\n\nProb for Fare_log = 2 was equal to 0.2601396 while for Fare_log = 3 was equal to 0.401072. The odds increase by 1.90454. The same what model results suggests -> 1.90454.\nQuiz\nThe fitted coefficient from the medical school logistic regression model is 5.45. The exponential of this is 233.73.\nDonald’s GPA is 2.9, and thus the model predicts that the probability of him getting into medical school is 3.26%. The odds of Donald getting into medical school are 0.0337, or—phrased in gambling terms—29.6:1. If Donald hacks the school’s registrar and changes his GPA to 3.9, then which of the following statements is FALSE:\nPossible Answers\n\nHis expected odds of getting into medical school improve to 7.8833 (or about 9:8).\nHis expected probability of getting into medical school improves to 88.7%.\nHis expected log-odds of getting into medical school improve by 5.45.\nHis expected probability of getting into medical school improves to 7.9%.\n\nCorrect answers on the top of the page\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf_aug %>% mutate(Survived_hat=round(.fitted)) %>%\n  select(Survived, Survived_hat) %>% table\n\n\n        Survived_hat\nSurvived   0   1\n       0 462  83\n       1 219 123\n\n\nShow the code\n#Out of sample predictions\nDiCaprio<-data.frame(Fare_log=1)\naugment(model, newdata = DiCaprio, type.predict = 'response')\n\n\n# A tibble: 1 × 2\n  Fare_log .fitted\n     <dbl>   <dbl>\n1        1   0.156"
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---introduction",
    "href": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---introduction",
    "title": "Statistics",
    "section": "Bayesian Statistics - Introduction",
    "text": "Bayesian Statistics - Introduction\n\nIntroduction\nThe role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update these probability distributions to reflect what has been learned from data.\nLet say I want to set an advertisement on social media. They claim, adds on their surface has 10% of clicks. I a bit sceptical and asses probable efectivnes may range between 0 and 0.20. I assume that binomial model will imitate process generating visitors. Binomial model is my generative model then.\n\n\nShow the code\nn_samples <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_samples, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)\n\npar(mfrow=c(1,2))\n# Visualize proportion clicks\nhist(proportion_clicks)\n# Visualize n_visitors\nhist(n_visitors)\n\n\n\n\n\n\n\n\n\nBelow I present joint distribution over both the underlying proportion of clicks and how many visitors I would get.\n\n\n\n\n\n\n\n\n\nI ran my ad campaign, and 13 people clicked and visited your site when the ad was shown a 100 times. I would now like to use this new information to update the Bayesian model. The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data.\n\n\nShow the code\n# Create the posterior data frame\nposterior <- prior[prior$n_visitors == 13, ]\n\n# Visualize posterior proportion clicks - below I condition the joint distribution - of prior distribution of proportion_clicks and distribution of n_visitors \nhist(posterior$proportion_clicks)\n\n\n\n\n\n\n\n\n\nNow we want to use this updated proportion_clicks to predict how many visitors we would get if we reran the ad campaign.\n\n\nShow the code\n# Assign posterior to a new variable called prior\nprior <- posterior\n\n# Take a look at the first rows in prior\nhead(prior)\n\n\n    proportion_clicks n_visitors\n1           0.1467209         13\n14          0.1514527         13\n79          0.1440560         13\n160         0.1312125         13\n180         0.1109677         13\n190         0.1470336         13\n\n\nShow the code\n# Replace prior$n_visitors with a new sample and visualize the result\nn_samples <-  nrow(prior)\nn_ads_shown <- 100\nprior$n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = prior$proportion_clicks)\nhist(prior$n_visitors)\n\n\n\n\n\n\n\n\n\n\n\nPriors\n\nBeta distribution\nThe Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you’ll explore how the two parameters of the Beta distribution determine its shape.\nSo the larger the shape parameters are, the more concentrated the beta distribution becomes.\n\n\nShow the code\n# Explore using the rbeta function\nbeta_1 <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)\nbeta_2 <- rbeta(n = 1000000, shape1 = 100, shape2 = 100)\nbeta_3 <- rbeta(n = 1000000, shape1 = 100, shape2 = 20)\nbeta_4 <- rbeta(n = 1000000, shape1 = 5, shape2 = 95)\n\n\n\npar(mfrow=c(2,2))\nhist(beta_1, breaks=seq(0,1,0.02), main = \"shape1 = 1, shape2 = 1\")\nhist(beta_2, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 100\")\nhist(beta_3, breaks=seq(0,1,0.02), main = \"shape1 = 100, shape2 = 20\")\nhist(beta_4, breaks=seq(0,1,0.02), main = \"shape1 = 5, shape2 = 95\")\n\n\n\n\n\n\n\n\n\nThe 4th graphs represents the best following setence: Most ads get clicked on 5% of the time, but for some ads it is as low as 2% and for others as high as 8%.\n\n\n\nContrasts and comparison\nLet say, I initialize also text add campaign, get 6 visitors out of 100 views and now I want to compare which one video or text add is more cost effective.\n\n\nShow the code\n# Define parameters\nn_draws <- 100000\nn_ads_shown <- 100\nproportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)\nn_visitors <- rbinom(n = n_draws, size = n_ads_shown, \n                     prob = proportion_clicks)\nprior <- data.frame(proportion_clicks, n_visitors)\n\n# Create the posteriors for video and text ads\nposterior_video <- prior[prior$n_visitors == 13, ]\nposterior_text <- prior[prior$n_visitors == 6, ]\n\n# Visualize the posteriors\nhist(posterior_video$proportion_clicks, xlim = c(0, 0.25))\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior_text$proportion_clicks, xlim = c(0, 0.25))\n\n\n\n\n\n\n\n\n\nShow the code\nposterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000],\n                        text_prop = posterior_text$proportion_click[1:4000])\n\n# Calculate the posterior difference: video_prop - text_prop\nposterior$prop_diff <- posterior$video_prop - posterior$text_prop \n\n# Visualize prop_diff\nhist(posterior$prop_diff)\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate the median of prop_diff\nmedian(posterior$prop_diff)\n\n\n[1] 0.06589016\n\n\nShow the code\n# Calculate the proportion\nmean(posterior$prop_diff > 0.0)\n\n\n[1] 0.9495\n\n\nShow the code\n#Different adds have differnt costs then:\nvisitor_spend <- 2.53\nvideo_cost <- 0.25\ntext_cost <- 0.05\n\n# Add the column posterior$video_profit\nposterior$video_profit <- posterior$video_prop * visitor_spend - video_cost\n\n# Add the column posterior$text_profit\nposterior$text_profit <- posterior$text_prop * visitor_spend - text_cost\n\n# Visualize the video_profit and text_profit columns\nhist(posterior$video_profit)\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior$text_profit)\n\n\n\n\n\n\n\n\n\nShow the code\n# Add the column posterior$profit_diff\nposterior$profit_diff <- posterior$video_profit - posterior$text_profit\n\n# Visualize posterior$profit_diff\nhist(posterior$profit_diff)\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate a \"best guess\" for the difference in profits\nmedian(posterior$profit_diff)\n\n\n[1] -0.03329788\n\n\nShow the code\n# Calculate the probability that text ads are better than video ads\nmean(posterior$profit_diff < 0)\n\n\n[1] 0.6325\n\n\nShow the code\n#So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose the text ads is better.\n\n\n\nChangeing Generative model\nCompany has changed the way how they price adds. Now they take money just for full day of exposition. Binomial model, which approximate participation of succes in all trials (click in all views) is no longer valid. For new scenario. Poison distribution is now needed.\nThe Poison distribution takes only one parameter which is the mean number of events per time unit\nIn R you can simulate from a Poisson distribution using rpois where lambda is the average number of occurrences:\n\n\nShow the code\n# Change the model according to instructions\nn_draws <- 100000\nmean_clicks <- runif(n_draws, min = 0, max = 80) #this is my prior\nn_visitors <- rpois(n = n_draws, mean_clicks)\n\nprior <- data.frame(mean_clicks, n_visitors)\nposterior <- prior[prior$n_visitors == 19, ]\n\nhist(prior$mean_clicks)\n\n\n\n\n\n\n\n\n\nShow the code\nhist(posterior$mean_clicks)\n\n\n\n\n\n\n\n\n\n\n\n\nDealing with 2 parameter model\n\n\nShow the code\n#  the temperatures of Sweden water in 21 th of June in few following year\ntemp <- c(19,23,20,17,23)\n# Defining the parameter grid - here are are my priors about the posible values of parameters of distribution\npars <- expand.grid(mu = seq(8,30, by = 0.5), \n                    sigma = seq(0.1, 10, by= 0.3))\n# Defining and calculating the prior density for each parameter combination\npars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)\npars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)\npars$prior <- pars$mu_prior * pars$sigma_prior\n# Calculating the likelihood for each parameter combination\nfor(i in 1:nrow(pars)) {\n  likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])\n  pars$likelihood[i] <- prod(likelihoods)\n}\n# Calculate the probability of each parameter combination\npars$probability <- pars$likelihood * pars$prior\npars$probability <- pars$probability / sum(pars$probability )\n\nlibrary(lattice)\nlevelplot(probability ~ mu * sigma, data = pars)\n\n\n\n\n\n\n\n\n\nWhat’s likely the average water temperature for this lake on 20th of Julys, and what’s the probability the water temperature is going to be 18 or more on the next 20th?\nRight now the posterior probability distribution is represented as a data frame with one row per parameter combination with the corresponding probability.\n\n\nShow the code\nhead(pars)\n\n\n    mu sigma   mu_prior sigma_prior       prior likelihood probability\n1  8.0   0.1 0.01079819         0.1 0.001079819          0           0\n2  8.5   0.1 0.01312316         0.1 0.001312316          0           0\n3  9.0   0.1 0.01579003         0.1 0.001579003          0           0\n4  9.5   0.1 0.01880982         0.1 0.001880982          0           0\n5 10.0   0.1 0.02218417         0.1 0.002218417          0           0\n6 10.5   0.1 0.02590352         0.1 0.002590352          0           0\n\n\nBut my questions are much easier to answer if the posterior is represented as a large number of samples, like in earlier chapters. So, let’s draw a sample from this posterior.\n\n\nShow the code\nsample_indices <- sample(1:nrow(pars), size=10000, replace=TRUE, prob=pars$probability)\npars_sample <- pars[sample_indices,c(\"mu\",\"sigma\")]\nhead(pars_sample)\n\n\n       mu sigma\n1252 26.0   8.2\n525  22.5   3.4\n388  21.5   2.5\n612  21.0   4.0\n522  21.0   3.4\n429  19.5   2.8\n\n\nWhat is probabibility of temperature being 18 or above? Not mean temperature, the actual temperature.\n\n\nShow the code\n#rnorm is vectorized and implicitly loops over mu and sigma\npred_temp<- rnorm(10000, mean=pars_sample$mu, sd=pars_sample$sigma)\n\npar(mfrow=c(1,2))\nhist(pars_sample$mu,30, main = 'probability distribution of mean temperature')\nhist(pred_temp,30, main = 'probability distribution of tempeture' )\n\n\n\n\n\n\n\n\n\nShow the code\nmean(pred_temp>=18)\n\n\n[1] 0.736\n\n\n\n\nAutomatisation - BEST package\nThe Bayesian model behind BEST assumes that the generative model for the data is a t-distribution; a more flexible distribution than the normal distribution as it assumes that data points might be outliers to some degree. This makes BEST’s estimate of the mean difference robust to outliers in the data.\nThe t-distribution is just like the normal distribution, a generative model with a mean and a standard deviation that generates heap shaped data. The difference is that the t-distribution has an extra parameter, sometimes called the degrees-of-freedom parameter, that governs how likely the t-distribution is to generate outliers far from its center.\nAnother way in which BEST is different is that BEST uses a so-called Markov chain Monte Carlo method to fit the model. Markov chain Monte Carlo, or MCMC for short, returns a table of samples from the posterior, we can work with the output just like before.\n\n\nShow the code\n# The IQ of zombies on a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)\n\n# Calculate the mean difference in IQ between the two groups\nmean(iq_brains) - mean(iq_regular)\n\n# Fit the BEST model to the data from both groups\nlibrary(BEST)\nlibrary(rjags)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\n\n# Plot the model result\nplot(best_posterior)\n\n\nAssume that a super smart mutant zombie (IQ = 150) got into the iq_regular group by mistake. This might mess up the results as you and your colleagues really were interested in how diet affects normal zombies.\n\n\nShow the code\n# The IQ of zombies given a regular diet and a brain based diet.\niq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)\niq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, \n                150) # <- Mutant zombie\n\n# Modify the data above and calculate the difference in means\nmean(iq_brains) - mean(iq_regular)\n\n\n\n\nShow the code\n# Fit the BEST model to the modified data and plot the result\nlibrary(BEST)\nbest_posterior <- BESTmcmc(iq_brains, iq_regular)\nplot(best_posterior)\n\n\n\n\nConclusions\nBayes allows you to tweak, change and tinker with the model to better fit the data analytical problem you have. But a last reason to use Bayes is because it is optimal, kind of. It can be shown, theoretically, that no other method learns as efficiently from data as Bayesian inference.\nIn above examples I show what Bayesian model is about: * I describe my expectations of proportion_clicks as uniform distribution (prior) * Then i describe a generative model which will be responsible for generating views based on proportion_clicks - the second source of variability. For this aim I use two diffrent distribution - binomial and poison - depending on specifity of exercise. * I was able to say which add wass better, more, I was able to say which add was better in probability way."
  },
  {
    "objectID": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---intermediate",
    "href": "posts/2022-06-02 statistics/3_Statistics.html#bayesian-statistics---intermediate",
    "title": "Statistics",
    "section": "Bayesian Statistics - Intermediate",
    "text": "Bayesian Statistics - Intermediate\n\nLikelihood\nOn the example of poll. Imagine I am taking part in election to local goverment. Based on many historical election poles I can count on 45% of votes. Votes chances are approximate by bheta function.\n\n\nShow the code\ndf<-data.frame(sample=seq(0,1,0.01),\n               density=dbeta(x=seq(0,1,0.01),shape1=45,shape2=55))\ndf %>% ggplot(aes(x=sample,y=density))+\n  geom_line()+\n  ggtitle(\"Density function\")\n\n\nLets imagine that i receive 60% of votes in ellection pole. I can assume that binomial distribution is well suited for generative model responsible for how many votes I am geting. Then I may ask myself: **How probable would be obtaining such a results (60%) of votes under different succes_rate (paramter of Binomial distribution).\n\n\nShow the code\ndf<-data.frame(likelihood=dbinom(x=6,size=10,prob=seq(0,1,0.1)), \n               parameter_p=seq(0,1,0.1))\n\ndf %>% ggplot(aes(x=parameter_p,y=likelihood))+\n  geom_line()+\n  ggtitle(\"Likelihood distribution over different succes_rate parameters\")\n\n\nThe likelihood function summarizes the likelihood of observing polling data X under different values of the underlying support parameter p. Thus, the likelihood is a function of p that depends upon the observed data X\n\n\nPosterior\nSince I’ve got the prior & likelihood:\n\nprior: let say based on the historical pole % of votes I can count on is described by betha distribution Betha(45.55) –> most probable is geting 45% votes\nlikelihood: is denoting to the most recent data shown above\n\nI can approach now to modeling posterior model of p According to Bayes rules posterior is calculating by:\nposterior = prior * likelihood\nHowever, in more sophisticated model settings, tidy, closed-form solutions to this formula might not exist. Very loosely speaking, the goal here is to send information out to the JAGS program, which will then design an algorithm to sample from the posterior, based on which I will then simulate the posterior.\n\nCompiling rjags model\nBuilt from previous polls & election data, my prior model of is a Beta(,) with shape parameters a=45 and b=55. For added insight into p, I also polled potential voters. The dependence of X, the number of these voters that support you, on p is modeled by the Bin(n,p) distribution.\nIn the completed poll, X=6 of n=10 voters supported you. The next goal is to update my model of in light of these observed polling data! To this end, I will use the rjags package to approximate the posterior model of . This exercise will be break down into the 3 rjags steps: define, compile, simulate.\n\n\nShow the code\nlibrary(rjags)\n\n# DEFINE the model\nvote_model <- \"model{\n    # Likelihood model for X\n    X ~ dbin(p, n)\n    \n    # Prior model for p\n    p ~ dbeta(a, b)\n}\"\n\n# COMPILE the model    \nvote_jags <- jags.model(textConnection(vote_model), \n    data = list(a = 45, b = 55, X = 6, n = 10),\n    inits = list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 100))\n\n# SIMULATE the posterior\nvote_sim <- coda.samples(model = vote_jags, variable.names = c(\"p\"), n.iter = 10000)\n\n# PLOT the posterior\nplot(vote_sim, trace = FALSE)"
  },
  {
    "objectID": "posts/2022-11-11-unit-tests/index.html",
    "href": "posts/2022-11-11-unit-tests/index.html",
    "title": "Unit Tests",
    "section": "",
    "text": "What Unit testing are and how they work?\n\n\nA unit test aims to check whether a part of your code operates in the intended way. Writing them has the following benefits:\n\nReduces bugs when developing new features or when changing the existing functionality\nPrevents unexpected output\nHelps detecting edge cases\nTests can serve as documentation\n\n\n\n\nThe more certainty you want to have that your code works as intended, the more you want to invest in testing.\nEspecially when code is used in production, writing tests can be very valuable and ultimately time-saving.\n\n\n\n\n\nWhen you are exploring a dataset for the first time, you will be less inclined to write them.\n\n\n\n\n\nPytest is a great tool for writing unit tests in Python. It makes writing tests short, easy to read, and provides great output. You can install pytest using pip install pytest and create a folder structure like the following:\nproject/\n├─ src/\n│ ├─ regex.py\n│ ├─ init.py\n├─ test/\n│ ├─ test_regex.py\n│ ├─ init.py\n\n\n\n\n\n\n\n\nIn src/regex.py I insert following python code:\n\n## Help on function extract_money in module src.regex:\n## \n## extract_money(text)\n##     Extract monetary value from string by looking for\n##     a pattern of a digit, followed by 'euro'.\n##     e.g. 5 euro --> 5\n##     Args:\n##         text (str): Text containing monetary value\n##     Returns:\n##         float: The extracted value\n\nWhile test/test_regex.py I hold following code.\n\n## import sys\n## sys.path.insert(0, '/Users/lrabalski1/Desktop/prv/blog/content/post/2022-11-11-unit-tests')\n## \n## \n## from src.regex import extract_money\n## import pytest\n## \n## \n## def test_empty_string():\n##     empty_string = \"\"\n##     extracted_money = extract_money(empty_string)\n##     expected_output = None\n##     assert extracted_money == expected_output\n## \n## def test_money_with_decimal_numbers():\n##     decimal_string = \"5.49 euro\"\n##     extracted_money = extract_money(decimal_string)\n##     assert extracted_money == 5.49\n\nIf the code is localised in the file with suffix “test_” and within folder “test_” then running code bellow from root directory will run test unit test.\n\n## ============================= test session starts ==============================\n## platform darwin -- Python 3.8.12, pytest-7.2.0, pluggy-1.0.0 -- /Users/lrabalski1/miniforge3/envs/everyday_use/bin/python3.8\n## cachedir: .pytest_cache\n## rootdir: /Users/lrabalski1/Desktop/prv/blog_testtt/posts/2022-11-11-unit-tests\n## collecting ... collected 2 items\n## \n## test/test_regex.py::test_empty_string PASSED                             [ 50%]\n## test/test_regex.py::test_money_with_decimal_numbers FAILED               [100%]\n## \n## =================================== FAILURES ===================================\n## _______________________ test_money_with_decimal_numbers ________________________\n## \n##     def test_money_with_decimal_numbers():\n##         decimal_string = \"5.49 euro\"\n##         extracted_money = extract_money(decimal_string)\n## >       assert extracted_money == 5.49\n## E       assert 9.0 == 5.49\n## \n## ../../../blog/content/post/2022-11-11-unit-tests/test/test_regex.py:18: AssertionError\n## =========================== short test summary info ============================\n## FAILED test/test_regex.py::test_money_with_decimal_numbers - assert 9.0 == 5.49\n## ========================= 1 failed, 1 passed in 0.41s ==========================\n\nAs you can see\n\nfirst test went positive, function work well with empty string\nsecond test failed, we got an AssertionError, stating that 9 is not equal to 5.49. It seems that our function only extracted the last digit of 5.49. Without this test, we would have been less likely to catch this error. The function would have returned the value 9, instead of throwing us an error.\n\n\n\n\n\nThe next step is to fix our code and make sure that it now passes the test that failed before, while still passing the first test that we wrote as well. A great thing about unit testing is that it gives you an overview of how your changes affect the project as a whole. The tests will make you aware if any of your code additions cause any unexpected consequences.\nFurthermore, the fact that our initial function did not pass the decimal number test raises the question what other examples might come up that we did not anticipate. Maybe we will get a number separated by a comma, like 5,49 euro. Or the monetary value is formatted as € 5.49,-. Unit testing allows us to quickly check for these cases and some other less-common **edge cases**."
  },
  {
    "objectID": "posts/2022-11-21-ts-forecast/index.html",
    "href": "posts/2022-11-21-ts-forecast/index.html",
    "title": "TS - forecast",
    "section": "",
    "text": "data set source"
  },
  {
    "objectID": "posts/2022-11-21-ts-forecast/index.html#general-exploration-of-ts",
    "href": "posts/2022-11-21-ts-forecast/index.html#general-exploration-of-ts",
    "title": "TS - forecast",
    "section": "general exploration of TS",
    "text": "general exploration of TS\nFirst we plot the data in chronological order. Since we will model this as an AR process, we look to the PACF to set a cutoff on the order of the process\nThe database was collected during 60 days, this is a real database of a Brazilian company of large logistics. Twelve predictive attributes and a target that is the total of orders for daily.\nData looks stationary.\n\n\nShow the code\nplot(df$Banking.orders..2., type='l')\nabline(reg=lm(df$Banking.orders..2.~index(df)))"
  },
  {
    "objectID": "posts/2022-11-21-ts-forecast/index.html#ar",
    "href": "posts/2022-11-21-ts-forecast/index.html#ar",
    "title": "TS - forecast",
    "section": "AR",
    "text": "AR\nAs name suggest autoregression is regression made upon past values. The simplest autoregression model is known as AR(1): \\(y_t=b_0+b_1*y_{t-1}+e_t\\) \\(e_t\\) is changeable within time error with stable variance and mean = 0.\n\nselecting parameter of AR\n\n\nShow the code\npacf(df$Banking.orders..2.)\n\n\n\n\n\nWe can see that the value of the PACF crosses the 5% significance threshold at lag 3. This is consistent with the results from the ar() function available in R’s stats package. ar() automatically chooses the order of an autoregressive model if one is not specified:\n\n\nShow the code\nar(df$Banking.orders..2., method = \"mle\")\n## \n## Call:\n## ar(x = df$Banking.orders..2., method = \"mle\")\n## \n## Coefficients:\n##       1        2        3  \n## -0.1360  -0.2014  -0.3175  \n## \n## Order selected 3  sigma^2 estimated as  1413820146\n\n\nNotice that the ar() function has also provided us with the coefficients for the model. We may, however, want to limit the coefficients. For example, looking at the PACF, we might wonder whether we really want to include a coefficient for the lag – 1 term or whether we should assign that term a mandatory coefficient of 0 given that its PACF value is well below the threshold used for significance. In this case, we can use the arima() function also from the stats package. Here, we demonstrate how to call the function to fit an AR(3), by setting the order parameter to c(3, 0, 0), where 3 refers to the order of the AR component\n\n\nShow the code\nest <- arima(x = df$Banking.orders..2.,order = c(3, 0, 0))\nest\n## \n## Call:\n## arima(x = df$Banking.orders..2., order = c(3, 0, 0))\n## \n## Coefficients:\n##           ar1      ar2      ar3  intercept\n##       -0.1358  -0.2013  -0.3176  79075.350\n## s.e.   0.1299   0.1289   0.1296   2981.124\n## \n## sigma^2 estimated as 1413818670:  log likelihood = -717.42,  aic = 1444.83\n\n\nTo inject prior knowledge or opinion into our model, we can constraint a coefficient to be 0. For example, if we want to constrain the lag – 1 term to remain 0 in our model, we use the following call:\n\n\nShow the code\nest.1 <- arima(x =  df$Banking.orders..2.,order = c(3, 0, 0), \n               fixed = c(0, NA, NA, NA))\n\n\nWe now inspect our model performance on our training data to assess the goodness of fit of our model to this data set. We can do this in two ways. First, we plot the ACF of the residuals (that, is the errors) to see if there is a pattern of self-correlation that our model does not cover.\nPlotting the residuals is quite simple thanks to the output of the arima() function\n\n\nShow the code\nacf(est.1$residuals)\n\n\n\n\n\nShow the code\nest.1\n## \n## Call:\n## arima(x = df$Banking.orders..2., order = c(3, 0, 0), fixed = c(0, NA, NA, NA))\n## \n## Coefficients:\n##       ar1      ar2      ar3  intercept\n##         0  -0.1831  -0.3031  79190.705\n## s.e.    0   0.1289   0.1298   3345.253\n## \n## sigma^2 estimated as 1439790211:  log likelihood = -717.96,  aic = 1443.91\n\n\nNone of the values of the ACF cross the significance threshold.\nWe do not see a pattern of self-correlation here among the residuals (i.e., the error terms). If we had seen such a pattern, we would likely want to return to our original model and consider including additional terms to add complexity to account for the significant autocorrelation of the residuals.\n\n\nforecasting 1 time step ahead\napplying by hand\nwhat’s important there are different type of parametrizations of ARIMA models\nThere are multiple (equivalent) parametrizations for ARIMA models. There’s the one you quote (sometimes called the ARMAX parametrization):\n\\(y_t = \\phi y_{t-1} + c + \\epsilon_t\\)\nAnd the equivalent regression with ARMA errors parametrization:\n\\((y_t - c') = \\phi (y_{t-1} - c') + \\epsilon_t\\)\nThe forecast package uses the second parametrization.\nThe package author explains the difference between the two parametrizations and the rationale for choosing this one on his blog. Mostly, when adding other regressors (other than the constant), this version is easier to interpret. Since the function forecast::Arima allows for other regressors, it makes sense to treat them all in the same way.\nurl\n\n\nShow the code\nest.1$coef['intercept']\n## intercept \n##  79190.71\nest.1$coef['ar1']\n## ar1 \n##   0\nest.1$coef['ar2']\n##        ar2 \n## -0.1831204\nest.1$coef['ar3']\n##        ar3 \n## -0.3031044\n\n\nx<-c(NA,NA,NA,df$Banking.orders..2.)\n\ny_pred = zoo::rollapply(zoo(x),\n               width=3,\n               FUN=function(w){\n                 est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(w[3]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(w[2]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(w[1]-est.1$coef['intercept']),0) \n                 \n                \n                 },\n               partial = FALSE\n               )\n\ny_pred <- c(as.vector(y_pred))\n\n\n\n\nShow the code\nplot(df$Banking.orders..2., type = 'l')\nlines(fitted(est.1,h=1), col = 3, lwd = 5) ## use the forecast package\nlines(y_pred, col = 6, lwd = 2) ## fitted by hand\n\n\n\n\n\nNow let’s think about the quality of the forecast. If we calculate the correlation between the predicted value and the actual value, we get 0.29. This is not bad in some contexts, but remember that sometimes differencing the data will remove what seemed like a strong relationship and replace it with one that is essentially random. This will be the case particularly if the data was not truly stationary when we fit it, so that an unidentified trend masquerades as good model performance when it is actually a trait of the data we should have addressed before modeling.\nWe can difference both the series and the predicted values to see whether the change from one time period to the next is well predicted by the model. Even after differenc‐ ing, our predictions and the data show similar patterns, suggesting our model is a meaningful one\n\n\nShow the code\ncor(y_pred[1:60],df$Banking.orders..2.[1:60])\n## [1] 0.2998776\ncor(diff(y_pred)[1:59],diff(df$Banking.orders..2.)[1:59])\n## [1] 0.2187947\nplot(diff(df$Banking.orders..2.)[1:59],diff(y_pred)[1:59])\n\n\n\n\n\n\n\nForecasting many steps into the future\nLooking back at the original plot of the forecast versus actual values, we see that the main difference between the forecast and the data is that the forecast is less variable than the data.\nIt may predict the direction of the future correctly, but not the scale of the change from one time period to another. This is not a problem per se but rather reflects the fact that forecasts are means of the predicted distributions and so necessarily will have lower variability than sampled data.\nAs you can see in the Figure below, the variance of the prediction decreases with increasing forward horizon. The reason for this—which highlights an important limitation of the model—is that the further forward in time we go, the less the actual data matters because the coefficients for input data look only at a finite previous set of time points (in this model, going back only to lag – 3; i.e., time – 3). The future prediction approaches the mean value of the series as the time horizon grows, and hence the variance of both the error term and of the forecast values shrinks to 0 as the forecast values tend toward the unconditional mean value.\n\n\nShow the code\nplot(df$Banking.orders..2., type = 'l')\nlines(fitted(est.1,h=1), col = 2, lwd = 6) ## use the forecast package\nlines(fitted(est.1,h=4), col = 4, lwd = 4) ## use the forecast package\nlines(fitted(est.1,h=7), col = 3, lwd = 2) ## use the forecast package\n\n\n\n\n\n\n\ncalculating forecast by hand\nBelow how prediction for point 8 in time series in calculcated using window_width=3\n\n\nShow the code\n\nts<-df$Banking.orders..2.\n\n\ny_hat_6_window_1 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(ts[5]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(ts[4]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[3]-est.1$coef['intercept']),0) \n\ny_hat_7_window_2 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(y_hat_6-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(ts[5]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[4]-est.1$coef['intercept']),0) \n## Error in list2(...): object 'y_hat_6' not found\n\ny_hat_8_window_3 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(y_hat_7_window_2-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(y_hat_6-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[5]-est.1$coef['intercept']),0) \n## Error in list2(...): object 'y_hat_7_window_2' not found\n\n\nfitted(est.1,h=3)[8] == y_hat_8_window_3\n## Error in eval(expr, envir, enclos): object 'y_hat_8_window_3' not found\n\n\nBelow show how variance is diminishing continously along broader and broader horizont width.\n\n\nShow the code\n## R\nvar(fitted(est.1, h = 3), na.rm = TRUE)\n## [1] 174870141\nvar(fitted(est.1, h = 5), na.rm = TRUE) \n## [1] 32323722\nvar(fitted(est.1, h = 10), na.rm = TRUE) \n## [1] 1013396\nvar(fitted(est.1, h = 20), na.rm = TRUE) \n## [1] 1176.689"
  },
  {
    "objectID": "posts/2022-11-21-ts-forecast/index.html#ma-model",
    "href": "posts/2022-11-21-ts-forecast/index.html#ma-model",
    "title": "TS - forecast",
    "section": "MA model",
    "text": "MA model\nDo not confuse the MA model (moving average model) with a moving average. They are not the same thing.\nMA models are by definition weakly stationary.\nA moving average model can be expressed similarly to an autoregressive model except that the terms included in the linear equation refer to present and past error terms rather than present and past values of the process itself. So an MA model of order q is expressed as:\n\\(yt = μ +e_t +θ_1 × e_{t-1} +θ_2 × e_{t-2}...+θ_q × e_{t-q}\\)\n\nEconomists talk about these error terms as “shocks” to the system,\nwhile someone with an electrical engineering background could talk about this as a series of impulses and the model itself as a finite impulse response filter, meaning that the effects of any particular impulse remain only for a finite period of time.\n\nThe wording is unimportant, but the concept of many independent events at different past times affecting the current value of the process, each making an individual contribution, is the main idea.\n\nselecting parameter of AR\nWe see significant values at lags 3 and 9, so we fit an MA model with these lags.\n\n\nShow the code\nacf(df$Banking.orders..2.)\n\n\n\n\n\n\n\nShow the code\nma.est = arima(x = df$Banking.orders..2.,\n                     order = c(0, 0, 9),\n  fixed = c(0, 0, NA, rep(0, 5), NA, NA))\nma.est\n## \n## Call:\n## arima(x = df$Banking.orders..2., order = c(0, 0, 9), fixed = c(0, 0, NA, rep(0, \n##     5), NA, NA))\n## \n## Coefficients:\n##       ma1  ma2      ma3  ma4  ma5  ma6  ma7  ma8      ma9  intercept\n##         0    0  -0.4725    0    0    0    0    0  -0.0120  79689.809\n## s.e.    0    0   0.1459    0    0    0    0    0   0.1444   2674.593\n## \n## sigma^2 estimated as 1399698249:  log likelihood = -717.31,  aic = 1442.61\n\n\nNote that the Box.test() input requires us to specify the number of degrees of freedom— that is, how many model parameters were free to be estimated rather than being con‐ strained to a specific value. In this case, the free parameters were the intercept as well as the MA3 and MA9 terms.\nWe cannot reject the null hypothesis that there is no temporal correlation between residual points.\n\n\nShow the code\nBox.test(ma.est$residuals, lag = 10, type = \"Ljung\", fitdf = 3)\n## \n##  Box-Ljung test\n## \n## data:  ma.est$residuals\n## X-squared = 7.6516, df = 7, p-value = 0.3643\n\n\n\n\ncalculating forecast by hand"
  },
  {
    "objectID": "posts/2022-11-21-ts-forecast/index.html#more-complicated-case",
    "href": "posts/2022-11-21-ts-forecast/index.html#more-complicated-case",
    "title": "TS - forecast",
    "section": "more complicated case",
    "text": "more complicated case\n\n\nShow the code\nclass(AirPassengers)\n## [1] \"ts\"\n#This is the start of the time series\nstart(AirPassengers)\n## [1] 1949    1\n#This is the end of the time series\nend(AirPassengers)\n## [1] 1960   12\n#The cycle of this time series is 12months in a year\nfrequency(AirPassengers)\n## [1] 12\nsummary(AirPassengers)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   104.0   180.0   265.5   280.3   360.5   622.0\n\n\n\n\nShow the code\n#The number of passengers are distributed across the spectrum\nplot(AirPassengers)\n#This will plot the time series\nabline(reg=lm(AirPassengers~time(AirPassengers)))\n\n\n\n\n\nShow the code\n# This will fit in a line\n\n\n\n\nShow the code\ncycle(AirPassengers)\n##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949   1   2   3   4   5   6   7   8   9  10  11  12\n## 1950   1   2   3   4   5   6   7   8   9  10  11  12\n## 1951   1   2   3   4   5   6   7   8   9  10  11  12\n## 1952   1   2   3   4   5   6   7   8   9  10  11  12\n## 1953   1   2   3   4   5   6   7   8   9  10  11  12\n## 1954   1   2   3   4   5   6   7   8   9  10  11  12\n## 1955   1   2   3   4   5   6   7   8   9  10  11  12\n## 1956   1   2   3   4   5   6   7   8   9  10  11  12\n## 1957   1   2   3   4   5   6   7   8   9  10  11  12\n## 1958   1   2   3   4   5   6   7   8   9  10  11  12\n## 1959   1   2   3   4   5   6   7   8   9  10  11  12\n## 1960   1   2   3   4   5   6   7   8   9  10  11  12\n\n#This will print the cycle across years.\n\nplot(aggregate(AirPassengers,FUN=mean))\n\n\n\n\n\nShow the code\n#This will aggregate the cycles and display a year on year trend\n\nboxplot(AirPassengers~cycle(AirPassengers))\n\n\n\n\n\nShow the code\n#Box plot across months will give us a sense on seasonal effect\n\n\n\nImportant Inferences\n\nThe year on year trend clearly shows that the #passengers have been increasing without fail.\nThe variance and the mean value in July and August is much higher than rest of the months.\nEven though the mean value of each month is quite different their variance is small. Hence, we have strong seasonal effect with a cycle of 12 months or less.\n\nExploring data becomes most important in a time series model – without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.\n\n\nSolving unstationary issues\nWe know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, let’s test the resultant series.\n\n\nShow the code\ntseries::adf.test(diff(log(AirPassengers)), alternative=\"stationary\", k=0)\n## \n##  Augmented Dickey-Fuller Test\n## \n## data:  diff(log(AirPassengers))\n## Dickey-Fuller = -9.6003, Lag order = 0, p-value = 0.01\n## alternative hypothesis: stationary\n\nlog(AirPassengers)\n##           Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n## 1949 4.718499 4.770685 4.882802 4.859812 4.795791 4.905275 4.997212 4.997212\n## 1950 4.744932 4.836282 4.948760 4.905275 4.828314 5.003946 5.135798 5.135798\n## 1951 4.976734 5.010635 5.181784 5.093750 5.147494 5.181784 5.293305 5.293305\n## 1952 5.141664 5.192957 5.262690 5.198497 5.209486 5.384495 5.438079 5.488938\n## 1953 5.278115 5.278115 5.463832 5.459586 5.433722 5.493061 5.575949 5.605802\n## 1954 5.318120 5.236442 5.459586 5.424950 5.455321 5.575949 5.710427 5.680173\n## 1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n## 1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n## 1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n## 1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n## 1959 5.886104 5.834811 6.006353 5.981414 6.040255 6.156979 6.306275 6.326149\n## 1960 6.033086 5.968708 6.037871 6.133398 6.156979 6.282267 6.432940 6.406880\n##           Sep      Oct      Nov      Dec\n## 1949 4.912655 4.779123 4.644391 4.770685\n## 1950 5.062595 4.890349 4.736198 4.941642\n## 1951 5.214936 5.087596 4.983607 5.111988\n## 1952 5.342334 5.252273 5.147494 5.267858\n## 1953 5.468060 5.351858 5.192957 5.303305\n## 1954 5.556828 5.433722 5.313206 5.433722\n## 1955 5.743003 5.613128 5.468060 5.627621\n## 1956 5.872118 5.723585 5.602119 5.723585\n## 1957 6.001415 5.849325 5.720312 5.817111\n## 1958 6.001415 5.883322 5.736572 5.820083\n## 1959 6.137727 6.008813 5.891644 6.003887\n## 1960 6.230481 6.133398 5.966147 6.068426\n\n\nWe see that the series is stationary enough to do any kind of time series modelling.\nNext step is to find the right parameters to be used in the ARIMA model. We already know that the ‘d’ component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series:\n\n\nShow the code\nacf(diff(log(AirPassengers)))\n\n\n\n\n\nShow the code\n\n\npacf(diff(log(AirPassengers)))\n\n\n\n\n\nClearly, ACF plot cuts off after the first lag. Hence, we understood that value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.\n\n\nShow the code\n(fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))\n## \n## Call:\n## arima(x = log(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, \n##     1, 1), period = 12))\n## \n## Coefficients:\n##           ma1     sma1\n##       -0.4018  -0.5569\n## s.e.   0.0896   0.0731\n## \n## sigma^2 estimated as 0.001348:  log likelihood = 244.7,  aic = -483.4\n\n\nLet’s fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :\n\n\nShow the code\n(fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))\n## \n## Call:\n## arima(x = log(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, \n##     1, 1), period = 12))\n## \n## Coefficients:\n##           ma1     sma1\n##       -0.4018  -0.5569\n## s.e.   0.0896   0.0731\n## \n## sigma^2 estimated as 0.001348:  log likelihood = 244.7,  aic = -483.4\n\npred <- predict(fit, n.ahead = 10*12)\n\nts.plot(AirPassengers,2.718^pred$pred, log = \"y\", lty = c(1,3))"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html",
    "title": "TS - missing data imputation & Smoothing",
    "section": "",
    "text": "Show the code\nrequire(zoo)\nrequire(data.table)\nlibrary(dplyr)\nlibrary(lubridate)\n\nunemp <- fread(paste0(data_file_path,\"bezrobocie_USA.csv\")) %>% data.table::melt( id.vars='Year',\n                                                           variable.name = \"months\",\n                                                           value.name='UNRATE') %>% left_join(\n  data.frame(month_nr=c(1:12),\n             months= c(\"Jan\",\"Feb\",\"Mar\",\n                       \"Apr\",\"May\",\"Jun\",\n                       \"Jul\",\"Aug\",\"Sep\",\n                       \"Oct\",\"Nov\",\"Dec\"))\n) %>% mutate(DATE=as_date('0000-01-01',format = '%Y-%m-%d')+years(as.numeric(Year)) + months(month_nr-1)) \n\nhead(unemp)\n##    Year months UNRATE month_nr       DATE\n## 1: 1948    Jan    3.4        1 1948-01-01\n## 2: 1949    Jan    4.3        1 1949-01-01\n## 3: 1950    Jan    6.5        1 1950-01-01\n## 4: 1951    Jan    3.7        1 1951-01-01\n## 5: 1952    Jan    3.2        1 1952-01-01\n## 6: 1953    Jan    2.9        1 1953-01-01\n\n\nunemp = unemp[, DATE := as.Date(DATE)][!is.na(UNRATE),.(DATE, UNRATE)]\nsetkey(unemp, DATE)\n\n## Creating dataset with random missing values\nrand.unemp.idx <- sample(1:nrow(unemp), .1*nrow(unemp))\nrand.unemp <- unemp[-rand.unemp.idx]\n\n## Creating dataset with systematical missing values, appearing in month with highest unemployment rate\nhigh.unemp.idx <- which(unemp$UNRATE > 8)\nhigh.unemp.idx <- sample(high.unemp.idx, .5 * length(high.unemp.idx))\nbias.unemp <- unemp[-high.unemp.idx]\n\n\n## to identyfy missing data I wil use rolling joins tool from data.table package    \nall.dates <- seq(from = unemp$DATE[1], to = tail(unemp$DATE, 1), by = \"months\")\nrand.unemp = rand.unemp[J(all.dates), roll=FALSE]\nbias.unemp = bias.unemp[J(all.dates), roll=FALSE]\n\n## forward filling\nrand.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]\nbias.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]\n\n## Mean moving average with use of lookahead phenomen\nrand.unemp[, impute.rm.lookahead := rollapply(data=c(UNRATE,NA, NA), width=3,\n          FUN= function(x) {\n                         if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)\n                         })]         \nbias.unemp[, impute.rm.lookahead := rollapply(c(UNRATE, NA,NA), 3,\n            FUN= function(x) {\n                         if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)\n                         })]         \n\n\n\n\n\n## Mean moving average withou use of lookahead phenomen\nrand.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,\n             function(x) {\n                         if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)\n                         })]         \nbias.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,\n             function(x) {\n                         if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)\n                         })]    \n\n\n\n\n\n## linear interpolation fullfilling NA with linear interpolation between two data points\nrand.unemp[, impute.li := na.approx(UNRATE, maxgap=Inf)]\nbias.unemp[, impute.li := na.approx(UNRATE)]\n\nzz <- c(NA, 9, 3, NA, 3, 2,NA,5,6,10,NA,NA,NA,0)\nna.approx(zz, na.rm = FALSE, maxgap=2)\n##  [1]   NA  9.0  3.0  3.0  3.0  2.0  3.5  5.0  6.0 10.0   NA   NA   NA  0.0\nna.approx(zz, na.rm = FALSE, maxgap=Inf)\n##  [1]   NA  9.0  3.0  3.0  3.0  2.0  3.5  5.0  6.0 10.0  7.5  5.0  2.5  0.0\nna.approx(zz,xout=11, na.rm = FALSE, maxgap=Inf)\n## [1] 7.5\n\n\n\n\n\n## Using root mean square error to compare methods\nprint(rand.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),\n             .SDcols = c(\"impute.ff\", \"impute.rm.nolookahead\", \"impute.rm.lookahead\", \"impute.li\")])\n##    impute.ff impute.rm.nolookahead impute.rm.lookahead  impute.li\n## 1: 0.2147439             0.2275167          0.02948718 0.09668662\n\nprint(bias.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),\n             .SDcols = c(\"impute.ff\", \"impute.rm.nolookahead\", \"impute.rm.lookahead\", \"impute.li\")])\n##    impute.ff impute.rm.nolookahead impute.rm.lookahead impute.li\n## 1: 0.2633296             0.2248878          0.01550786 0.1327443"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html#metrics",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html#metrics",
    "title": "TS - missing data imputation & Smoothing",
    "section": "Metrics",
    "text": "Metrics\n\nautocorelation\nAutocorelation is measuring the direction of change basing on one point. Since the point on sinusoid close to each other have similar value this autocorelation is high if measuring on distance of pi /6 (0.52). In case of distance of 1 pi value is just opposite so ACF is equal -1.\n\n\nShow the code\nrequire(data.table)\n## R\nx <- 1:100\ny <- sin(x * pi /6)\nplot(y, type = \"b\")\n\n\n\n\n\nShow the code\nacf(y)\n\n\n\n\n\nShow the code\n\n## R\ncor(y, shift(y, 1), use = \"pairwise.complete.obs\")\n## [1] 0.870187\ncor(y, shift(y, 2), use = \"pairwise.complete.obs\") \n## [1] 0.5111622"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html#visualisation",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html#visualisation",
    "title": "TS - missing data imputation & Smoothing",
    "section": "Visualisation",
    "text": "Visualisation\nAllows for visualising multiple micro time series within dataset. It is called Gant chart,\n\n\nShow the code\nrequire(timevis)\nrequire(data.table)\ndonations <- fread(paste0(data_file_path,\"donations.csv\"))\nd <- donations[, .(min(timestamp), max(timestamp)), user]\nnames(d) <- c(\"content\", \"start\", \"end\")\nd <- d[start != end]\ntimevis(d[sample(1:nrow(d), 20)])\n\n\n\n\n+\n-\n\n\n\n\n\n\npartial-autocorelation\nPartial autocorelation shows which point have informational meaning and which simple derives from harmonical periods of time. For seasonal, wihtout noise process, PACF show which correlation for given delay, are the true ones, eliminating redunduntion.It helps to aproximate how much data do we need to poses to apply sufficient window for given time scale.\n\n\nShow the code\n## R\ny <- sin(x * pi /6)\nplot(y[1:30], type = \"b\") \n\n\n\n\n\nShow the code\npacf(y)"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html#simulation",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html#simulation",
    "title": "TS - missing data imputation & Smoothing",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html#smoothing",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html#smoothing",
    "title": "TS - missing data imputation & Smoothing",
    "section": "Smoothing",
    "text": "Smoothing\nSmoothing is commonelly used forecasting method. Smoothed time series can be used as zero hypothesis to for testing more sophisticated methods.\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nunemp = r.unemp\n#unemp.index = unemp.DATE\n\ndf = unemp.copy()\ndf = df[((df.DATE >=pd.to_datetime('2014-01-01')) & (df.DATE < pd.to_datetime('2019-01-01')))]\n## /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.  In a future version these will be considered non-comparable.Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n##   result = libops.scalar_compare(x.ravel(), y, op)\ndf = df.rename(columns={\"UNRATE\": \"data\"})\n#df.reset_index(drop=True, inplace=True)\n\n\ntrain = df[['data']].iloc[:-12, :]\ntest = df[['data']].iloc[-12:, :]\n# train.index = pd.to_datetime(train.index)\n# test.index = pd.to_datetime(test.index)\n## We can use the pandas.DataFrame.ewm() function to calculate the exponentially weighted moving average for a certain number of previous periods.\n\n\n\nmoving average\nAn improvement over simple average is the average of n last points. Obviously the thinking here is that only the recent values matter. Calculation of the moving average involves what is sometimes called a “sliding window” of size n:\n\n\nShow the code\n\ndef average(series):\n    return float(sum(series))/len(series)\n\n# moving average using n last points\ndef moving_average(series, n):\n    return average(series[-n:])\n\nmoving_average(train.data,4)\n## 4.2\n\n\n\n\nWeighted Moving Average\nA weighted moving average is a moving average where within the sliding window values are given different weights, typically so that more recent points matter more.\nInstead of selecting a window size, it requires a list of weights (which should add up to 1). For example if we picked [0.1, 0.2, 0.3, 0.4] as weights, we would be giving 10%, 20%, 30% and 40% to the last 4 points respectively. In Python:\n\n\nShow the code\n# weighted average, weights is a list of weights\ndef weighted_average(series, weights):\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series[-n-1] * weights[n]\n    return result\n  \nweights = [0.1, 0.15, 0.25, 0.5]\nweighted_average(train.data.values, weights)\n\n## 4.16\n\n\n\n\nexponentially weightening\nThe exponentially weighted function is calculated recursively:\n\\[\\begin{split}\\begin{split}\ny_0 &= x_0\\\\\ny_t &= \\alpha x_t + (1 - \\alpha) y_{t-1} ,\n\\end{split}\\end{split}\\]\nwhere alpha is smoothing factor \\(0 < \\alpha \\leq 1\\) . The higher the α, the faster the method “forgets”.\nThere is an aspect of this method that programmers would appreciate that is of no concern to mathematicians: it’s simple and efficient to implement. Here is some Python. Unlike the previous examples, this function returns expected values for the whole series, not just one point.\n\n\nShow the code\n# given a series and alpha, return series of smoothed points\ndef exponential_smoothing(series, alpha):\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n\n\nres_exp_smooth8 = exponential_smoothing(train.data.values, alpha=0.8)\nres_exp_smooth5 = exponential_smoothing(train.data.values, alpha=0.5)\nres_exp_smooth2 = exponential_smoothing(train.data.values, alpha=0.2)\n\n\n\n\nConclusion\nI showed some basic forecasting methods: moving average, weighted moving average and, finally, single exponential smoothing. One very important characteristic of all of the above methods is that remarkably, they can only forecast a single point. That’s correct, just one.\n\n\nDouble exponential smoothing\na.k.a Holt Method\nIn case of forecasting simple exponential weightening isn’t giving good results for data posessing longterm trend. For this purpose it is good to apply method aimed for data with trend (Holt) or with trend and seasonality (Holt-Winter).\nDouble exponential smoothing is nothing more than exponential smoothing applied to both level and trend.\n\n\nShow the code\n\n# given a series and alpha, return series of smoothed points\ndef double_exponential_smoothing(series, alpha, beta):\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # we are forecasting\n          value = result[-1]\n        else:\n          value = series[n]\n        last_level= level\n        level =  alpha*value + (1-alpha)*(last_level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\n\nres_double_exp_smooth_alpha_9_beta9=double_exponential_smoothing(train.data.values, alpha=0.9, beta=0.9)\n\n\n\n\nTriple Exponential Smoothing\na.k.a Holt-Winters Method\n\nInitial Trend\nFor double exponential smoothing we simply used the first two points for the initial trend. With seasonal data we can do better than that, since we can observe many seasons and can extrapolate a better starting trend. The most common practice is to compute the average of trend averages across seasons.\n\n\nShow the code\n\ndef initial_trend(series, slen):\n    sum = 0.0\n    for i in range(slen):\n        sum += float(series[i+slen] - series[i]) / slen\n    return sum / slen\n\nres_initial = initial_trend(train.data.values,12)\nres_initial\n## -0.07361111111111113\n\n\nThe value of -0.074 can be interpreted that unemployment rate between first two years change on average by -0.074 between each pair of the same month.\n\n\nInitial Seasonal Components\nThe situation is even more complicated when it comes to initial values for the seasonal components. Briefly, we need to\n\ncompute the average level for every observed season (in our case YEAR) we have,\ndivide every observed value by the average for the season it’s in\nand finally average each of these numbers across our observed seasons.\n\n\n\nShow the code\ndef initial_seasonal_components(series, slen):\n    seasonals = {}\n    season_averages = []\n    n_seasons = int(len(series)/slen)\n    # compute season averages\n    for j in range(n_seasons):\n        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))\n    # compute initial values\n    for i in range(slen):\n        sum_of_vals_over_avg = 0.0\n        for j in range(n_seasons):\n            sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]\n        seasonals[i] = sum_of_vals_over_avg/n_seasons\n    return seasonals\n\ninitial_seasonal_components(train.data.values,12)\n## {0: 0.2833333333333332, 1: 0.2583333333333331, 2: 0.20833333333333304, 3: 0.10833333333333317, 4: 0.10833333333333339, 5: -0.01666666666666683, 6: -0.04166666666666696, 7: -0.04166666666666674, 8: -0.11666666666666714, 9: -0.216666666666667, 10: -0.21666666666666679, 11: -0.3166666666666669}\n\n\nSeasonal values we can interpret as average distance value from seasonal average. We can see that January {0} is on higher than average and December value {11} is lower than average. We can see that those month differ from each other exactly with the power of those values\n\n\nShow the code\ndf[pd.to_datetime(df.DATE).dt.month.isin([1,12])]\n##            DATE  data\n## 792  2014-01-01   6.6\n## 803  2014-12-01   5.6\n## 804  2015-01-01   5.7\n## 815  2015-12-01   5.0\n## 816  2016-01-01   4.8\n## 827  2016-12-01   4.7\n## 828  2017-01-01   4.7\n## 839  2017-12-01   4.1\n## 840  2018-01-01   4.0\n## 851  2018-12-01   3.9\n\n\n\n\nShow the code\ndef triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):\n    result = []\n    seasonals = initial_seasonal_components(series, slen)\n    for i in range(len(series)+n_preds):\n        if i == 0: # initial values\n            smooth = series[0]\n            trend = initial_trend(series, slen)\n            result.append(series[0])\n            continue\n        if i >= len(series): # we are forecasting\n            m = i - len(series) + 1\n            result.append((smooth + m*trend) + seasonals[i%slen])\n        else:\n            val = series[i]\n            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)\n            trend = beta * (smooth-last_smooth) + (1-beta)*trend\n            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]\n            result.append(smooth+trend+seasonals[i%slen])\n    return result\n\nres_triple_exp_smooth = triple_exponential_smoothing(train.data.values, 12, 0.7, 0.02, 0.9, 10)\n\n\nA Note on α, β and γ\nYou may be wondering from where values 0.7, 0.02 and 0.9 for α, β and γ It was done by way of trial and error: simply running the algorithm over and over again and selecting the values that give you the smallest SSE. This process is known as fitting.\nThere are more efficient methods at zooming in on best values. One good algorithm for this is Nelder-Mead, which is what tgres uses.\n\n\n\nfitting data\n\n\nShow the code\nres = [res_exp_smooth8,res_exp_smooth5,res_exp_smooth2,res_double_exp_smooth_alpha_9_beta9,res_triple_exp_smooth]\nRMSE = []\ni=1\nfor i in range(len(res)):\n  RMSE.append(np.sqrt(np.mean(np.square((train.data.values[0:48]- res[i][0:48])))))\nRMSE\n## [0.029444314294186542, 0.08240165108170797, 0.2272843505233408, 0.10916100840899878, 0.06909009711283208]\n\n\nIn case of fitting smoothed data to raw data, the best fit possess single exponenetial smoothing method with alpha =0.8 (putting higher weight on most recent data). This is exactly what could be expected. Is it then the best forecasting method for my data?\nObviously not.\nSince all method take data point from time t for estimating smoothed value for time t such a models are not forecasting one’s. We are dealing here with lookahead problem. In order to predict we are using data which shouldn’t be available at the moment of making prediction.\nOut of three methods prediction capabilities posses Holt method (using trend to linearly predict further data points) and Holt-Winter method (using trend and seasonality to predict further data points).\n\n\nplot\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport datetime\nplt.style.use('Solarize_Light2')\n\n   \nplt.clf()\nfig = plt.figure(figsize=(5,8))\n# f.set_figwidth(10)#inches\n# f.set_figheight(20)#inches\nax1 = fig.add_subplot(5, 1, 1) \nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth8, label='exp_smooth_alpha_0.8')\nax2 =fig.add_subplot(5, 1, 2)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth5, label='exp_smooth_alpha_0.5')\nax3 =fig.add_subplot(5, 1, 3)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth2, label='exp_smooth_alpha_0.2')\nax4 =fig.add_subplot(5, 1, 4)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_double_exp_smooth_alpha_9_beta9, label='res_double_exp_smooth_alpha_9_beta9')\nax5 =fig.add_subplot(5, 1, 5)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_triple_exp_smooth, label='res_triple_exp_smooth')\nax1.set_title('raw data vs exponential forecast')\nax1.legend(loc=\"upper left\")\nax2.legend(loc=\"upper left\")\nax3.legend(loc=\"upper left\")\nax4.legend(loc=\"upper left\")\nax5.legend(loc=\"upper left\")\nax1.sharex(ax5)\nax2.sharex(ax5)\nax3.sharex(ax5)\nax4.sharex(ax5)\n\nfig.tight_layout()\nfig.savefig('index_files/figure-html/unnamed-chunk-15-1.png', bbox_inches='tight')\n\nplt.show()"
  },
  {
    "objectID": "posts/2022-11-10 TS_missing_data_imputation/index.html#regression",
    "href": "posts/2022-11-10 TS_missing_data_imputation/index.html#regression",
    "title": "TS - missing data imputation & Smoothing",
    "section": "regression",
    "text": "regression\n\nautoregression models\nAs name suggest autoregression is regression made upon past values. The simplest autoregression model is known as AR(1): \\(y_t=b_0+b_1*y_{t-1}+e_t\\) \\(e_t\\) is changeable within time error with stable variance and mean = 0."
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html",
    "title": "Grow of Metropolis",
    "section": "",
    "text": "Aims:\n\nFIlter out the data localisation where people are supposed to live in (cities; villages etc)\nSelect cities which are metropolis based on population on external source\nWrite a script which will ascribe metropolis to a city. Additionally model a process of consecutive cities which would be ascribed to the city bearing in mind that city will grow thanks to this process.\n\nData Source\nDocumentation"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#preparing-environment",
    "title": "Grow of Metropolis",
    "section": "Preparing environment",
    "text": "Preparing environment\nSetting default markdown option responsible of code chunk behaviour.\nFirstly I choose prefered python environment on which I have installed useful libraries.\n\n\nShow the code\nlibrary(reticulate)\nmyenvs=conda_list()\nenvname=myenvs$name[3]\nuse_condaenv(envname, required = TRUE)\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\")\nreticulate::py_config()\n## python:         /Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\n## libpython:      /Users/lrabalski1/miniforge3/envs/everyday_use/lib/libpython3.8.dylib\n## pythonhome:     /Users/lrabalski1/miniforge3/envs/everyday_use:/Users/lrabalski1/miniforge3/envs/everyday_use\n## version:        3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17)  [Clang 11.1.0 ]\n## numpy:          /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/numpy\n## numpy_version:  1.21.4\n## \n## NOTE: Python version was forced by RETICULATE_PYTHON\n\n\nBellow I present two function:\n\nradius - function which based on city population is calculating a radius within which city is able to absorb cities from this range\n_calcualate_metrocity_impact - calculate impact on metrocity on given city\n\n\n\nShow the code\ndef radius(population):\n    METRO_CITY_POPULATION_CONSTANT = -1/1443000\n    MIN_METRO_CITY_RADIUS = 10\n    MAX_METRO_CITY_RADIUS = 100 - MIN_METRO_CITY_RADIUS\n    return MIN_METRO_CITY_RADIUS + MAX_METRO_CITY_RADIUS * (1 - np.exp(METRO_CITY_POPULATION_CONSTANT *  population))\n\ndef _calcualate_metrocity_impact(max_radius, distance_to_metro_city):\n    METRO_CITY_POWER_CONSTANT = -1.4\n    impact = np.exp(METRO_CITY_POWER_CONSTANT  * distance_to_metro_city / max_radius)\n    return impact\n\n\nFunction responsible for calculating distances between 2 points on earth surface.\n\n\nShow the code\n#https://towardsdatascience.com/heres-how-to-calculate-distance-between-2-geolocations-in-python-93ecab5bbba4\ndef haversine_distance_code(lat1, lon1, lat2, lon2):\n    r = 6371\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2 - lat1)\n    delta_lambda = np.radians(lon2 - lon1)\n    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n    return np.round(res, 2)\n\n\n\n\nShow the code\ndf= pd.read_csv(file_path_name, sep=\"\\t\", \n                names=['geonameid','name','asciiname','alternatenames','latitude','longitude','feature class','feature code','country code','cc2','admin1 code','admin2 code','admin3 code','admin4 code','population','elevation','dem','timezone','modification date',])\n\n\nDataset readme states that column feature classes contains level information of:\nA: country, state, region,…\nH: stream, lake, …\nL: parks,area, …\nP: city, village,…\nR: road, railroad\nS: spot, building, farm\nT: mountain,hill,rock,…\nU: undersea\nV: forest,heath,…\nWe will be interested on object of level P, and maybe A.\n\n\nShow the code\nimport requests\n\nurl = 'http://www.geonames.org/export/codes.html'\nhtml = requests.get(url).content\ndf_list = pd.read_html(html)\ndf_legend = df_list[-1]\ndf_legend = df_legend.rename(columns={df_legend.columns[0]: 'feature code',\n                                     df_legend.columns[1]: 'short  descr',\n                                     df_legend.columns[2]: 'long descr'})\ndf_legend = pd.merge(df[['feature code','feature class']].drop_duplicates(),df_legend, on='feature code')\ndf_legend\n##     feature code  ...                                         long descr\n## 0           PPLQ  ...                                                NaN\n## 1            STM  ...  a body of running water moving to a lower leve...\n## 2           HLLS  ...  rounded elevations of limited extent rising ab...\n## 3            CNL  ...                          an artificial watercourse\n## 4            PPL  ...  a city, town, village, or other agglomeration ...\n## ..           ...  ...                                                ...\n## 186          BCN  ...                 a fixed artificial navigation mark\n## 187         HSEC  ...  a large house, mansion, or chateau, on a large...\n## 188          RES  ...  a tract of public land reserved for future use...\n## 189         STNR  ...  a facility for producing and transmitting info...\n## 190         BLDA  ...  a building containing several individual apart...\n## \n## [191 rows x 4 columns]\n\n\n\n\nShow the code\ndf = df[df['feature class'].isin(['P','A'])]\ndf_check = pd.merge(df,df_legend, on=['feature code','feature class'])\n\n# sorting by the biggest objects I can see that those are cities\ndf_check[df_check['feature class']=='P'].sort_values('population', ascending=False).head(5)\n\n# administrative object located in object of level P\ndf_check[df_check['feature class']=='A'].sort_values('population', ascending=False).head(5)\n\n#z tej tabeli wynika, ze PPLX to sekcje zaludnionych miejsc, sa to ulice, dzielnice, wiec wykluczam, sa czescia miast\ndf_check[['feature class','feature code', 'short  descr']].drop_duplicates()\n\n#finalnie musze skupic sie na na obu klasach, jednoczesnie usuwajac duplikaty\ndf = df[(df['feature class'].isin(['P'])) & \n        (df.population != 0) & \n        ~(df['feature code'].isin(['PPLX']))].drop_duplicates('name')\n\n\ndf.index.name = 'city_id'\ndf.reset_index(inplace=True)\n\n\n\n\nShow the code\ndf.groupby(['feature class','feature code']).agg({'population': ['mean', 'min', 'max']})\n##                               population                  \n##                                     mean      min      max\n## feature class feature code                                \n## P             PPL           3.238169e+03        5   244969\n##               PPLA          3.652322e+05   118433   768755\n##               PPLA2         3.543081e+04     5696   226794\n##               PPLA3         5.619329e+03      110   248125\n##               PPLC          1.702139e+06  1702139  1702139\n##               PPLF          1.750000e+02      175      175"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-in-poland",
    "title": "Grow of Metropolis",
    "section": "Metropolis in Poland",
    "text": "Metropolis in Poland\nwikipedia Warszawa, Katowice, Kraków, Łódź, Trójmiasto, Poznań, Wrocław, Bydgoszcz, Szczecin, Lublin.\n\n\nShow the code\ndf_metropolie = df[df.name.isin(\n    ['Warsaw','Katowice','Kraków','Łódź',\n     'Gdańsk','Gdynia',#'Trójmiasto',\n     'Poznań','Wrocław','Bydgoszcz','Szczecin','Lublin'])][\n    ['city_id','name','population','latitude','longitude']]\ndf_metropolie['iteration']=0 \n#df_metropolie['radius'] = radius(df_metropolie['population'])\ndf_metropolie=df_metropolie.add_suffix('_metro')\ndf_metropolie\n##       city_id_metro name_metro  ...  longitude_metro  iteration_metro\n## 188            3191     Warsaw  ...         21.01178                0\n## 917           12873     Lublin  ...         22.56667                0\n## 1734          25287    Wrocław  ...         17.03333                0\n## 1916          27732   Szczecin  ...         14.55302                0\n## 2279          32047     Poznań  ...         16.92993                0\n## 2654          36976       Łódź  ...         19.47395                0\n## 2774          38634     Kraków  ...         19.93658                0\n## 2889          40299   Katowice  ...         19.02754                0\n## 3089          43232     Gdynia  ...         18.53188                0\n## 3090          43241     Gdańsk  ...         18.64912                0\n## 3298          45802  Bydgoszcz  ...         18.00762                0\n## \n## [11 rows x 6 columns]"
  },
  {
    "objectID": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "href": "posts/2022-06-01-grow_of_metropolis/index.html#metropolis-absorption-algorithm",
    "title": "Grow of Metropolis",
    "section": "metropolis absorption algorithm",
    "text": "metropolis absorption algorithm\n\nInstruction\n\nstworze id kolumne z indeksem\nzlacze tabele z metropoliami i wszystkimi miastami im do tej pory przypisanymi, wylicze zagregowana ludnosc oraz promien metropoli\ncroos joinuje do kazdego miasta bez przypisanej metropolii tabele z metropolia\nwylicze odleglosc miejscowosci od metropoli i pozbede sie tych wierszy ktore sa poza promieniem\ndla pozostalych miejscowosci wylicze moc metropolii\nzrobie slice max groupujac po id miejscowosci pozostawiajc metropolie wchlaniajaca - tak powstanie tabela incrementalna do ktorej potem bede rbindowal nastepne tego typu tabele\nw obu tabelach powstanie tabele z indeksem mowiacy o n-iteracji z jakiej pochodzi przypisanie miejscowosci do metropolii oraz stan populacji\nwszystko zamkne w lupie while ktory bedzie wykonywany tak dlugo jak zostanie odnotowany przyrost w tabeli incrementalnej\n\n\n\nShow the code\ndf_cities = df[['city_id','name','population','latitude','longitude']]\ndf_cities = df_cities.loc[~df_cities.city_id.isin(df_metropolie.city_id_metro)]\ndf_cities.head(5)\n##    city_id      name  population  latitude  longitude\n## 0       13  Prędocin         536  51.14791   21.32704\n## 1       16     Poraj         266  50.89962   23.99191\n## 2       37    Żyrzyn        1400  51.49918   22.09170\n## 3       41  Żyrardów       41179  52.04880   20.44599\n## 4       42   Żyraków        1400  50.08545   21.39622\n\n\n\n\nwlasciwy algorytm\n\n\nShow the code\ndf_miasta_w_puli =df_cities\ncolumn_names = ['city_id','name','population'] +df_metropolie.columns.values.tolist()\ndf_miasta_wchloniete=pd.DataFrame(columns=column_names)\nstart = True\niteration =0\n\n\n# start funkcji\nwhile start == True:\n    df_metropolie_powiekszone=df_metropolie.append(df_miasta_wchloniete, ignore_index=True)\n    df_metropolie_powiekszone.population = df_metropolie_powiekszone.population.combine_first(df_metropolie_powiekszone.population_metro)\n    \n    df_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n        ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n        {'population':['sum']}).reset_index()\n    df_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\n    df_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n    df_miasta_w_puli['key'] = 1\n    df_metropolie_powiekszone_popul['key'] = 1\n    df_x = pd.merge(df_miasta_w_puli, df_metropolie_powiekszone_popul, on='key', suffixes=('','_y')).drop(\"key\", 1)\n    #calculating distance between two coordinates \n    #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n    distances_km = []\n    for row in df_x.itertuples():\n        distances_km.append(\n            haversine_distance_code( row.latitude, row.longitude ,row.latitude_metro, row.longitude_metro)\n        )\n    df_x['distance_km'] = distances_km\n    df_x = df_x[df_x.radius >= df_x.distance_km]\n    df_x['impact'] = _calcualate_metrocity_impact(df_x.radius,df_x.distance_km)\n    #stwierdzam do ktorej finalnie metropoli miejscowosci zostaje zaliczon\n    idx = df_x.groupby(['name','population'])['impact'].transform(max) == df_x['impact']\n    df_x = df_x[idx]\n    iteration+= 1\n    df_x['iteration_metro']=iteration\n    pre_rows_num=df_miasta_wchloniete.shape[0]\n    df_miasta_wchloniete=df_miasta_wchloniete.append(\n        df_x[column_names], ignore_index=True)\n    #pozbywam sie miast juz wchlonietych\n    indx = df_miasta_w_puli.city_id.isin(df_miasta_wchloniete.city_id)\n    df_miasta_w_puli = df_miasta_w_puli[~indx]\n    if pre_rows_num == df_miasta_wchloniete.shape[0]:\n        start = False\n## <string>:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n## <string>:10: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ndf_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby(\n    ['city_id_metro','name_metro','population_metro','latitude_metro','longitude_metro',]).agg(\n    {'population':['sum']}).reset_index()\ndf_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1)\ndf_metropolie_powiekszone_popul['radius'] = radius(df_metropolie_powiekszone_popul['population'])\n\n\n\n\nShow the code\n#finalne populacje metropoli\ndf_metropolie_powiekszone_popul.head(5)\n\n#przypisanie miast do metropoli wraz numerem iteracji\n##    city_id_metro name_metro  ...  population     radius\n## 0           3191     Warsaw  ...     5167905  97.494601\n## 1          12873     Lublin  ...      531712  37.739146\n## 2          25287    Wrocław  ...     1038518  56.178876\n## 3          27732   Szczecin  ...      589846  40.197589\n## 4          32047     Poznań  ...     1116528  58.484992\n## \n## [5 rows x 7 columns]\ndf_miasta_wchloniete.head(5)\n##   city_id           name  ... longitude_metro iteration_metro\n## 0      41       Żyrardów  ...        21.01178               1\n## 1     189       Zręczyce  ...        19.93658               1\n## 2     215       Żoliborz  ...        21.01178               1\n## 3     291          Złota  ...        21.01178               1\n## 4     339  Zielonki-Wieś  ...        21.01178               1\n## \n## [5 rows x 9 columns]"
  },
  {
    "objectID": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "href": "posts/2022-11-27-monte-carlo-simulation/index.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "A Monte Carlo simulation is a type of computational algorithm that estimates the probability of occurrence of an undeterminable event due to the involvement of random variables. The algorithm relies on repeated random sampling in an attempt to determine the probability. This means simulating an event with random inputs a large number of times to obtain your estimation. You can determine other factors as well, and we will see that in the example. Monte Carlo simulations can be utilized in a broad range of fields spanning from economics, gambling, engineering, energy, and anything in-between. So, no matter what career field you are in, it’s an excellent thing to know about.\n#The Dice Game\nOur simple game will involve two six-sided dice. In order to win, the player needs to roll the same number on both dice. A six-sided die has six possible outcomes (1, 2, 3, 4, 5, and 6). With two dice, there is now 36 possible outcomes (1 and 1, 1 and 2, 1 and 3, etc., or 6 x 6 = 36 possibilities). In this game, the house has more opportunities to win (30 outcomes vs. the player’s 6 outcomes), meaning the house has the quite the advantage.\nLet’s say our player starts with a balance of $1,000 and is prepared to lose it all, so they bet $1 on every roll (meaning both dice are rolled) and decide to play 1,000 rolls. Because the house is so generous, they offer to payout 4 times the player’s bet when the player wins. For example, if the player wins the first roll, their balance increases by $4, and they end the round with a balance of $1,004. If they miraculously went on a 1,000 roll win-streak, they could go home with $5,000. If they lost every round, they could go home with nothing. Not a bad risk-reward ratio… or maybe it is.\n\n\nShow the code\n# Importing Packages\nimport matplotlib.pyplot as plt\nimport random\n\n\n\n\nLet’s define a function that will randomize an integer from 1 to 6 for both dice (simulating a roll). The function will also compare the two dice to see if they are the same. The function will return a Boolean variable, same_num, to store if the rolls are the same or not. We will use this value later to determine actions in our code.\n\n\nShow the code\n# Creating Roll Dice Function\ndef roll_dice():\n    die_1 = random.randint(1, 6)\n    die_2 = random.randint(1, 6)\n    # Determining if the dice are the same number\n    if die_1 == die_2:\n        same_num = True\n    else:\n        same_num = False\n    return same_num\n\n\nThese are initialized as lists and will be updated at the end of each game.\n\n\nShow the code\n# Inputs\nnum_simulations = 100\nmax_num_rolls = 1000\nbet = 1\n\n# Tracking\nwin_probability = []\nend_balance = []\n\n\n\n\nShow the code\n# Creating Figure for Simulation Balances\nfig = plt.figure()\nplt.title(\"Monte Carlo Dice Game [\" + str(num_simulations) + \"simulations]\")\nplt.xlabel(\"Roll Number\")\nplt.ylabel(\"Balance [$]\")\nplt.xlim([0, max_num_rolls])\n\n\n(0.0, 1000.0)\n\n\nOnce the number of rolls hits 1,000, we can calculate the player’s win probability as the number of wins divided by the total number of rolls. We can also store the ending balance for the completed game in the tracking variable end_balance. Finally, we can plot the num_rolls and balance variables to add a line to the figure we defined earlier.\n\n\n\n\n\nShow the code\n# For loop to run for the number of simulations desired\nfor i in range(num_simulations):\n    balance = [1000]\n    num_rolls = [0]\n    num_wins = 0    # Run until the player has rolled 1,000 times\n    while num_rolls[-1] < max_num_rolls:\n        same = roll_dice()        # Result if the dice are the same number\n        if same:\n            balance.append(balance[-1] + 4 * bet)\n            num_wins += 1\n        # Result if the dice are different numbers\n        else:\n            balance.append(balance[-1] - bet)\n        num_rolls.append(num_rolls[-1] + 1)# Store tracking variables and add line to figure\n    win_probability.append(num_wins/num_rolls[-1])\n    end_balance.append(balance[-1])\n    plt.plot(num_rolls, balance)\n\n\n\n\n\nThe last step is displaying meaningful data from our tracking variables. We can display our figure (shown below) that we created in our for loop. Also, we can calculate and display (shown below) our overall win probability and ending balance by averaging our win_probability and end_balance lists.\n\n\nShow the code\n# Averaging win probability and end balance\noverall_win_probability = sum(win_probability)/len(win_probability)\noverall_end_balance = sum(end_balance)/len(end_balance)# Displaying the averages\nprint(\"Average win probability after \" + str(num_simulations) + \"runs: \" + str(overall_win_probability))\n\n\nAverage win probability after 100runs: 0.16773999999999997\n\n\nShow the code\nprint(\"Average ending balance after \" + str(num_simulations) + \"runs: $\" + str(overall_end_balance))\n\n\nAverage ending balance after 100runs: $838.7\n\n\n\n\n\nThe most important part of any Monte Carlo simulation (or any analysis for that matter) is drawing conclusions from the results. From our figure, we can determine that the player rarely makes a profit after 1,000 rolls. In fact, the average ending balance of our 10,000 simulations is $833.66 (your results may be slightly different due to randomization). So, even though the house was “generous” in paying out 4 times our bet when the player won, the house still came out on top.\nWe also notice that our win probability is about 0.1667, or approximately 1/6. Let’s think about why that might be. Returning back to one of the earlier paragraphs, we noted that the player had 6 outcomes in which they could win. We also noted there are 36 possible rolls. Using these two numbers, we would expect that the player would win 6 out of 36 rolls, or 1/6 rolls, which matches our Monte Carlo prediction. Pretty cool!"
  },
  {
    "objectID": "posts/2022-10-29-graph-dataset/artykuł.html",
    "href": "posts/2022-10-29-graph-dataset/artykuł.html",
    "title": "DGL graph datastructure",
    "section": "",
    "text": "Very simple example below\n\n\nShow the code\n# Each value of the dictionary is a list of edge tuples.\n# Nodes are integer IDs starting from zero. Nodes IDs of different types have\n# separate countings.\nimport torch\nimport dgl\n## /Users/lrabalski1/miniforge3/envs/tensorflow_mac/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n##   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nratings = dgl.heterograph(\n    {('user', '+1', 'movie') : [(0, 0), (0, 1), (1, 0)],\n     ('user', '-1', 'movie') : [(2, 1)]})\nratings\n\n## Graph(num_nodes={'movie': 2, 'user': 3},\n##       num_edges={('user', '+1', 'movie'): 3, ('user', '-1', 'movie'): 1},\n##       metagraph=[('user', 'movie', '+1'), ('user', 'movie', '-1')])\n\n\n\n\nIn order to save graph as object, very convenient method is to use pickle.\n\n\nShow the code\nimport pickle\n\noutput_file = 'builded_graph.pkl'\ndirectory = 'input/'\nwith open(directory + output_file, 'rb') as f:\n    g = pickle.load(f)\n\n\n#loading saved graph\nwith open(directory  + output_file, 'rb') as f:\n    sp_matrix = pickle.load(f)\n\n\nTo create a more realistic heterograph let’s use the ACM dataset. ## Graphs Dataset can be downloaded from here. It’s stored in mat (matrix) object. Within which we can find object/matrices stored in a compresed sparsed format. More about further.\n\n\nShow the code\nimport scipy.io\nimport urllib.request\n\ndata = scipy.io.loadmat(data_file_path+'ACM.mat')\n\n\n\n\n\nMany different ways to store sparsed matrices may be find in scipy documentation.\nFor us most important will be csr_matrix and csc_matrix.\nCSC format is almost identical, except that values are indexed first by column with a column-major order. Usually, the CSC is used when there are more rows than columns. On the contrary, the CSR works better for a ‘wide’ format. So, her is taking CSR as an example here.\nBelow short example how sparsed matrices can be handle with scipy package.\n\n\nShow the code\nimport numpy as np\nfrom scipy.sparse import csr_matrix, csc_matrix\narr = np.array([[0, 0, 0], [0, 0, 1], [1, 2, 0]])\n\narr_csr = csr_matrix(arr)\narr_csc = csc_matrix(arr)\n\nprint(type(arr_csr))\n## <class 'scipy.sparse.csr.csr_matrix'>\nprint(type(arr_csc))\n\n# `CSC` format is almost identical, except that values are indexed first by column with a column-major order. Usually, the `CSC` is used when there are more rows than columns. On the contrary, the `CSR` works better for a ‘wide’ format. So, her is taking CSR as an example here\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint(arr_csr)\n##   (1, 2) 1\n##   (2, 0) 1\n##   (2, 1) 2\nprint(arr_csc)\n\n# however in order to get access ti those indexes need to use method to_coo.\n##   (2, 0) 1\n##   (2, 1) 2\n##   (1, 2) 1\narr_csr.tocoo().row\n## array([1, 2, 2], dtype=int32)\narr_csr.tocoo().col\n\n#Viewing stored data (not the zero items) with the data property\n## array([2, 0, 1], dtype=int32)\nprint(arr_csr.data)\n## [1 1 2]\nprint(arr_csc.data)\n\n#Counting nonzeros with the count_nonzero() method:\n## [1 2 1]\nprint(arr_csr.count_nonzero())\n## 3\nprint(arr_csc.count_nonzero)\n## <bound method _data_matrix.count_nonzero of <3x3 sparse matrix of type '<class 'numpy.int64'>'\n##  with 3 stored elements in Compressed Sparse Column format>>\nprint(arr_csr.toarray())\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\nprint(arr_csc.todense())\n\n## [[0 0 0]\n##  [0 0 1]\n##  [1 2 0]]\n\n\n\n\n\n\n\n\n\nShow the code\nimport scipy.sparse as sp\n\nsp_matrix = data['PvsA']\nprint(type(sp_matrix))\n## <class 'scipy.sparse.csc.csc_matrix'>\nprint('#Papers:', sp_matrix.shape[0])\n## #Papers: 12499\nprint('#Authors:',sp_matrix.shape[1])\n## #Authors: 17431\nprint('#Links:', sp_matrix.nnz)\n\n# ways of populating graph with coo_matrix\n## #Links: 37055\npp_g = dgl.bipartite_from_scipy(sp_matrix, utype='paper', etype='written-by', vtype='author')\n\npp_g.is_homogeneous\n## False\nprint(pp_g.number_of_nodes())\n## 29930\nprint(pp_g.number_of_edges())\n## 37055\nprint(pp_g.successors(3))\n## tensor([ 4295, 13161])\nprint('Node types:', pp_g.ntypes)\n## Node types: ['paper', 'author']\nprint('Edge types:', pp_g.etypes)\n## Edge types: ['written-by']\nprint('Canonical edge types:', pp_g.canonical_etypes)\n## Canonical edge types: [('paper', 'written-by', 'author')]\n\n\nIn order to visualize the interactions (edges) between nodes let use following function.\n\n\nShow the code\nimport pygraphviz as pgv\ndef plot_graph(nxg, plot_name):\n    ag = pgv.AGraph(strict=False, directed=True)\n    for u, v, k in nxg.edges(keys=True):\n        ag.add_edge(u, v, label=k)\n    ag.layout('dot')\n    ag.draw(plot_name+'.png')\n\n\n\n\nShow the code\nplot_graph(nxg=pp_g.metagraph(),plot_name='simple_graph')\n\n\n\n\n\n\nUsing ACM dataset\n\n\nShow the code\nimport torch\n\n# Unfortunately following code no longer works\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : data['PvsA'],\n        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n        ('paper', 'citing', 'paper') : data['PvsP'],\n        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n        ('paper', 'is-about', 'subject') : data['PvsL'],\n        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n    })\n\n# we need to a little bit tweak the code the get the same result as above.\n## Error in py_call_impl(callable, dots$args, dots$keywords): dgl._ffi.base.DGLError: dgl.heterograph no longer supports graph construction from a SciPy sparse matrix, use dgl.from_scipy instead.\nG = dgl.heterograph({\n        ('paper', 'written-by', 'author') : \n          (torch.tensor(data['PvsA'].tocoo().col),torch.tensor(data['PvsA'].tocoo().row )),\n         ('author', 'writing', 'paper') : \n           (torch.tensor(data['PvsA'].tocoo().row) ,torch.tensor(data['PvsA'].tocoo().col)),\n        ('paper', 'citing', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().col),torch.tensor(data['PvsP'].tocoo().row )),\n        ('paper', 'cited', 'paper') : \n          (torch.tensor(data['PvsP'].tocoo().row) ,torch.tensor(data['PvsP'].tocoo().col)),\n        ('paper', 'is-about', 'subject') : \n          (torch.tensor(data['PvsL'].tocoo().col),torch.tensor(data['PvsL'].tocoo().row )),\n        ('subject', 'has', 'paper') : \n          (torch.tensor(data['PvsL'].tocoo().row) ,torch.tensor(data['PvsL'].tocoo().col))\n    })\n  \nplot_graph(nxg=G.metagraph(),plot_name='more_complicated_graph')\n\n\n\n\n\n\n\nOn github repository we can find a method allowing for building graphs using pandas dataphrame.\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.insert(1, repo_directory)\nfrom builder import PandasGraphBuilder\n\nusers=pd.DataFrame(data=range(1,101), columns=['user_id'])\nproducts=pd.DataFrame(data=range(1,50), columns=['product_id'])\ninteractions=pd.DataFrame(data={\n  'user_id': np.random.choice(users.user_id,1000,replace=True),\n  'product_id' :np.random.choice(products.product_id,1000,replace=True)}\n  )\n\n\n\n\ngraph_builder = PandasGraphBuilder()\ngraph_builder.add_entities(users, 'user_id', 'user')\ngraph_builder.add_entities(products, 'product_id', 'product')\n\ngraph_builder.add_binary_relations(interactions, 'user_id','product_id', 'interaction')\ngraph_builder.add_binary_relations(interactions, 'product_id','user_id', 'interaction-by')\n\nprint('starting graph building')\n## starting graph building\ng = graph_builder.build()\n\nplot_graph(nxg=g.metagraph(),plot_name='pandas_graph')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "List of all articles can be found below",
    "section": "",
    "text": "Monte Carlo Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nŁukasz Rąbalski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - forecast\n\n\n\n\n\n\n\nTime Series\n\n\nforecast\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTS - missing data imputation & Smoothing\n\n\n\n\n\n\n\nTime Series\n\n\nSmoothing\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Tests\n\n\n\n\n\n\n\nUnit testing\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nPackage Build\n\n\n\n\n\n\n  \n\n\n\n\nR lib: data.table\n\n\n\n\n\n\n\ndata.table\n\n\n\n\nR data.table\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDGL graph datastructure\n\n\n\n\n\n\n\nDGL\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR_Markdown\n\n\n\n\n\n\n\nRmarkdown\n\n\n\n\nPresentation of R Markdown functionality\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nDescribing statistical methods\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrow of Metropolis\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Łukasz Rąbalski blog",
    "section": "",
    "text": "Hi! My name is Łukasz Rąbalski.\nI hope you will find content served here interesting and useful.\nWhat you may find here:\n- Python / R / SQL algorithm\n- Application showcases of statistical / algorithmical method aimed toward solving real life problems.\n- Explanation of mathematical foundation lying behind above mentioned methods\nEach article has attached Category and one or more tags which makes it easier to find useful content.\nThis page was developed with awesome R package blogdown 👍"
  }
]