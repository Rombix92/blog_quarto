{"title":"TS - forecast","markdown":{"yaml":{"title":"TS - forecast","date":"2022-11-20","tags":[],"categories":["Time Series","forecast"],"toc":true},"headingText":"markdown ----","containsRefs":false,"markdown":"\n\n```{r markdown_parameters, include=FALSE}\nknitr::opts_chunk$set(#fig.width=12, \n                      fig.height=4,\n                       out.width = '100%'\n                      ) \nknitr::opts_chunk$set(include =TRUE, #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\n\n                      warning = FALSE,\n                      message =FALSE,\n                      collapse=TRUE,\n                      error=TRUE\n                      )\noptions(scipen=999)\n```\n\n```{r, include =FALSE}\n\nlibrary(reticulate)\nlibrary(zoo)\nlibrary(forecast)\n\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\")\n\nmyenvs=conda_list()\nenvname=myenvs$name[3]\nuse_condaenv(envname, required = TRUE)\n\n```\n\n[data set source](https://archive.ics.uci.edu/ml/datasets/Daily+Demand+Forecasting+Orders)\n\n```{r, include =FALSE}\ndata_file_path = '/Users/lrabalski1/Desktop/prv/data/'\n\ndf <- read.csv(url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00409/Daily_Demand_Forecasting_Orders.csv\"), header=TRUE, sep=';') \n```\n\n# simple case\n\n## general exploration of TS\n\nFirst we plot the data in chronological order. Since we will model this as an AR process, we look to the PACF to set a cutoff on the order of the process\n\nThe database was collected during 60 days, this is a real database of a Brazilian company of large logistics. Twelve predictive attributes and a target that is the total of orders for daily.\n\nData looks stationary.\n\n```{r}\nplot(df$Banking.orders..2., type='l')\nabline(reg=lm(df$Banking.orders..2.~index(df)))\n```\n\n## AR\n\nAs name suggest autoregression is regression made upon past values. The simplest autoregression model is known as AR(1): $y_t=b_0+b_1*y_{t-1}+e_t$ $e_t$ is changeable within time error with stable variance and mean = 0.\n\n### selecting parameter of AR\n\n```{r}\npacf(df$Banking.orders..2.)\n```\n\nWe can see that the value of the PACF crosses the 5% significance threshold at lag 3. This is consistent with the results from the ar() function available in R's stats package. ar() automatically chooses the order of an autoregressive model if one is not specified:\n\n```{r}\nar(df$Banking.orders..2., method = \"mle\")\n```\n\nNotice that the ar() function has also provided us with the coefficients for the model. We may, however, want to limit the coefficients. For example, looking at the PACF, we might wonder whether we really want to include a coefficient for the lag -- 1 term or whether we should assign that term a mandatory coefficient of 0 given that its PACF value is well below the threshold used for significance. In this case, we can use the arima() function also from the stats package. Here, we demonstrate how to call the function to fit an AR(3), by setting the order parameter to c(3, 0, 0), where 3 refers to the order of the AR component\n\n```{r}\nest <- arima(x = df$Banking.orders..2.,order = c(3, 0, 0))\nest\n```\n\nTo inject prior knowledge or opinion into our model, we can constraint a coefficient to be 0. For example, if we want to constrain the lag -- 1 term to remain 0 in our model, we use the following call:\n\n```{r}\nest.1 <- arima(x =  df$Banking.orders..2.,order = c(3, 0, 0), \n               fixed = c(0, NA, NA, NA))\n\n\n```\n\nWe now inspect our model performance on our training data to assess the goodness of fit of our model to this data set. We can do this in two ways. First, we plot the ACF of the residuals (that, is the errors) to see if there is a pattern of self-correlation that our model does not cover.\n\nPlotting the residuals is quite simple thanks to the output of the arima() function\n\n```{r}\nacf(est.1$residuals)\nest.1\n```\n\nNone of the values of the ACF cross the significance threshold.\n\nWe do not see a pattern of self-correlation here among the residuals (i.e., the error terms). If we had seen such a pattern, we would likely want to return to our original model and consider including additional terms to add complexity to account for the significant autocorrelation of the residuals.\n\n### forecasting 1 time step ahead\n\napplying by hand\n\nwhat's important there are different type of parametrizations of ARIMA models\n\nThere are multiple (equivalent) parametrizations for ARIMA models. There's the one you quote (sometimes called the ARMAX parametrization):\n\n$y_t = \\phi y_{t-1} + c + \\epsilon_t$\n\nAnd the equivalent regression with ARMA errors parametrization:\n\n$(y_t - c') = \\phi (y_{t-1} - c') + \\epsilon_t$\n\nThe forecast package uses the second parametrization.\n\nThe package author explains the difference between the two parametrizations and the rationale for choosing this one on his blog. Mostly, when adding other regressors (other than the constant), this version is easier to interpret. Since the function forecast::Arima allows for other regressors, it makes sense to treat them all in the same way.\n\n[url](https://stats.stackexchange.com/questions/236633/r-arima-order1-0-0-forecast-not-giving-what-expected)\n\n```{r}\nest.1$coef['intercept']\nest.1$coef['ar1']\nest.1$coef['ar2']\nest.1$coef['ar3']\n\n\nx<-c(NA,NA,NA,df$Banking.orders..2.)\n\ny_pred = zoo::rollapply(zoo(x),\n               width=3,\n               FUN=function(w){\n                 est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(w[3]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(w[2]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(w[1]-est.1$coef['intercept']),0) \n                 \n                \n                 },\n               partial = FALSE\n               )\n\ny_pred <- c(as.vector(y_pred))\n\n```\n\n```{r}\nplot(df$Banking.orders..2., type = 'l')\nlines(fitted(est.1,h=1), col = 3, lwd = 5) ## use the forecast package\nlines(y_pred, col = 6, lwd = 2) ## fitted by hand\n```\n\nNow let's think about the quality of the forecast. If we calculate the correlation between the predicted value and the actual value, we get 0.29. This is not bad in some contexts, but remember that sometimes differencing the data will remove what seemed like a strong relationship and replace it with one that is essentially random. This will be the case particularly if the data was not truly stationary when we fit it, so that an unidentified trend masquerades as good model performance when it is actually a trait of the data we should have addressed before modeling.\n\nWe can difference both the series and the predicted values to see whether the change from one time period to the next is well predicted by the model. Even after differenc‐ ing, our predictions and the data show similar patterns, suggesting our model is a meaningful one\n\n```{r}\ncor(y_pred[1:60],df$Banking.orders..2.[1:60])\ncor(diff(y_pred)[1:59],diff(df$Banking.orders..2.)[1:59])\nplot(diff(df$Banking.orders..2.)[1:59],diff(y_pred)[1:59])\n```\n\n### Forecasting many steps into the future\n\nLooking back at the original plot of the forecast versus actual values, we see that the main difference between the forecast and the data is that the forecast is less variable than the data.\n\nIt may predict the direction of the future correctly, but not the scale of the change from one time period to another. This is not a problem per se but rather reflects the fact that forecasts are means of the predicted distributions and so necessarily will have lower variability than sampled data.\n\nAs you can see in the Figure below, the variance of the prediction decreases with increasing forward horizon. The reason for this---which highlights an important limitation of the model---is that the further forward in time we go, the less the actual data matters because the coefficients for input data look only at a finite previous set of time points (in this model, going back only to lag -- 3; i.e., time -- 3). The future prediction approaches the mean value of the series as the time horizon grows, and hence the variance of both the error term and of the forecast values shrinks to 0 as the forecast values tend toward the unconditional mean value.\n\n```{r}\nplot(df$Banking.orders..2., type = 'l')\nlines(fitted(est.1,h=1), col = 2, lwd = 6) ## use the forecast package\nlines(fitted(est.1,h=4), col = 4, lwd = 4) ## use the forecast package\nlines(fitted(est.1,h=7), col = 3, lwd = 2) ## use the forecast package\n```\n\n### calculating forecast by hand\n\nBelow how prediction for point 8 in time series in calculcated using window_width=3\n\n```{r}\n\nts<-df$Banking.orders..2.\n\n\ny_hat_6_window_1 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(ts[5]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(ts[4]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[3]-est.1$coef['intercept']),0) \n\ny_hat_7_window_2 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(y_hat_6-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(ts[5]-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[4]-est.1$coef['intercept']),0) \n\ny_hat_8_window_3 = est.1$coef['intercept'] + \ndplyr::coalesce(est.1$coef['ar1']*(y_hat_7_window_2-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar2']*(y_hat_6-est.1$coef['intercept']),0) + \ndplyr::coalesce(est.1$coef['ar3']*(ts[5]-est.1$coef['intercept']),0) \n\n\nfitted(est.1,h=3)[8] == y_hat_8_window_3\n\n\n\n```\n\nBelow show how variance is diminishing continously along broader and broader horizont width.\n\n```{r}\n## R\nvar(fitted(est.1, h = 3), na.rm = TRUE)\nvar(fitted(est.1, h = 5), na.rm = TRUE) \nvar(fitted(est.1, h = 10), na.rm = TRUE) \nvar(fitted(est.1, h = 20), na.rm = TRUE) \n```\n\n## MA model\n\nDo not confuse the MA model (moving average model) with a moving average. They are not the same thing.\n\nMA models are by definition weakly stationary.\n\nA moving average model can be expressed similarly to an autoregressive model except that the terms included in the linear equation refer to present and past error terms rather than present and past values of the process itself. So an MA model of order q is expressed as:\n\n$yt = μ +e_t +θ_1 × e_{t-1} +θ_2 × e_{t-2}...+θ_q × e_{t-q}$\n\n-   Economists talk about these error terms as \"shocks\" to the system,\n\n-   while someone with an electrical engineering background could talk about this as a series of impulses and the model itself as a finite impulse response filter, meaning that the effects of any particular impulse remain only for a finite period of time.\n\nThe wording is unimportant, but the concept of many independent events at different past times affecting the current value of the process, each making an individual contribution, is the main idea.\n\n### selecting parameter of AR\n\nWe see significant values at lags 3 and 9, so we fit an MA model with these lags.\n\n```{r}\nacf(df$Banking.orders..2.)\n```\n\n```{r}\nma.est = arima(x = df$Banking.orders..2.,\n                     order = c(0, 0, 9),\n  fixed = c(0, 0, NA, rep(0, 5), NA, NA))\nma.est\n```\n\nNote that the Box.test() input requires us to specify the number of degrees of freedom--- that is, how many model parameters were free to be estimated rather than being con‐ strained to a specific value. In this case, the free parameters were the intercept as well as the MA3 and MA9 terms.\n\nWe cannot reject the null hypothesis that there is no temporal correlation between residual points.\n\n```{r}\nBox.test(ma.est$residuals, lag = 10, type = \"Ljung\", fitdf = 3)\n```\n\n### calculating forecast by hand\n\n# this is equasion for proces MA(1), based on this equasation parameters are calculated\n\n$y_t = μ + θ_1 × e_{t -1} + e_t$ Then if i want to predict future one point forward i am getting following equasation:\n\n$y_{t+1} = μ + θ_1 × e_{t} + e_{t+1}$.\n\nSince I don't know value of\\$ e\\_{t+1}\\$ it is diminishing and i finished with following equsation\n\n\\$y\\_{t+1} = μ + θ_1 × e\\_{t} \\$.\n\n```{r}\n\nfitted(ma.est, h=1)[11]\nfitted(ma.est)\nma.est$residuals\n\nts<-df$Banking.orders..2.\n\nma.est$coef['intercept']\n\nres=c()\nh=1\nfor (i in seq(11,60)){\n  res = c(res,\n          ma.est$coef['intercept'] + # ma.est$residuals[i] tego nie wliczam bo nie znam w błedu 1 punkt w przyszłość\n  ma.est$coef[['ma3']] * ma.est$residuals[i-3 +h] +   \n  ma.est$coef[['ma9']] * ma.est$residuals[i-9 +h] \n  )\n}\n\n\n\n\n\n```\n\n### forecasting 1 time step ahead\n\n```{r}\nfitted(ma.est, h=1)\n```\n\nMA models exhibit strong mean reversion and so forecasts rapidly converge to the mean of the process. This makes sense given that the process is considered to be a function of white noise.\n\n```{r}\nplot(c(df$Banking.orders..2.,rep(NA,12)), type = 'l')\nlines(c(fitted(ma.est, h=1),predict(ma.est, n.ahead = 12)$pred)\n      , col = 3, lwd = 5) ## use the forecast package\nlines(c(rep(NA,11),res)\n      , col = 4, lwd = 2) ## by hand\n\n```\n\nIf you forecast beyond the range of the model established by its order, the forecast will necessarily be the mean of the process by definition of the process. Consider an MA(1) model: $yt = μ + θ1 × et -1 + et$ To predict one time step in the future, our estimate for $y_{t+1} = μ + θ1 × yt + et$.\n\nIf we want to predict two time steps in the future, our estimate is: $E(y_{t+2} =μ+e_{t+2} +θ_1 ×e_{t+1})=μ+0+θ_1 ×0=μ$\n\nWith an MA(1) process we cannot offer an informed prediction beyond one step ahead, and for an MA(q) process in general we cannot offer a more informed predic‐ tion beyond q steps than the mean value emitted by the process. By informed predic‐ tion, I mean one in which our most recent measurements have an impact on the forecast.\n\nWe can see this by producing predictions with our MA(9) model that we just fit, and for which we now seek predictions 10 time steps ahead.\n\nWhen we attempt to predict 10 time steps into the future, we predict the mean for every time step\n\n```{r}\nfitted(ma.est, h=10)[40:50]\nfitted(ma.est, h=9)[40:50]\n```\n\n## more complicated case\n\n```{r}\nclass(AirPassengers)\n#This is the start of the time series\nstart(AirPassengers)\n#This is the end of the time series\nend(AirPassengers)\n#The cycle of this time series is 12months in a year\nfrequency(AirPassengers)\nsummary(AirPassengers)\n```\n\n```{r}\n#The number of passengers are distributed across the spectrum\nplot(AirPassengers)\n#This will plot the time series\nabline(reg=lm(AirPassengers~time(AirPassengers)))\n# This will fit in a line\n```\n\n```{r}\ncycle(AirPassengers)\n\n#This will print the cycle across years.\n\nplot(aggregate(AirPassengers,FUN=mean))\n#This will aggregate the cycles and display a year on year trend\n\nboxplot(AirPassengers~cycle(AirPassengers))\n#Box plot across months will give us a sense on seasonal effect\n\n```\n\n### Important Inferences\n\n1.  The year on year trend clearly shows that the #passengers have been increasing without fail.\n\n2.  The variance and the mean value in July and August is much higher than rest of the months.\n\n3.  Even though the mean value of each month is quite different their variance is small. Hence, we have strong seasonal effect with a cycle of 12 months or less.\n\nExploring data becomes most important in a time series model -- without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.\n\n### Solving unstationary issues\n\nWe know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, let's test the resultant series.\n\n```{r}\ntseries::adf.test(diff(log(AirPassengers)), alternative=\"stationary\", k=0)\n\nlog(AirPassengers)\n```\n\nWe see that the series is stationary enough to do any kind of time series modelling.\n\nNext step is to find the right parameters to be used in the ARIMA model. We already know that the 'd' component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series:\n\n```{R}\nacf(diff(log(AirPassengers)))\n\n\npacf(diff(log(AirPassengers)))\n```\n\nClearly, ACF plot cuts off after the first lag. Hence, we understood that value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.\n\n```{r}\n(fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))\n```\n\nLet's fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :\n\n```{r}\n(fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))\n\npred <- predict(fit, n.ahead = 10*12)\n\nts.plot(AirPassengers,2.718^pred$pred, log = \"y\", lty = c(1,3))\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"highlight-style":"github","toc":true,"output-file":"index.html"},"language":{"code-summary":"Show the code"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"darkly","title-block-banner":true,"code-copy":true,"code-block-border-left":"#31BAE9","title":"TS - forecast","date":"2022-11-20","tags":[],"categories":["Time Series","forecast"]},"extensions":{"book":{"multiFile":true}}}}}