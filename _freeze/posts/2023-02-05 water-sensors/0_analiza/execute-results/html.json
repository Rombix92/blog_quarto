{
  "hash": "49d46f92785e37e2d40fcaa1b5e602fa",
  "result": {
    "markdown": "---\ntitle-block-banner: true\nauthor: ≈Åukasz RƒÖbalski\ntitle: Water sensors \ndescription: Presentation of Time Series exploration, anomaly detection, and predictions techniques\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    toc-depth: 3\n    embed-resources: true\ncategories: ['Python','Time Series','anomaly detection']\ntags: []\neditor: source\nfig.height: 4\nout.width: '100%'\ninclude: TRUE  #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho: TRUE  #echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nwarning: FALSE\nmessage: FALSE\nerror: FALSE\n---\n\n```{=html}\n<style type=\"text/css\">\n.main-container {\n  max-width: 100% !important;\n  margin: auto;\n}\n</style>\n```\n\n\n\n\n\n\n<white> \\##### libraries\n\n\n\n\n\n\n\nIn the¬†attached¬†file¬†you will find data containing two water sensors:\nwater level and water velocity.\n\nColumn named \"level\" refers to raw level data and column named\n\"velocity\" refers to raw velocity data. Columns \"final_level\" and\n\"final_velocity\" refer to data that was manually corrected or removed\nbecause of malfunctions of sensors or other reasons. You have three\ntasks here:\n\n1.  Analyze the level and velocity data, find patterns, state own\n    observations and conclusions.\n\n2.  Having the knowledge of what was corrected and statistical methods\n    prepare simple solution to automatically find suspicious or\n    corrupted\n    data. (without using final columns to get the results, use them only\n    as a\n    reference).\n\n3.  Prepare a simple prediction model for both channels.\n\n# data exploration\n\n> Analyze the level and velocity data, find patterns, state own\n> observations and conclusions.\n\n## missing data\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport math \ndf = pd.read_csv('/Users/lrabalski1/Desktop/prv/data/water_sensors.csv', parse_dates=[\"time\"])\nprint('min time value: ',df.time.min(),'\\n max time value: ',df.time.max())\n## min time value:  2019-01-01 00:00:00 \n##  max time value:  2021-05-31 23:55:00\ndf=pd.DataFrame(\n  pd.date_range('2019-01-01 00:00:00','2021-05-31 23:55:00',freq='5T'),\n  columns=['time']\n  ).merge(df, how='left', on=['time'])\ndf.index=df.time\n\n\nprint('NA within level_final: ',sum(df.level_final.isna()),'\\nNA within velocity_final: ', sum(df.velocity_final.isna()))\n\n## NA within level_final:  13725 \n## NA within velocity_final:  13725\ndf['level_data_malfunction']=(df.level.isna()) | (df.level!=df.level_final)\ndf['velocity_data_malfunction']=(df.velocity.isna()) | (df.velocity!=df.velocity_final)\n\n\n# create a list of our conditions\nconditions_level = [\n    (df['level_data_malfunction'] ==False),\n    (df['level_data_malfunction'] ==True) & ( df['level_final'].isna()),\n    (df['level_data_malfunction'] ==True) & ( ~df['level_final'].isna())\n    ]\nvalues_level = [np.nan,'sensors_malfunction','manual_correction']\nconditions_velocity = [\n    (df['velocity_data_malfunction'] ==False),\n    (df['velocity_data_malfunction'] ==True) & ( df['velocity_final'].isna()),\n    (df['velocity_data_malfunction'] ==True) & ( ~df['velocity_final'].isna())\n    ]\nvalues_velocity = [np.nan,'sensors_malfunction','manual_correction']\ndf['velocity_reason_of_correction'] = np.select(conditions_velocity, values_velocity)\ndf['level_reason_of_correction'] = np.select(conditions_level, values_level)\nprint('\\n\\n')\n```\n:::\n\n\nMissing data in both variable (velocity and level), which can be\nexplained as sensors malfunction, is observed in 13725 data points. For\nboth variable missing data appears in the same datapoint. Probably\nsensors are responsible for measuring both measures.\n\nAdditionally in case of level measure we can observe 204 datapoint where\ndata were manually corrected.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(pd.crosstab(df['velocity_reason_of_correction'],df['level_reason_of_correction']))\n## level_reason_of_correction     manual_correction     nan  sensors_malfunction\n## velocity_reason_of_correction                                                \n## nan                                          204  240087                    0\n## sensors_malfunction                            0       0                13725\n```\n:::\n\n\n## patterns\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\ndf['time'] = pd.to_datetime(df['time'])\ndf_t=df.copy()\ndf_t.index = df_t['time']\ndf_t=df_t.drop(['time','level_data_malfunction', 'velocity_data_malfunction','velocity_reason_of_correction', 'level_reason_of_correction'], axis=1)\n\ncheck=df_t.sort_values('level_final', ascending=False)\n```\n:::\n\n\nFrom the chart it can bee seen that:\n\n-   there is rather no trend within each timeseries\n\n-   in level data one big anomaly can be observed in the middle of a\n    period\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n# downsampling\ndf_M = df_t.resample(\"D\").mean()\n\nstyles1 = ['b-','r-']\nplt.clf()\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True)\ndf_M[['level_final']].plot(ax=axes[0],style=styles1)\ndf_M[['velocity_final']].plot(ax=axes[1],style=styles1)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n\n```{.python .cell-code}\ncheck=df_M.sort_values('level_final', ascending=False)\n```\n:::\n\n\n### stationarity\n\nAs it can be seen on the both plots out time series seeme to be\nstationary, rolling mean and SD is stable within time.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nrolling_mean = df_M.rolling(7).mean()\nrolling_std = df_M.rolling(7).std()\n\n\n#f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 1, 1) \n\nplt.plot(df_M['level_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['level_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['level_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Level Data\")\nplt.xticks([])\n## ([], [])\nplt.legend(loc=\"best\")\n\nfig.add_subplot(2, 1, 2) \nplt.plot(df_M['velocity_final'], color=\"blue\",label=\"Original Data\")\nplt.plot(rolling_mean['velocity_final'], color=\"red\", label=\"Rolling Mean 7 days\")\nplt.plot(rolling_std['velocity_final'], color=\"black\", label = \"Rolling SD 7 days\")\nplt.title(\"Velocity Data\")\nplt.legend(loc=\"best\")\nplt.xticks([])\n## ([], [])\nplt.show()\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-7-3.png){width=100%}\n:::\n:::\n\n\nStiationarity of a data is support by the fact that p-value of\nDickey--Fuller test is lower than 5 percent (null hypothesis it claiming\nthat time series is non-stationary) and the test statistic is lower than\nthe critical value. We can also draw these conclusions from inspecting\nthe data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.tsa.stattools import adfuller\nadft_velocity = adfuller(df_M.dropna(subset=['velocity_final'])['velocity_final'],autolag=\"AIC\")\nadft_level = adfuller(df_M.dropna(subset=['level_final'])['level_final'],autolag=\"AIC\")\n\nadft=adft_level\noutput_level = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nadft=adft_velocity\noutput_velocity = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n\nresults = pd.DataFrame(output_velocity.join(output_level, lsuffix='_velocity', rsuffix='_level'))\nresults\n##    Values_velocity  ...                 Metric_level\n## 0        -4.758844  ...              Test Statistics\n## 1         0.000065  ...                      p-value\n## 2         8.000000  ...             No. of lags used\n## 3       841.000000  ...  Number of observations used\n## 4        -3.438149  ...          critical value (1%)\n## 5        -2.864983  ...          critical value (5%)\n## 6        -2.568603  ...         critical value (10%)\n## \n## [7 rows x 4 columns]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntabela(py$results)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-5f29d3a2b6650c3f5e75\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5f29d3a2b6650c3f5e75\">{\"x\":{\"filter\":\"top\",\"vertical\":false,\"filterHTML\":\"<tr>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none;position: absolute;width: 200px;opacity: 1\\\">\\n      <div data-min=\\\"-4.7588442824438\\\" data-max=\\\"841\\\" data-scale=\\\"15\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none;position: absolute;width: 200px;opacity: 1\\\">\\n      <div data-min=\\\"-4.5674129740265\\\" data-max=\\\"835\\\" data-scale=\\\"15\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"extensions\":[\"Buttons\"],\"caption\":\"<caption><\\/caption>\",\"data\":[[-4.7588442824438,6.51519038186205e-05,8,841,-3.43814949093876,-2.86498279559325,-2.56860322243917],[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\",\"critical value (1%)\",\"critical value (5%)\",\"critical value (10%)\"],[-4.5674129740265,0.000148263119551768,14,835,-3.43820570888786,-2.86500757854652,-2.56861642403815],[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\",\"critical value (1%)\",\"critical value (5%)\",\"critical value (10%)\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Values_velocity<\\/th>\\n      <th>Metric_velocity<\\/th>\\n      <th>Values_level<\\/th>\\n      <th>Metric_level<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"dom\":\"Blfrtip\",\"pageLength\":10,\"lengthMenu\":[10,25,200],\"scrollX\":true,\"buttons\":[\"copy\",\"csv\",\"excel\"],\"columnDefs\":[{\"targets\":null,\"visible\":false},{\"className\":\"dt-center\",\"targets\":\"_all\"}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n### autocorelation\n\nWe can see that both variable are weakly autocorelated, however velocity\ndata better. Reason for bad autocorelation in level data measured this\nway, may have fact of big anomaly within dataset.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef my_function(df,col, array):\n  dicts = {}\n  for idx, x in enumerate(array):\n    dicts[x] = df[col].autocorr(x)\n  return dicts\nprint('autocorelation of level data :')\n## autocorelation of level data :\nmy_function(df_M,'level_final',array=[1,7,14,30,365])\n## {1: 0.6187899970924978, 7: 0.16030304961078023, 14: 0.1660215318228444, 30: 0.08793288098871045, 365: 0.016746891095633046}\nprint('\\n autocorelation of velocity data :')\n## \n##  autocorelation of velocity data :\nmy_function(df_M,'velocity_final',array=[1,7,14,30,365])\n## {1: 0.822156599237431, 7: 0.4759130585597128, 14: 0.3678199431634758, 30: 0.15582363183139622, 365: -0.11352389683675475}\n```\n:::\n\n\n### data imputation\n\nI will choose best method based on longest period upon which I have full\ndata.\n\nMissing data is spread along all time range.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\ndf_na = df_t.copy()\ndf_na['time']=df_na.index\ndf_na = df_na.reset_index(drop=True)\ndf_na['date']=df_na['time'].dt.date\ndf_na['na_velocity_final'] = df_na.velocity_final.isna()\ndf_na['na_level_final'] = df_na.level_final.isna()\ndates_with_na = df_na.groupby('date')[['na_level_final','na_velocity_final']].sum().reset_index()\ndates_with_na['any_na'] = dates_with_na.sum(axis=1)\n## <string>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\ndates_with_na[dates_with_na.any_na > 0]\n##            date  na_level_final  na_velocity_final  any_na\n## 116  2019-04-27              58                 58     116\n## 117  2019-04-28             119                119     238\n## 129  2019-05-10              64                 64     128\n## 142  2019-05-23              38                 38      76\n## 163  2019-06-13               4                  4       8\n## ..          ...             ...                ...     ...\n## 854  2021-05-04               1                  1       2\n## 855  2021-05-05               3                  3       6\n## 856  2021-05-06               2                  2       4\n## 869  2021-05-19               9                  9      18\n## 870  2021-05-20               9                  9      18\n## \n## [105 rows x 4 columns]\nplt.clf()\nfig = plt.figure()\nplt.plot(dates_with_na.date, dates_with_na.any_na)\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-11-1.png){width=100%}\n:::\n:::\n\n\nThe longest chain of missing data is of 5476 consecutive data points (5\nseconds \\* 5476 = 7.6 hours) .\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlongest_consecutive_na_chain = (~df_na['na_level_final']).cumsum().value_counts()\nlongest_consecutive_na_chain = longest_consecutive_na_chain[longest_consecutive_na_chain!=1]-1\n\n\ndf_na.index = df_na['time']\nlongest_concecutive_full_data_chain  = (df_na['na_level_final']).cumsum()\nlongest_concecutive_full_data_chain_counts = longest_concecutive_full_data_chain.value_counts()\n\nlongest_concecutive_full_data_chain_counts=longest_concecutive_full_data_chain_counts[longest_concecutive_full_data_chain_counts!=1]-1\n\nselected_chain = longest_concecutive_full_data_chain[longest_concecutive_full_data_chain==longest_concecutive_full_data_chain_counts.index[0]]\n\n```\n:::\n\n\nAny automatic technic od missing data imputation like:\n\n-   Mean, median and mode imputation\n\n-   Forward and backward filling\n\n-   linear / nearest imputation\n\ncould behave bad in such a kind o missing pattern (5476 consecutive data\npoints is missing)\n\nThats why I will prepare two different imputation schema, check their\naccuracy for given dataset and choose better one:\n\n-   **Method 1**: fill the missing data with the averaged value of given\n    second of year. For seconds from january to may i will have maximum\n    3 data points (2019;2020;2021); for rest maximum of 2 data points.\n\n-   **Method 2**:fill data with previous day data point with similar\n    daytime\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n\n# preparing dataset for Method 1\ndf_fill = df_t.copy()\ndf_fill['time']=df_fill.index\n#df_fill = df_fill.reset_index(drop=True)\ndf_fill['time_agg'] = pd.DatetimeIndex(df_fill.time).strftime(\"%m-%d %H:%M:%S\")\ndf_time_agg = df_fill.groupby('time_agg').agg(\n  velocity_final_mean=('velocity_final','mean'),\n  level_final_mean=('level_final','mean'),\n  velocity_mean=('velocity','mean'),\n  level_mean=('level','mean')\n).reset_index()\n\n\n#'after agregation to day of year I still have 54 missing value, which mostly come from one chain of missing data. I will replace them with data from previous day.')\n#df_time_agg.isna().sum()\n\ndf_time_agg = df_time_agg.join(df_time_agg.shift(24*12), rsuffix='_lag_1_day')\ndf_time_agg.velocity_final_mean = df_time_agg.velocity_final_mean.fillna(df_time_agg.velocity_final_mean_lag_1_day)\ndf_time_agg.velocity_mean = df_time_agg.velocity_mean.fillna(df_time_agg.velocity_mean_lag_1_day)\ndf_time_agg.level_final_mean = df_time_agg.level_final_mean.fillna(df_time_agg.level_final_mean_lag_1_day)\ndf_time_agg.level_mean = df_time_agg.level_mean.fillna(df_time_agg.level_mean_lag_1_day)\ndf_time_agg = df_time_agg[df_time_agg.columns.drop(list(df_time_agg.filter(regex='_lag_1_day')))]\n\n\n# testing two method\n## preparing test data\n\ndf_test = df_fill.loc[selected_chain.index].copy()\ndf_test['velocity_final_missing'] = df_test.velocity_final\ndf_test['level_final_missing'] = df_test.level_final\nrng = np.random.RandomState(42)\nrandom_indices = rng.choice(range(len(df_test)), size=round(len(df_test)/50))\ndf_test = df_test.reset_index(drop=True)\ndf_test.loc[random_indices, ['velocity_final_missing','level_final_missing']] = np.nan\n\n\n## fillig missing data\n### time_agg method - Method 1\ndf_test = df_test.merge(df_time_agg, how='left', on =['time_agg'])\ndf_test.index = df_test['time']\ndf_test['velocity_final_fill_method_1'] = df_test.velocity_final_missing.fillna(df_test.velocity_final_mean)\ndf_test['level_final_fill_method_1'] = df_test.level_final_missing.fillna(df_test.level_final_mean)\n\n### laged value - Method 2\ncols= ['velocity_final_fill_method_2','level_final_fill_method_2']\ntime_shifts = np.array([-24*12,-24*12+1,-24*12-1,-24*12+2,-24*12-2,-24*12+3,-24*12-3,-24*12*2,-24*12*2+1,-24*12*2-1,-24*12*2+2,-24*12*2+2,-24*12*2+3,-24*12*2-3])\n\ndf_test['velocity_final_fill_method_2'] = df_test.velocity_final_missing\ndf_test['level_final_fill_method_2'] = df_test.level_final_missing\n\ni=-1\nwhile df_test[['velocity_final_fill_method_2','level_final_fill_method_2']].isna().sum().sum() >0:\n  i=+1\n  time_shifts = [x+1 for x in time_shifts]\n  for col,time_shift in  ((x, y) for x in cols for y in time_shifts):\n    #print (col,time_shift)\n    df_test[col] = df_test[col].fillna(df_test[col].shift(time_shift))\n\n\n\n\n\n\n# testing difference\nlevel_dist_1 = abs(df_test.level_final_fill_method_2-df_test.level_final)\nlevel_dist_2 = abs(df_test.level_final_fill_method_1-df_test.level_final)\n\nvelocity_dist_1 = abs(df_test.velocity_final_fill_method_2-df_test.velocity_final)\nvelocity_dist_2 = abs(df_test.velocity_final_fill_method_1-df_test.velocity_final)\n\n\nplt.clf()\nfig = plt.figure()\nfig.add_subplot(2, 2, 1)\n\nplt.plot(level_dist_1, color=\"red\", label=\"Level - Method 1\")\nplt.plot(level_dist_2, color=\"black\", label = \"Level - Method 2\")\n#plt.title(\"Absolute Error for level missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 2)\nplt.bar(['Level - method 1','Level - method 2'], [level_dist_1.sum(),level_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 3)\nplt.plot(velocity_dist_1, color=\"red\", label=\"Velocity - Method 1\")\nplt.plot(velocity_dist_2, color=\"black\", label = \"Velocity - Method 2\")\n#plt.title(\"Absolute Error for velocity missing data imputation\")\nplt.legend(loc=\"best\")\nplt.xticks(rotation=45)\n## (array([17897., 17911., 17928., 17942., 17956., 17970., 17987., 18001.,\n##        18017.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nfig.add_subplot(2, 2, 4)\nplt.bar(['Velocity - method 1','Velocity - method 2'], [velocity_dist_1.sum(),velocity_dist_2.sum()])\n#plt.title(\"Absolute Error for level missing data imputation\")\n## <BarContainer object of 2 artists>\nplt.xticks(rotation=30)\n## ([0, 1], [Text(0, 0, ''), Text(0, 0, '')])\nfig.tight_layout()\nplt.show()\n\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-13-3.png){width=100%}\n:::\n:::\n\n\n\n\n### periodic decomposition\n\nBoth dataset show quite strong daily seasonality (level higher then\nvelocity) Both dataset show weak weekly seasonality (again level higher\nthen velocity) There is no observable trend in both variables.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\n\n\n# daily seasonality\ndf_D = df_fill_final\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','velocity_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-15-5.png){width=100%}\n:::\n\n```{.python .cell-code}\ndecompose = seasonal_decompose(df_D.loc[df_D.index > '2021-04-01','level_final_fill'],model='additive', period = 12*24)\ndecompose.plot()\nplt.show()\n\n# weakly seasonality\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-15-6.png){width=100%}\n:::\n\n```{.python .cell-code}\ndf_D = df_fill_final.resample(\"D\").mean()\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','velocity_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-15-7.png){width=100%}\n:::\n\n```{.python .cell-code}\ndecompose = seasonal_decompose(df_D.loc[df_D.index >= '2020-05-12','level_final_fill'],model='additive', period = 7)\ndecompose.plot()\nplt.show()\n\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-15-8.png){width=100%}\n:::\n:::\n\n\n# automatically detect suspicious data\n\n## simple solution\n\n> Having the knowledge of what was corrected ... prepare simple solution\n> to automatically find suspicious or corrupted data\n\n\n\n\n\nBelow I present two periods within which data correction of level\nvariable took place. It shows that there is no more then 600 level\nvalue, while manuall correction reported that there were higher values\nto observe. Probably sensors are not able to make measurement above this\nlevel.\n\nAccording to this my suggestion would be:\n\n> **üö®** **when the measurement of level will hit 600 sound the alarm\n> üö®**\n\nThe data correction process which took place within 2019-06-13 was\nsubstle in size. Hard to be understooded without any extra information.\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = df.loc[df['level_reason_of_correction'] == 'manual_correction', ['level']] #anomaly\n\n\nsel_dates = (df.index>a.index.min()) & (df.index < a.index.max())\nsel_dates_1 = np.isin(df.index.date,list(set(a.index.date))[0:2])\nsel_dates_2 = np.isin(df.index.date,list(set(a.index.date))[2:])\n\na_1 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_1==True), ['level']] #anomaly\n\n\na_2 = df.loc[(df['level_reason_of_correction'] == 'manual_correction') & (sel_dates_2==True), ['level']] #anomaly\n\ncheck = df.loc[sel_dates_2,['level','level_final']]\n\nplt.clf()\nfig = plt.figure()\n\nfig.add_subplot(2, 1, 1) \nplt.plot( df.loc[sel_dates_1,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_1,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_1.index,a_1['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18078., 18109., 18140., 18170., 18201., 18231., 18262.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\n\nfig.add_subplot(2, 1, 2) \nplt.plot( df.loc[sel_dates_2,['level_final']], color='green', label = 'corrected',linestyle='dashed')\nplt.plot( df.loc[sel_dates_2,['level']], color='black', label = 'Uncorrected')\nplt.scatter(a_2.index,a_2['level'], color='red', label = 'bad data',alpha=0.5, marker='*')\nplt.xticks(rotation=30)\n## (array([18272.   , 18272.125, 18272.25 , 18272.375, 18272.5  , 18272.625,\n##        18272.75 , 18272.875, 18273.   ]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\nplt.legend()\nfig.tight_layout()\nplt.show();\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-17-13.png){width=100%}\n:::\n:::\n\n\n## Statistical method\n\n> Having the knowledge of statistical methods prepare simple solution to\n> automatically find suspicious or corrupted data.\n\nIn order to detect anomalies in dataset I used IsolationForest algorithm\nwhich identifies anomalies by isolating outliers in the data.\n\nThe `contamination` parameter defines a rough estimate of the percentage\nof the outliers in our dataset. Based on what i know about size of\nmanual correction within my dataset I assigned contamination to be 0.1%\nin our case.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom_state = np.random.RandomState(42)\noutliers_fraction = float(round(np.mean(df['level_reason_of_correction']=='manual_correction'),ndigits=3))\n\n\nscaler = StandardScaler()\nnp_scaled_velocity = scaler.fit_transform(df_fill_final.velocity_fill.values.reshape(-1, 1))\nnp_scaled_level = scaler.fit_transform(df_fill_final.level_fill.values.reshape(-1, 1))\n\ndata_velocity = pd.DataFrame(np_scaled_velocity)\ndata_level = pd.DataFrame(np_scaled_level)\n\n#train isolation forest\nmodel_velocity =  IsolationForest(contamination=outliers_fraction+0.01,random_state=random_state)\nmodel_level =  IsolationForest(contamination=outliers_fraction,random_state=random_state)\n\nmodel_velocity.fit(data_velocity)\n## IsolationForest(contamination=0.011,\n##                 random_state=RandomState(MT19937) at 0x16A366940)\nmodel_level.fit(data_level)\n## IsolationForest(contamination=0.001,\n##                 random_state=RandomState(MT19937) at 0x16A366940)\ndf_fill_final['velocity_anomaly'] = model_velocity.predict(data_velocity)\ndf_fill_final['level_anomaly'] = model_level.predict(data_level)\n\n\n# checking correctness\ndf.index = df['time']\ndf_fill_final = df_fill_final.join(df['level_reason_of_correction'])\n\n\n# visualization\n\na = df_fill_final.loc[df_fill_final['velocity_anomaly'] == -1, ['velocity_fill']] #anomaly\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(a.index,a['velocity_fill'], color='red', label = 'Anomaly')\nplt.scatter(df_fill_final.index, df_fill_final['velocity_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.legend()\nplt.show();\n\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-18-15.png){width=100%}\n:::\n\n```{.python .cell-code}\na = df_fill_final.loc[(df_fill_final['level_anomaly'] == -1), ['level_fill']] #anomaly\na_true = df_fill_final.loc[(df_fill_final['level_reason_of_correction'] == 'manual_correction') , ['level_fill']] #anomaly\n\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(df_fill_final.index, df_fill_final['level_fill'], color='black', label = 'Normal',s=1, alpha=0.3)\nplt.scatter(a.index,a['level_fill'], color='red', label = 'Anomaly_IsolationForest',alpha=0.5, marker='*')\nplt.scatter(a_true.index,a_true['level_fill'], color='blue', label = 'bad data',alpha=0.5,marker='o',s=20)\nplt.legend()\nplt.show();\n\n```\n\n::: {.cell-output-display}\n![](0_analiza_files/figure-html/unnamed-chunk-18-16.png){width=100%}\n:::\n\n```{.python .cell-code}\ndf_fill_final['anomaly_detected'] = (df_fill_final['level_anomaly'] == -1)\ndf_fill_final['anomaly_true'] = df_fill_final['level_reason_of_correction'] == 'manual_correction'\n```\n:::\n\n\nIsolationForest managed to detect all manually coreccted data and\nadditionally assign this label to 50 extra data points.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(pd.crosstab(df_fill_final['anomaly_detected'] ,df_fill_final['anomaly_true'] ))\n## anomaly_true       False  True\n## anomaly_detected              \n## False             253762     0\n## True                  50   204\n```\n:::\n\n\n# prediction model\n\n> > Prepare a simple prediction model for both channels.\n\nI prepare models which are trying to predict magnitude of level and\nvelocity simply on the base of previous 5 day history of given measure.\n\nArchitecture of model is fully connected Neural Network that starts by\nflattening the data and then runs it through two Dense layers. I used\nmean squared error as the loss, rather than MAE, because unlike MAE,\nit's smooth around zero, which is a useful property for gradient\ndescent.\n\nThere are much better architectures designed for Time Series, like LSTM,\nhowever due to efficiency reason i chose this one (it tooks 20 seconds\nto learn this model on Macbook Pro M1, and additionally I faced some\nproblem with running more LSTM model from keras on MacOS architecture).\n\nResults of models were compared to Commons sense baseline. In case of\npredicting magnitude of measure 24 h from time point I assume that a\ncommon-sense approach is to always predict that the temperature 24 hours\nfrom now will be equal to the temperature right now.\n\nFor both channels models wasn't able to beat Common sense baseline. I\nassume that basing on my past experience LSTM would be able.\n\nVelocity: Test MAE: 0.40 Commons sense baseline MAE: 0.04\n\nLevel: Test MAE: 0.23 Commons sense baseline MAE: Validation MAE: 0.08\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_NN =  df_fill_final.copy()\n\nanalysed_variable='level'\ncolumn_name= analysed_variable + '_final_fill'\nmodel_name = analysed_variable + '_model_recurrent.keras'\n\n\n\nnum_train_samples = int(0.5 * len(df_NN))\nnum_val_samples = int(0.25 * len(df_NN))\nnum_test_samples = len(df_NN) - num_train_samples - num_val_samples\nprint(\"num_train_samples:\", num_train_samples)\nprint(\"num_val_samples:\", num_val_samples)\nprint(\"num_test_samples:\", num_test_samples)\n\n# We‚Äôre going to use the first 127008 timesteps as training data, so we‚Äôll compute the mean and standard deviation only on this fraction of the data.\nmean = df_NN[:num_train_samples].mean(axis=0)\ndf_NN -= mean\nstd = df_NN[:num_train_samples].std(axis=0)\ndf_NN /= std\n\n#Next, let‚Äôs create a Dataset object that yields batches of data from the past five days along with a target temperature 24 hours in the future.\n\n\nimport numpy as np \nfrom tensorflow import keras\n\n\n\nsampling_rate = 6 # 1 data point is 5 minut, setting sampling_rate = 6 means that i will take one data point for 30 minutes\nsequence_length = 240 # 5 days =  5 * 24 * 2 = 1440\ndelay = sampling_rate * (sequence_length + 48 - 1)  # usefull equation allowing for calculating required delay ; 288 is one day, it means that I will predict measure 1 day later\nbatch_size = 256 \n\n\n# checking is it working ok\n\nint_sequence = np.arange(288*24*24)\ndummy_dataset = keras.utils.timeseries_dataset_from_array(\n    data=int_sequence[:-delay],                                 \n    targets=int_sequence[delay:], \n    sampling_rate= sampling_rate,                             \n    sequence_length=sequence_length,  \n    #shuffle=True,                                    \n    batch_size=batch_size\n)\n\nfor inputs, targets in dummy_dataset:\n  break\ndf_NN.index[keras.backend.get_value(inputs[20][-1])]\ndf_NN.index[keras.backend.get_value(targets[20])]\n\n\n# preparing dataset to learning, validating, testing and predicting\n\n\ndf_NN=df_NN.[column_name]\n\ntrain_dataset = keras.utils.timeseries_dataset_from_array(\n    df_NN[:-delay],\n    targets=df_NN[column_name][delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=0,\n    end_index=num_train_samples)\n  \nval_dataset = keras.utils.timeseries_dataset_from_array(\n    df_NN[:-delay],\n    targets=df_NN[column_name][delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=num_train_samples,\n    end_index=num_train_samples + num_val_samples)\n  \ntest_dataset = keras.utils.timeseries_dataset_from_array(\n    df_NN[:-delay],\n    targets=df_NN[column_name][delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=True,\n    batch_size=batch_size,\n    start_index=num_train_samples + num_val_samples)\n\npredict_dataset = keras.utils.timeseries_dataset_from_array(\n    df_NN[:-delay],\n    targets=df_NN[column_name][delay:],\n    sampling_rate=sampling_rate,\n    sequence_length=sequence_length,\n    shuffle=False,\n    batch_size=batch_size\n    )\n\n#Each dataset yields a tuple (samples, targets), where samples is a batch of 256 samples, each containing 5 consecutive days of input data, and targets is the corresponding array of 256 target temperatures. Note that the samples are randomly shuffled, so two consecutive sequences in a batch (like samples[0] and samples[1]) aren‚Äôt necessarily temporally close.\nfor samples, targets in train_dataset:\n     print(\"samples shape:\", samples.shape)\n     print(\"targets shape:\", targets.shape)\n     samples=samples\n     targets=targets\n     break\n\n\nsamples.shape\npreds = samples[:, -1, 0] * std[0] + mean[0] \ntargets.shape\ndef evaluate_naive_method(dataset):\n    total_abs_err = 0. \n    samples_seen = 0 \n    for samples, targets_std in dataset:\n        preds = samples[:, -1, 0] * std[0] + mean[0]\n        targets = targets_std * std[0] + mean[0]\n        total_abs_err += np.sum(np.abs(preds - targets))\n        samples_seen += samples.shape[0]\n    return total_abs_err / samples_seen\n  \nprint(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\") \nprint(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")\n\nfrom tensorflow.keras import layers\n  \ninputs = keras.Input(shape=(sequence_length, df_NN.shape[-1]))\nx = layers.Flatten()(inputs)\nx = layers.Dense(16, activation=\"relu\")(x)\noutputs = layers.Dense(1)(x)\n\n# inputs = keras.Input(shape=(None,  df_NN.shape[-1]))\n# outputs = layers.SimpleRNN(16)(inputs)\n\n\nmodel = keras.Model(inputs, outputs)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(model_name,          \n                                    save_best_only=True)\n] \nmodel.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\nhistory = model.fit(train_dataset,\n                    epochs=10,\n                    validation_data=val_dataset,\n                    callbacks=callbacks)\n  \nmodel = keras.models.load_model(model_name)              \nprint(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")\n\n\nimport matplotlib.pyplot as plt\nloss = history.history[\"mae\"]\nval_loss = history.history[\"val_mae\"]\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\nplt.title(\"Training and validation MAE\")\nplt.legend()\nplt.show()\n\n\ndf_NN_final = df_NN.copy()\ny_pred_array = model.predict(predict_dataset)* std[0] + mean[0]\ny_pred = pd.Series(np.ravel(y_pred_array), name='predict')\ny_pred.index += delay\ndf_NN_final['time'] = df_NN_final.index\ndf_NN_final[column_name] = df_NN_final[column_name] * std[0] + mean[0]\ndf_NN_final = df_NN_final.reset_index(drop=True)\ndf_NN_final = df_NN_final.join(y_pred)\n\n\n\nimport matplotlib.pyplot as plt\ndf_NN_final[[column_name,'predict']].plot()\ndf_NN_final.index=df_NN_final.time\n\n\nplt.clf()\nfig = plt.figure()\nplt.scatter(df_NN_final.index[-10000:-1],df_NN_final[column_name][-10000:-1], color='green', label = 'Anomaly_2',alpha=0.02, marker='*')\nplt.scatter(df_NN_final.index[-10000:-1],df_NN_final['predict'][-10000:-1], color='blue', label = 'Anomaly_2',alpha=0.02, marker='*')\nplt.ylim((0.4,0.8))\nplt.show();\n```\n:::\n",
    "supporting": [
      "0_analiza_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.0/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.26/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.12.1/js/jquery.dataTables.min.js\"></script>\n<script src=\"../../site_libs/jszip-1.12.1/jszip.min.js\"></script>\n<link href=\"../../site_libs/dt-ext-buttons-1.12.1/css/buttons.dataTables.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-ext-buttons-1.12.1/js/dataTables.buttons.min.js\"></script>\n<script src=\"../../site_libs/dt-ext-buttons-1.12.1/js/buttons.html5.min.js\"></script>\n<script src=\"../../site_libs/dt-ext-buttons-1.12.1/js/buttons.colVis.min.js\"></script>\n<script src=\"../../site_libs/dt-ext-buttons-1.12.1/js/buttons.print.min.js\"></script>\n<link href=\"../../site_libs/nouislider-7.0.10/jquery.nouislider.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/nouislider-7.0.10/jquery.nouislider.min.js\"></script>\n<link href=\"../../site_libs/selectize-0.12.0/selectize.bootstrap3.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/selectize-0.12.0/selectize.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}