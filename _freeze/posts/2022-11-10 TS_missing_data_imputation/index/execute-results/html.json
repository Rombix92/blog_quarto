{
  "hash": "f561219c8fc15ee58c832365d1abae21",
  "result": {
    "markdown": "---\ntitle: \"TS - missing data imputation & Smoothing\"\ntags: ['missing data','time series','imputation', 'python', 'R']\ncategories: ['Time Series', 'Smoothing']\n---\n\n\n\n\n\n\n\n\n## Dealing with missing data in Time Series\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(zoo)\nrequire(data.table)\nlibrary(dplyr)\nlibrary(lubridate)\n\nunemp <- fread(paste0(data_file_path,\"bezrobocie_USA.csv\")) %>% data.table::melt( id.vars='Year',\n                                                           variable.name = \"months\",\n                                                           value.name='UNRATE') %>% left_join(\n  data.frame(month_nr=c(1:12),\n             months= c(\"Jan\",\"Feb\",\"Mar\",\n                       \"Apr\",\"May\",\"Jun\",\n                       \"Jul\",\"Aug\",\"Sep\",\n                       \"Oct\",\"Nov\",\"Dec\"))\n) %>% mutate(DATE=as_date('0000-01-01',format = '%Y-%m-%d')+years(as.numeric(Year)) + months(month_nr-1)) \n\nhead(unemp)\n##    Year months UNRATE month_nr       DATE\n## 1: 1948    Jan    3.4        1 1948-01-01\n## 2: 1949    Jan    4.3        1 1949-01-01\n## 3: 1950    Jan    6.5        1 1950-01-01\n## 4: 1951    Jan    3.7        1 1951-01-01\n## 5: 1952    Jan    3.2        1 1952-01-01\n## 6: 1953    Jan    2.9        1 1953-01-01\n\n\nunemp = unemp[, DATE := as.Date(DATE)][!is.na(UNRATE),.(DATE, UNRATE)]\nsetkey(unemp, DATE)\n\n## Creating dataset with random missing values\nrand.unemp.idx <- sample(1:nrow(unemp), .1*nrow(unemp))\nrand.unemp <- unemp[-rand.unemp.idx]\n\n## Creating dataset with systematical missing values, appearing in month with highest unemployment rate\nhigh.unemp.idx <- which(unemp$UNRATE > 8)\nhigh.unemp.idx <- sample(high.unemp.idx, .5 * length(high.unemp.idx))\nbias.unemp <- unemp[-high.unemp.idx]\n\n\n## to identyfy missing data I wil use rolling joins tool from data.table package    \nall.dates <- seq(from = unemp$DATE[1], to = tail(unemp$DATE, 1), by = \"months\")\nrand.unemp = rand.unemp[J(all.dates), roll=FALSE]\nbias.unemp = bias.unemp[J(all.dates), roll=FALSE]\n\n## forward filling\nrand.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]\nbias.unemp[, impute.ff := na.locf(UNRATE, na.rm = FALSE)]\n\n## Mean moving average with use of lookahead phenomen\nrand.unemp[, impute.rm.lookahead := rollapply(data=c(UNRATE,NA, NA), width=3,\n          FUN= function(x) {\n                         if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)\n                         })]         \nbias.unemp[, impute.rm.lookahead := rollapply(c(UNRATE, NA,NA), 3,\n            FUN= function(x) {\n                         if (!is.na(x[1])) x[1] else mean(x, na.rm = TRUE)\n                         })]         \n\n\n\n\n\n## Mean moving average withou use of lookahead phenomen\nrand.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,\n             function(x) {\n                         if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)\n                         })]         \nbias.unemp[, impute.rm.nolookahead := rollapply(c(NA, NA, UNRATE), 3,\n             function(x) {\n                         if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)\n                         })]    \n\n\n\n\n\n## linear interpolation fullfilling NA with linear interpolation between two data points\nrand.unemp[, impute.li := na.approx(UNRATE, maxgap=Inf)]\nbias.unemp[, impute.li := na.approx(UNRATE)]\n\nzz <- c(NA, 9, 3, NA, 3, 2,NA,5,6,10,NA,NA,NA,0)\nna.approx(zz, na.rm = FALSE, maxgap=2)\n##  [1]   NA  9.0  3.0  3.0  3.0  2.0  3.5  5.0  6.0 10.0   NA   NA   NA  0.0\nna.approx(zz, na.rm = FALSE, maxgap=Inf)\n##  [1]   NA  9.0  3.0  3.0  3.0  2.0  3.5  5.0  6.0 10.0  7.5  5.0  2.5  0.0\nna.approx(zz,xout=11, na.rm = FALSE, maxgap=Inf)\n## [1] 7.5\n\n\n\n\n\n## Using root mean square error to compare methods\nprint(rand.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),\n             .SDcols = c(\"impute.ff\", \"impute.rm.nolookahead\", \"impute.rm.lookahead\", \"impute.li\")])\n##      impute.ff impute.rm.nolookahead impute.rm.lookahead   impute.li\n## 1: 0.009476615           0.007698324         0.005860335 0.001494989\n\nprint(bias.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2, na.rm = TRUE)),\n             .SDcols = c(\"impute.ff\", \"impute.rm.nolookahead\", \"impute.rm.lookahead\", \"impute.li\")])\n##     impute.ff impute.rm.nolookahead impute.rm.lookahead   impute.li\n## 1: 0.04706013            0.02379379          0.01049718 0.003763425\n```\n:::\n\n\n## Metrics\n\n### autocorelation\n\nAutocorelation is measuring the direction of change basing on one point. Since the point on sinusoid close to each other have similar value this autocorelation is high if measuring on distance of pi /6 (0.52). In case of distance of 1 pi value is just opposite so ACF is equal -1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(data.table)\n## R\nx <- 1:100\ny <- sin(x * pi /6)\nplot(y, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nacf(y)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=100%}\n:::\n\n```{.r .cell-code}\n\n## R\ncor(y, shift(y, 1), use = \"pairwise.complete.obs\")\n## [1] 0.870187\ncor(y, shift(y, 2), use = \"pairwise.complete.obs\") \n## [1] 0.5111622\n```\n:::\n\n\n## Visualisation\n\nAllows for visualising multiple micro time series within dataset. It is called Gant chart,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(timevis)\nrequire(data.table)\ndonations <- fread(paste0(data_file_path,\"donations.csv\"))\nd <- donations[, .(min(timestamp), max(timestamp)), user]\nnames(d) <- c(\"content\", \"start\", \"end\")\nd <- d[start != end]\ntimevis(d[sample(1:nrow(d), 20)])\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-64daad4f11989b4766f5\" class=\"timevis html-widget\" style=\"width:100%;height:384px;\">\n<div class=\"btn-group zoom-menu\">\n<button type=\"button\" class=\"btn btn-default btn-lg zoom-in\" title=\"Zoom in\">+</button>\n<button type=\"button\" class=\"btn btn-default btn-lg zoom-out\" title=\"Zoom out\">-</button>\n</div>\n</div>\n<script type=\"application/json\" data-for=\"htmlwidget-64daad4f11989b4766f5\">{\"x\":{\"items\":[{\"content\":\"660\",\"start\":\"2017-08-23 14:49:18\",\"end\":\"2018-05-29 11:03:34\"},{\"content\":\"929\",\"start\":\"2015-08-08 12:34:03\",\"end\":\"2018-03-14 11:22:06\"},{\"content\":\"386\",\"start\":\"2015-04-08 14:57:46\",\"end\":\"2017-08-07 17:32:33\"},{\"content\":\"688\",\"start\":\"2017-08-10 22:58:21\",\"end\":\"2018-02-01 18:02:39\"},{\"content\":\"101\",\"start\":\"2016-09-24 13:22:33\",\"end\":\"2017-10-14 11:34:39\"},{\"content\":\"380\",\"start\":\"2017-02-15 13:54:13\",\"end\":\"2017-12-11 16:51:45\"},{\"content\":\"179\",\"start\":\"2018-03-17 22:12:10\",\"end\":\"2018-04-24 12:16:23\"},{\"content\":\"807\",\"start\":\"2017-10-01 20:53:05\",\"end\":\"2018-03-28 13:47:44\"},{\"content\":\"527\",\"start\":\"2016-06-22 16:35:36\",\"end\":\"2016-10-02 13:10:15\"},{\"content\":\"678\",\"start\":\"2016-05-13 18:41:28\",\"end\":\"2017-06-23 15:23:53\"},{\"content\":\"730\",\"start\":\"2016-10-17 12:46:53\",\"end\":\"2017-07-18 14:48:49\"},{\"content\":\"69\",\"start\":\"2017-06-14 12:50:54\",\"end\":\"2017-11-08 13:58:54\"},{\"content\":\"16\",\"start\":\"2017-05-31 15:59:39\",\"end\":\"2017-10-01 12:42:23\"},{\"content\":\"597\",\"start\":\"2017-08-24 11:58:39\",\"end\":\"2018-04-13 16:25:31\"},{\"content\":\"665\",\"start\":\"2016-01-18 21:27:10\",\"end\":\"2018-05-08 22:19:58\"},{\"content\":\"188\",\"start\":\"2016-09-22 14:22:17\",\"end\":\"2017-02-02 17:51:34\"},{\"content\":\"134\",\"start\":\"2015-02-11 20:47:34\",\"end\":\"2018-04-10 15:36:12\"},{\"content\":\"924\",\"start\":\"2018-04-05 13:36:40\",\"end\":\"2018-05-22 16:24:25\"},{\"content\":\"912\",\"start\":\"2018-03-07 17:45:30\",\"end\":\"2018-05-07 11:26:37\"},{\"content\":\"906\",\"start\":\"2017-02-16 20:59:12\",\"end\":\"2017-10-16 21:01:36\"}],\"groups\":null,\"showZoom\":true,\"zoomFactor\":0.5,\"fit\":true,\"options\":[],\"height\":null,\"timezone\":null,\"api\":[]},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n### partial-autocorelation\n\nPartial autocorelation shows which point have informational meaning and which simple derives from harmonical periods of time. For seasonal, wihtout noise process, PACF show which correlation for given delay, are the true ones, eliminating redunduntion.It helps to aproximate how much data do we need to poses to apply sufficient window for given time scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R\ny <- sin(x * pi /6)\nplot(y[1:30], type = \"b\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n\n```{.r .cell-code}\npacf(y)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=100%}\n:::\n:::\n\n\n## Simulation\n\n## Smoothing\n\nSmoothing is commonelly used forecasting method. Smoothed time series can be used as zero hypothesis to for testing more sophisticated methods.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nunemp = r.unemp\n#unemp.index = unemp.DATE\n\ndf = unemp.copy()\ndf = df[((df.DATE >=pd.to_datetime('2014-01-01')) & (df.DATE < pd.to_datetime('2019-01-01')))]\n## /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.  In a future version these will be considered non-comparable.Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n##   result = libops.scalar_compare(x.ravel(), y, op)\ndf = df.rename(columns={\"UNRATE\": \"data\"})\n#df.reset_index(drop=True, inplace=True)\n\n\ntrain = df[['data']].iloc[:-12, :]\ntest = df[['data']].iloc[-12:, :]\n# train.index = pd.to_datetime(train.index)\n# test.index = pd.to_datetime(test.index)\n## We can use the pandas.DataFrame.ewm() function to calculate the exponentially weighted moving average for a certain number of previous periods.\n```\n:::\n\n\n### moving average\n\nAn improvement over simple average is the average of n last points. Obviously the thinking here is that only the recent values matter. Calculation of the moving average involves what is sometimes called a \"sliding window\" of size n:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\ndef average(series):\n    return float(sum(series))/len(series)\n\n# moving average using n last points\ndef moving_average(series, n):\n    return average(series[-n:])\n\nmoving_average(train.data,4)\n## 4.2\n```\n:::\n\n\n### Weighted Moving Average\n\nA weighted moving average is a moving average where within the sliding window values are given different weights, typically so that more recent points matter more.\n\nInstead of selecting a window size, it requires a list of weights ([**which should add up to 1**]{.underline}). For example if we picked \\[0.1, 0.2, 0.3, 0.4\\] as weights, we would be giving 10%, 20%, 30% and 40% to the last 4 points respectively. In Python:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# weighted average, weights is a list of weights\ndef weighted_average(series, weights):\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series[-n-1] * weights[n]\n    return result\n  \nweights = [0.1, 0.15, 0.25, 0.5]\nweighted_average(train.data.values, weights)\n\n## 4.16\n```\n:::\n\n\n### exponentially weightening\n\nThe exponentially weighted function is calculated recursively:\n\n$$\\begin{split}\\begin{split}\ny_0 &= x_0\\\\\ny_t &= \\alpha x_t + (1 - \\alpha) y_{t-1} ,\n\\end{split}\\end{split}$$\n\nwhere alpha is smoothing factor $0 < \\alpha \\leq 1$ . The higher the α, the faster the method \"forgets\".\n\nThere is an aspect of this method that programmers would appreciate that is of no concern to mathematicians: it's simple and efficient to implement. Here is some Python. Unlike the previous examples, this function returns expected values for the whole series, not just one point.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# given a series and alpha, return series of smoothed points\ndef exponential_smoothing(series, alpha):\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n\n\nres_exp_smooth8 = exponential_smoothing(train.data.values, alpha=0.8)\nres_exp_smooth5 = exponential_smoothing(train.data.values, alpha=0.5)\nres_exp_smooth2 = exponential_smoothing(train.data.values, alpha=0.2)\n\n```\n:::\n\n\n\n\n\n\n### Conclusion\n\nI showed some basic forecasting methods: moving average, weighted moving average and, finally, single exponential smoothing. One very important characteristic of all of the above methods is that remarkably, they can only forecast a single point. That's correct, just one.\n\n### Double exponential smoothing\n\na.k.a Holt Method\n\nIn case of forecasting simple exponential weightening isn't giving good results for data posessing longterm trend. For this purpose it is good to apply method aimed for data with trend (Holt) or with trend and seasonality (Holt-Winter).\n\nDouble exponential smoothing is nothing more than exponential smoothing applied to both level and trend.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n# given a series and alpha, return series of smoothed points\ndef double_exponential_smoothing(series, alpha, beta):\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # we are forecasting\n          value = result[-1]\n        else:\n          value = series[n]\n        last_level= level\n        level =  alpha*value + (1-alpha)*(last_level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\n\nres_double_exp_smooth_alpha_9_beta9=double_exponential_smoothing(train.data.values, alpha=0.9, beta=0.9)\n```\n:::\n\n\n### Triple Exponential Smoothing\n\na.k.a Holt-Winters Method\n\n#### Initial Trend\n\nFor double exponential smoothing we simply used the first two points for the initial trend. With seasonal data we can do better than that, since we can observe many seasons and can extrapolate a better starting trend. The most common practice is to compute the average of trend averages across seasons.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\ndef initial_trend(series, slen):\n    sum = 0.0\n    for i in range(slen):\n        sum += float(series[i+slen] - series[i]) / slen\n    return sum / slen\n\nres_initial = initial_trend(train.data.values,12)\nres_initial\n## -0.07361111111111113\n```\n:::\n\n\nThe value of -0.074 can be interpreted that unemployment rate between first two years change on average by -0.074 between each pair of the same month.\n\n#### Initial Seasonal Components\n\nThe situation is even more complicated when it comes to initial values for the seasonal components. Briefly, we need to\n\n1.  compute the average level for every observed season (in our case YEAR) we have,\n\n2.  divide every observed value by the average for the season it's in\n\n3.  and finally average each of these numbers across our observed seasons.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef initial_seasonal_components(series, slen):\n    seasonals = {}\n    season_averages = []\n    n_seasons = int(len(series)/slen)\n    # compute season averages\n    for j in range(n_seasons):\n        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))\n    # compute initial values\n    for i in range(slen):\n        sum_of_vals_over_avg = 0.0\n        for j in range(n_seasons):\n            sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]\n        seasonals[i] = sum_of_vals_over_avg/n_seasons\n    return seasonals\n\ninitial_seasonal_components(train.data.values,12)\n## {0: 0.2833333333333332, 1: 0.2583333333333331, 2: 0.20833333333333304, 3: 0.10833333333333317, 4: 0.10833333333333339, 5: -0.01666666666666683, 6: -0.04166666666666696, 7: -0.04166666666666674, 8: -0.11666666666666714, 9: -0.216666666666667, 10: -0.21666666666666679, 11: -0.3166666666666669}\n```\n:::\n\n\nSeasonal values we can interpret as average distance value from seasonal average. We can see that January {0} is on higher than average and December value {11} is lower than average. We can see that those month differ from each other exactly with the power of those values\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf[pd.to_datetime(df.DATE).dt.month.isin([1,12])]\n##            DATE  data\n## 792  2014-01-01   6.6\n## 803  2014-12-01   5.6\n## 804  2015-01-01   5.7\n## 815  2015-12-01   5.0\n## 816  2016-01-01   4.8\n## 827  2016-12-01   4.7\n## 828  2017-01-01   4.7\n## 839  2017-12-01   4.1\n## 840  2018-01-01   4.0\n## 851  2018-12-01   3.9\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):\n    result = []\n    seasonals = initial_seasonal_components(series, slen)\n    for i in range(len(series)+n_preds):\n        if i == 0: # initial values\n            smooth = series[0]\n            trend = initial_trend(series, slen)\n            result.append(series[0])\n            continue\n        if i >= len(series): # we are forecasting\n            m = i - len(series) + 1\n            result.append((smooth + m*trend) + seasonals[i%slen])\n        else:\n            val = series[i]\n            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)\n            trend = beta * (smooth-last_smooth) + (1-beta)*trend\n            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]\n            result.append(smooth+trend+seasonals[i%slen])\n    return result\n\nres_triple_exp_smooth = triple_exponential_smoothing(train.data.values, 12, 0.7, 0.02, 0.9, 10)\n```\n:::\n\n\nA Note on α, β and γ\n\nYou may be wondering from where values 0.7, 0.02 and 0.9 for α, β and γ It was done by way of trial and error: simply running the algorithm over and over again and selecting the values that give you the smallest SSE. This process is known as fitting.\n\nThere are more efficient methods at zooming in on best values. One good algorithm for this is Nelder-Mead, which is what tgres uses.\n\n### fitting data\n\n\n::: {.cell}\n\n```{.python .cell-code}\nres = [res_exp_smooth8,res_exp_smooth5,res_exp_smooth2,res_double_exp_smooth_alpha_9_beta9,res_triple_exp_smooth]\nRMSE = []\ni=1\nfor i in range(len(res)):\n  RMSE.append(np.sqrt(np.mean(np.square((train.data.values[0:48]- res[i][0:48])))))\nRMSE\n## [0.029444314294186542, 0.08240165108170797, 0.2272843505233408, 0.10916100840899878, 0.06909009711283208]\n```\n:::\n\n\nIn case of fitting smoothed data to raw data, the best fit possess single exponenetial smoothing method with **alpha =0.8** (putting higher weight on most recent data). This is exactly what could be expected. Is it then the best **forecasting method** for my data?\n\nObviously not.\n\nSince all method take data point from time *t* for estimating smoothed value for time *t* such a models are not forecasting one's. We are dealing here with **lookahead** problem. In order to predict we are using data which shouldn't be available at the moment of making prediction.\n\nOut of three methods prediction capabilities posses Holt method (using trend to linearly predict further data points) and Holt-Winter method (using trend and seasonality to predict further data points).\n\n### plot\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport datetime\nplt.style.use('Solarize_Light2')\n\n   \nplt.clf()\nfig = plt.figure(figsize=(5,8))\n# f.set_figwidth(10)#inches\n# f.set_figheight(20)#inches\nax1 = fig.add_subplot(5, 1, 1) \nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth8, label='exp_smooth_alpha_0.8')\nax2 =fig.add_subplot(5, 1, 2)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth5, label='exp_smooth_alpha_0.5')\nax3 =fig.add_subplot(5, 1, 3)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_exp_smooth2, label='exp_smooth_alpha_0.2')\nax4 =fig.add_subplot(5, 1, 4)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_double_exp_smooth_alpha_9_beta9, label='res_double_exp_smooth_alpha_9_beta9')\nax5 =fig.add_subplot(5, 1, 5)\nplt.plot(train.data.values, label='raw')\nplt.plot(res_triple_exp_smooth, label='res_triple_exp_smooth')\nax1.set_title('raw data vs exponential forecast')\nax1.legend(loc=\"upper left\")\nax2.legend(loc=\"upper left\")\nax3.legend(loc=\"upper left\")\nax4.legend(loc=\"upper left\")\nax5.legend(loc=\"upper left\")\nax1.sharex(ax5)\nax2.sharex(ax5)\nax3.sharex(ax5)\nax4.sharex(ax5)\n\nfig.tight_layout()\nfig.savefig('index_files/figure-html/unnamed-chunk-15-1.png', bbox_inches='tight')\n\nplt.show()\n\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=100%}\n:::\n:::\n\n\n## regression\n\n### autoregression models\n\nAs name suggest autoregression is regression made upon past values. The simplest autoregression model is known as AR(1): $y_t=b_0+b_1*y_{t-1}+e_t$ $e_t$ is changeable within time error with stable variance and mean = 0.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/visjs-7.4.9/vis-timeline-graph2d.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/visjs-7.4.9/vis-timeline-graph2d.min.js\"></script>\n<link href=\"../../site_libs/timeline-0.4/timevis.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/timevis-binding-2.1.0/timevis.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}