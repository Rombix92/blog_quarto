{
  "hash": "e490e4c904668cc34a2e177da44954db",
  "result": {
    "markdown": "---\ntitle-block-banner: true\nauthor: Łukasz Rąbalski\ntitle: NLP - multiple methods and concepts\ndescription: \"Notebook presenting several concept denoting to NLP like: \\n tockenization, \\n creating and saving embeddings, \\n using pretrained embeding, \\n visualizing embedding, \\n sequncial and bag of words approach to NLP models.\"\nformat:\n  html:\n    code-copy: true\n    code-line-numbers: true\n    code-fold: false\n    code-tools: true\n    code-summary: \"Show the code\"\n    toc: true\n    toc-location: left\n    number-sections: true\n    toc-depth: 5\n    embed-resources: true\ncategories: [\"Python\",\"NLP\", \"Tensorflow\"]\ntags: [\"Python\", \"NLP\", \"Neural Network\", \"Tensorflow\", \"embeddings\", \"tockenization\"]\neditor: source\nfig.height: 4\nout.width: '100%'\neval: FALSE\ninclude: TRUE  #prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho: TRUE  #echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nwarning: FALSE\nmessage: FALSE\nerror: FALSE\n---\n\n\n\n\n\n\n\n## Importing libraries\n\n\n::: {.cell tags='[]'}\n\n```{.python .cell-code}\nimport pandas as pd\nimport re #regular expression\n#pod maile\nimport datetime\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n#import tensorflow_hub as hub\n#import tensorflow_text\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n::: {.cell tags='[]'}\n\n```{.python .cell-code}\n# !pip install matplotlib==3.6.0\n!pip install scikit-learn==1.2.1\n# !pip install tensorflow_text\n```\n:::\n\n::: {.cell tags='[]'}\n\n```{.python .cell-code}\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\ntf.config.list_physical_devices('GPU')\n```\n:::\n\n\n\n## Preparing data\n\n### preparing functions\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndef df_to_dataset_maszyna(dataframe, shuffle=True, batch_size=32):\n    sentencex = dataframe.filter(regex=(\"tresc_maila_cutted_clean\")).tresc_maila_cutted_clean.values\n    categories = dataframe.filter(regex=(\"_KATEGOR$\")).values\n    ds = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            tf.cast(sentencex, tf.string),\n            tf.cast(categories, tf.int32)\n        )\n    ).batch(batch_size)\n    )\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.prefetch(batch_size)\n    return ds  \n\n\n## funkcja czyszczaca maile\ndef preprocess_text(sen):\n\n    # transorm  telephone number into password\n    sentence = re.sub('(?<=\\D[4-9])\\d{8}(?=\\D)', ' dziewiec cyfr ', sen) #zamieniam 9 cyfr na haslo, za wyjatkiem numerow zaczynajaych sie od 1 bo to id_rezer + zastanwiam jedną cyfrę by jak wyrzucam duplikatu kilka powieleniem zostało w bazie (max 6 (4-9))\n    sentence = re.sub('\\d{3} \\d{3} \\d{3}', 'dziewiec cyfr ', sentence)\n    sentence = re.sub('\\d{3}[\\s-]\\d{3}[\\s-]\\d{3}', 'dziewiec cyfr ', sentence)\n    sentence = re.sub('\\d{2} \\d{3} \\d{2} \\d{2}', 'dziewiec cyfr ', sentence)\n    #numer konta\n    sentence = re.sub('\\d{26}', 'dwadziescia szesc cyfr', sentence)\n    #transform date into password\n    sentence = re.sub('\\s\\d{2}[.-]\\d{2}\\s', 'data', sentence)\n    sentence = re.sub('\\d{1,4}[.-]\\d{1,2}[.-]\\d{1,4}', 'data', sentence)\n    # Remove numbers\n    sentence = re.sub('\\d', ' ', sentence)\n    # Remove punctuations \n    sentence = re.sub('[-!_#\"*?:;,.><+=\\\\\\)(\\/]', \" \", sentence)\n    sentence = re.sub('&nbsp', ' ', sentence)\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z](?=(\\s+)|($))\", ' ', sentence)\n    sentence = re.sub(r\"^[a-zA-Z](?=(\\s+)|($))\", ' ', sentence)\n    # Removing specific week ending sentence\n    sentence = re.sub(r\" użytkownik \", ' ', sentence)\n    # Removing specific week ending sentence\n    sentence = re.sub(r\"( pon )|( wt )|( śr )|( czw )|( czwartek )|( pt )|(fri)|( sob )|( niedz )\", ' ', sentence)\n    # Removing specific month ending sentence\n    sentence = re.sub(r\"( st )|( lut )|( mar )|( kw )|( cze )|( lip )| (lipca)|( sierp )|( wrz )|( paź )|( lis )|( gru )\", ' ', sentence)\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n    return sentence\n\n\n\n##funkcja tworzaca kolumne z maile do uczenia\n### tabele musi zawierac nastepujace kolumny\n#### tresc_maila_cutted\n#### client_email\ndef przetworzenie_kolumny_z_mailem(df_start):\n  #wczytuje plik z r\n  df_start = pd.DataFrame(df_start)\n  X = []\n  sentences = list(df_start[\"tresc_maila_cutted\"])\n  for i in range(len(sentences)):\n    #print(i)\n    X.append(preprocess_text(sentences[i]))\n  #przetworzanie info o mailu\n  X_email = []\n  #emaile_do = list(df_start[\"email_do\"])\n  if typ=='agent':\n    client_email = list(df_start[\"client_email\"])\n    print('x')\n    for i in range(len(client_email)):\n      #print(i)\n      #' od ' + re.sub('@.*', '', client_email[i]) + \n      #+ ' mail do ' + re.sub('@.*', '', emaile_do[i]) + ' do domeny ' + re.sub('.*@', '', emaile_do[i])\n      email_text = ' mail od ' + re.sub('.*@', '', client_email[i])\n      X_email.append(email_text)\n  \n  df_start_clean = df_start\n  #wstawiam nowa kolumne na drugim miejscu\n  df_start_clean.insert(1, \"tresc_maila_cutted_clean\", X, True)\n  print('x2')\n  if typ=='agent':\n      df_start_clean.insert(2, \"email_tekst\", X_email, True)\n  print('x3')\n  #usuwanie zduplikowanych wartosci\n  #df_start_clean = df_start_clean.drop_duplicates(subset=['tresc_maila_cutted_clean'])\n  \n  \n  df_start_clean['tresc_maila_cutted_clean'].replace('', np.nan, inplace=True)\n  df_start_clean['tresc_maila_cutted_clean'].replace(' ', np.nan, inplace=True)\n  df_start_clean.dropna(subset=['tresc_maila_cutted_clean'], inplace=True)\n  df_start_clean=pd.DataFrame(df_start_clean)\n  if typ=='agent':\n      df_start_clean['tresc_maila_cutted_clean'] = df_start_clean['tresc_maila_cutted_clean'] + df_start_clean['email_tekst']\n  df_start_clean=pd.DataFrame(df_start_clean)\n  #df_start_clean['tresc_maila_cutted_clean'][0]\n  #df_start_clean.info()\n  #df_start_clean.nunique()\n  #print(df_start_clean.head(4))\n  return df_start_clean\n\n\n\ndef predykcja(typ,nr_modelu, df):\n  EXPORT_PATH = \"/home/lrabalski/Text Classification/USE/model_\"+typ+\"/\"+str(nr_modelu)\n  print(EXPORT_PATH)\n  model = tf.keras.models.load_model(EXPORT_PATH)\n  # df_maile_do_predykcji=df_maile_do_predykcji.reset_index()\n  # df_maile_do_predykcji.head\n  df.reset_index(drop=True,inplace=True)\n  df_maile_pred=pd.DataFrame(model.predict(\n    df[['tresc_maila_cutted_clean']].values\n    ))\n  df_maile_pred_all=pd.concat([df,df_maile_pred], axis=1#,ignore_index=True\n  )\n  return df_maile_pred_all\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntyp=  'client' #   'agent' #  \nsave_dir ='/data/lrabalski/DOP/call_back_models/'\n```\n:::\n\n\n### preparing dataframe\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndf=pd.read_csv('https://raw.githubusercontent.com/NavePnow/Google-BERT-on-fake_or_real-news-dataset/master/data/fake_or_real_news.csv',\n              skiprows=1,\n              names=['title','text','label','title_vectors']).drop('title_vectors',axis=1).reset_index()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\n#### convert label into binary text\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndf.head()\nfor i in range(len(df)):\n    if df.loc[i, 'label'] == \"REAL\": #REAL equal 0\n        df.loc[i, 'label'] = 0\n    elif df.loc[i, 'label'] == \"FAKE\": #FAKE equal 1\n        df.loc[i, 'label'] = 1\n    if df.loc[i, 'text'] == \"\":\n        df = df.drop([i])\n\ndf['label'] = pd.Categorical(df['label'])\ndf['label'] = df.label.cat.codes\ndf['client_email']='xxx@xx.pl'\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndf = df.rename(columns={\"text\":\"tresc_maila_cutted\", \"label\": \"final__KATEGOR\"})\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndf_maile = df\n```\n:::\n\n\n### preparing datasets\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndf_start_clean = przetworzenie_kolumny_z_mailem(df_maile)\n\n\ntrain_df, test_df = train_test_split(df_start_clean, test_size=0.15,random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2,random_state=42) \n\nprint(\"Rozmiary datasetów\")\nprint(f\"Zbiór uczący {len(train_df)}\")\nprint(f\"Zbiór walidacyjny {len(val_df)}\")\nprint(f\"Zbiór testowy {len(test_df)}\")\n\n\ntrain_ds = df_to_dataset_maszyna(train_df, shuffle=True)\nval_ds = df_to_dataset_maszyna(val_df, shuffle=True)\ntest_ds = df_to_dataset_maszyna(test_df, shuffle=True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntmp_ds = df_to_dataset_maszyna(train_df, shuffle=True, batch_size=1)\nfor i in tmp_ds.shuffle(len(train_df)).take(2):\n    print(list(i))\n```\n:::\n\n\n### general parametrization\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\n\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nvocab = text_vectorization.get_vocabulary()\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n```\n:::\n\n\n### assesing models\n\n#### learning rate\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nhistory_lists[0]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nhistory_lists=list()\nfiles_list=list()\nfor (dir_path, dir_names, file_names) in os.walk(save_dir):\n    dir_names = [dir_path + '/' + x for x in  file_names if bool(re.match(r'history.pkl', x))]\n    history_lists = history_lists + dir_names   \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport pickle\ndf_model_accuracy = pd.DataFrame(columns=['model','val_accuracy'])\n\nfor i in range(len(history_lists)):\n    with open(history_lists[i], 'rb') as f:\n        data = pickle.load(f)\n    model=re.search(r'(?<=call_back_models/)[A-Za-z|_]+', history_lists[i]).group()\n    acc=max(data['val_accuracy'])\n    df_model_accuracy = df_model_accuracy.append(pd.DataFrame({'model':model, 'val_accuracy':acc} , index=[0]))\n\ndf_model_accuracy\n```\n:::\n\n\n#### visualizing learning rate\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## visualization of loss\nimport matplotlib.pyplot as plt \n\nplt.clf()\nhistory_dict = history.history \nloss_values = history_dict[\"loss\"] \nval_loss_values = history_dict[\"val_loss\"] \nepochs = range(1, len(loss_values) + 1) \nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\") \nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") \nplt.title(\"Training and validation loss\") \nplt.xlabel(\"Epochs\") \nplt.ylabel(\"Loss\") \nplt.legend() \nplt.show() \n```\n:::\n\n\n#### accuracy on test data\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## accuracy on test data\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\n##model = keras.models.load_model(\"embeddings_bidir_gru.keras\") \nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\nmodel.summary()\n```\n:::\n\n\n## Models\n\n### Encoding\n\nWord embeddings are vector representations of words thatmap human language into a structured geometric space.\nWord embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors)\n\nthe vectors obtained through \n * one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (the same dimensionality as the number of words in the vocabulary), \n * word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors)\n\n#### learned word embedding\n\n##### parameters\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\nimport os\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n\nsave_dir+model_name\n```\n:::\n\n\n##### learning\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\n\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nvocab = text_vectorization.get_vocabulary()\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = layers.Embedding(input_dim=max_tokens, output_dim=256, name=\"embedding\")(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n  \ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/\"+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(int_train_ds, validation_data=int_val_ds, epochs=20\n                   , callbacks=callbacks\n                   )\n```\n:::\n\n\n##### saving history\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)\n            \n## with open(save_dir+model_name +'/history.pkl', 'rb') as f:\n##     data = pickle.load(f)\n```\n:::\n\n\n##### saving embeddings \nhttps://www.tensorflow.org/text/guide/word_embeddings\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\n\n## savings for visualisation\nimport io\n\nweights = model.get_layer('embedding').get_weights()[0]\nvocab = text_vectorization.get_vocabulary()\n\nprint(weights.shape)\nprint(len(vocab))\n\nout_v = io.open(save_dir+model_name +'/vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open(save_dir+model_name +'/metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0 or not bool(re.match(r'[a-zA-z]', word)):\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()\n\n\n##len(pd.read_csv(save_dir+model_name +'/vectors.tsv',encoding='UTF-8')) == len(pd.read_csv(save_dir+model_name +'/metadata.tsv',encoding='UTF-8'))\n\n## to observe \n##http://projector.tensorflow.org/\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nvocab[4]\nweights[4]\n```\n:::\n\n\n##### visualizing embeddings\nhttps://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\nhttps://projector.tensorflow.org/\n\n#### fine tunning\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmodel = tf.keras.models.load_model(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntry:\n    initial_epochs = history.epoch[-1]\nexcept NameError:\n    initial_epochs = 0\ninitial_epochs=0\n\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\nfine_tune_epochs = 20\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\nhistory_fine = model.fit(int_train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=initial_epochs,\n                         validation_data=int_val_ds, \n                         callbacks=callbacks\n                        )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nx = model.get_layer('embedding').get_weights()[0:2]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport io\n\nweights = model.get_layer('embedding').get_weights()[0]\nvocab = text_vectorization.get_vocabulary()\n\nprint(weights.shape)\nprint(len(vocab))\n\nout_v = io.open(save_dir+model_name +'/vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open(save_dir+model_name +'/metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0 or not bool(re.match(r'[a-zA-z]', word)):\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()\n```\n:::\n\n\n#### PRETRAINED WORD EMBEDDINGS\n\nThe rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect that the features you need are fairly generic\n\n##### my_embeddings\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'learned_embeddings_bidir_lstm'\nmetadata = pd.read_csv(save_dir+model_name+'/metadata.tsv',encoding='UTF-8', header=0,names=['word'])\nvectors = pd.read_csv(save_dir+model_name+'/vectors.tsv',encoding='UTF-8', header=0,names=['vector'])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport numpy\nembeddings_index = {} \nfor index, word in enumerate(metadata.word):\n    coefs = numpy.fromstring(vectors.vector[index], \"f\", sep=\"\\t\")\n    embeddings_index[word] = coefs\n\nembedding_dim = coefs.shape[0]\nembedding_dim\n\n##embeddings_index\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\n## Next, let’s build an embedding matrix that you can load into an Embedding layer. \n##It must be a matrix of shape (max_words, embedding_dim), where each entry i contains \n##the embedding_dim-dimensional vector for the word of index i in the reference word index \n##(built during tokenization).\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nvocabulary = text_vectorization.get_vocabulary()             \nword_index = dict(zip(vocabulary, range(len(vocabulary))))   \n \nembedding_matrix = np.zeros((max_tokens, embedding_dim))     \nfor word, i in word_index.items():\n    if i < max_tokens:\n        embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:                         \n        embedding_matrix[i] = embedding_vector   \n\nembedding_layer = layers.Embedding(\n    max_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=True\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = embedding_layer(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n  \n\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=3)\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nlist(int_val_ds)\n```\n:::\n\n\n##### USE\n\n###### DENSE\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'USE_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntext_input = tf.keras.Input(shape=(), name=\"sentence\", dtype=tf.string)\ntext_embed = tfh.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\", \n                            name=\"text_embedding\")(text_input)\ndense_relu_64 = tf.keras.layers.Dense(256, activation=\"relu\")(text_embed)    \ndense_relu_64_2 = tf.keras.layers.Dense(256, activation=\"relu\")(dense_relu_64)  \nout  =  tf.keras.layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(dense_relu_64_2)\nmodel = tf.keras.Model(inputs=text_input, outputs=out)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(train_ds, \n                    validation_data=val_ds, \n                    epochs=20\n                   , callbacks=callbacks\n                   )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'USE_embeddings_bidir_lstm'\nimport pickle\n\n## with open(save_dir+model_name +'/history.pkl', 'wb') as f:\n##             pickle.dump(history.history, f)\n            \nwith open(save_dir+model_name +'/history.pkl', 'rb') as f:\n    data = pickle.load(f)\n```\n:::\n\n\n###### LSTM\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'USE_LSTM'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntext_input = tf.keras.Input(shape=(), name=\"sentence\", dtype=tf.string)\ntext_embed = tfh.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\", \n                            name=\"text_embedding\")(text_input)\nx = layers.Bidirectional(layers.LSTM(32))(text_embed)   \nx = layers.Dropout(0.5)(x) \noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)    \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\n\nmodel.summary()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nhistory = model.fit(train_ds, \n                    validation_data=val_ds, \n                    epochs=20\n                   , callbacks=callbacks\n                   )\n```\n:::\n\n\n### Sequence\nWhat if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what sequence models are about.\n\n#### LSTM\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel_name = 'raw_sequence_bidir_lstm'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\n##Preparing integer sequence datasets\nfrom tensorflow.keras import layers\n\nmax_length = 600 #In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words\nmax_tokens = 20000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n##**A sequence model built on one-hot encoded vector sequences**\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nlist(int_train_ds.take(1))[0][0][0]\n##The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary).\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nfrom tensorflow.keras import layers\n\nmax_length = 20\nmax_tokens = 50\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\nsentence = [ \"I write, erase, rewrite, Erase again, and then,A poppy blooms.\" ]\n\ntext_vectorization.adapt(sentence)\nvocabulary=text_vectorization.get_vocabulary()\nsentence = text_vectorization(sentence)\n\nimport tensorflow as tf\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = tf.one_hot(inputs, depth=max_tokens)\nmodel = keras.Model(inputs, embedded)\n\npredict=model.predict(sentence)\npredict.shape\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport tensorflow as tf\ninputs = keras.Input(shape=(None,), dtype=\"int64\")    \nembedded = tf.one_hot(inputs, depth=max_tokens)       \nx = layers.Bidirectional(layers.LSTM(32))(embedded)   \nx = layers.Dropout(0.5)(x) \noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)    \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nmodel.fit(int_train_ds, \n          validation_data=int_val_ds, \n          epochs=2,\n          callbacks=callbacks)\n```\n:::\n\n\n##### Wnioski\n\nA first observation: this model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000) (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review.\n\n#### Transformers\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport os\nmodel_name = \"full_transformer_encoder\"\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):  \n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(                          \n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)              \n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n  \n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions                        \n \n    def compute_mask(self, inputs, mask=None):                             \n        return tf.math.not_equal(inputs, 0)                                \n \n    def get_config(self):                                                  \n        config = super().get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config\n    \n\n  \nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim                         \n        self.dense_dim = dense_dim                         \n        self.num_heads = num_heads                         \n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n    def call(self, inputs, mask=None):                    \n        if mask is not None:                              \n            mask = mask[:, tf.newaxis, :]                 \n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n  \n    def get_config(self):                                 \n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nvocab_size = 20000 \nsequence_length = 600 \nembed_dim = 256 \nnum_heads = 2 \ndense_dim = 32 \n  \ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)   \nx = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                    save_best_only=True)\n] \nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'full_transformer_encoder'\nmodel = tf.keras.models.load_model(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                  custom_objects={\"TransformerEncoder\": TransformerEncoder,\n                    \"PositionalEmbedding\": PositionalEmbedding})\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntry:\n    initial_epochs = history.epoch[-1]\nexcept NameError:\n    initial_epochs = 0\ninitial_epochs=0\n\n\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\nfine_tune_epochs = 2\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\nhistory_fine = model.fit(int_train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=initial_epochs,\n                         validation_data=int_val_ds, \n                         callbacks=callbacks\n                        )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nhistory_fine.history\n```\n:::\n\n\n#### BERT\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nbert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nbert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntext_test = ['this is such an amazing movie!']\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nbert_model = hub.KerasLayer(tfhub_handle_encoder)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nbert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\n```\n:::\n\n\n\n### Bag of words\n\n#### Unigram\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport os\nmodel_name = 'Unigram_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n\n##### text vectorization\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nfrom tensorflow.keras.layers import TextVectorization\n\ntext_vectorization = TextVectorization(\n    max_tokens=20000,  #Limit the vocabulary to the 20,000 most frequent words                    \n    output_mode=\"multi_hot\",   #Encode the output tokens as multi-hot binary vectors.                     \n)\ntext_vectorization.adapt(text_only_train_ds)\n\n##Prepare processed versions of our training, validation, and test dataset. Make sure to specify num_parallel_calls to leverage multiple CPU cores\n\nbinary_1gram_train_ds = train_ds.map(               \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)                                                   \nbinary_1gram_val_ds = val_ds.map(                   \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)                           \nbinary_1gram_test_ds = test_ds.map(                 \n    lambda x, y: (text_vectorization(x), y),        \n    num_parallel_calls=4)\n\n\nfor inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break\n```\n:::\n\n\n###### playground\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntext_only_train_ds = tf.data.Dataset.from_tensor_slices(train_df['tresc_maila_cutted_clean'])\ntext_vectorization.adapt(text_only_train_ds)\n\nvocabulary = text_vectorization.get_vocabulary()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nvocabulary[0:5]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntest_sentence = \"do do na na nie proszę proszę\" \nencoded_sentence = text_vectorization(test_sentence)\nencoded_sentence[0:100]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ninverse_vocab = dict(enumerate(vocabulary))\ninverse_vocab[1]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ndecoded_sentence = \" \".join(inverse_vocab[int(i)] for i, check in enumerate(encoded_sentence.numpy()) if check==1)\ndecoded_sentence\n```\n:::\n\n\n##### building model\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n  \ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = keras.Input(shape=(max_tokens,))\n    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(len(train_df.filter(regex=(\"_KATEGOR$\")).columns), activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"rmsprop\",\n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\n\n##We call cache() on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory.                  \nhistory=model.fit(binary_1gram_train_ds.cache(), \n          validation_data=binary_1gram_val_ds.cache(),     \n          epochs=100,\n          callbacks=callbacks)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)\n```\n:::\n\n\n#### BIGRAMS WITH TF-IDF ENCODING \n\n\n::: {.cell}\n\n```{.python .cell-code}\n##TF-IDF stands for “term frequency, inverse document frequency.”\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'bigram_dense'\nif not os.path.exists(save_dir+model_name):\n   os.makedirs(save_dir+model_name)\n```\n:::\n\n\n##### text vectorization\n\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntext_vectorization = TextVectorization(\n    ngrams=3,\n    max_tokens=20000, \n    #output_mode=\"count\" #counting how many times each word or N-gram occurs\n    output_mode=\"tf_idf\"\n)\n\ntext_vectorization.adapt(text_only_train_ds)   \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nvocabulary = text_vectorization.get_vocabulary()\nvocabulary\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntrain_ds\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\ntfidf_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nmodel_name = 'bigram_dense'\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n     keras.callbacks.ModelCheckpoint(save_dir+model_name+\"/model_\"+typ+'_'+model_name+\".keras\",\n                                     save_best_only=True)\n ]\nmodel.fit(tfidf_2gram_train_ds.cache(),\n          validation_data=tfidf_2gram_val_ds.cache(),\n          epochs=100,\n          callbacks=callbacks)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n##| tags: []\nimport pickle\n\nwith open(save_dir+model_name +'/history.pkl', 'wb') as f:\n            pickle.dump(history.history, f)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}