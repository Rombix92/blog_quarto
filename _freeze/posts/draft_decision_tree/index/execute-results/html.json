{
  "hash": "73eea205a0d13001b95eb0cd21f7db9e",
  "result": {
    "markdown": "---\ntitle: \"draft\"\ndate: \"2022-11-20\"\ntags: []\ncategories: []\ndraft: TRUE\ntoc: TRUE\neditor: source\n---\n\n\n\n\n\n::: {.cell}\n\n```\n## \n##          0          1 \n## 0.04940961 0.95059039\n```\n:::\n\n\n# dataset exploration\n\n# decision tree\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```\n## Call:\n## rpart(formula = success ~ ., data = diamonds, method = \"class\")\n##   n= 37179 \n## \n##           CP nsplit rel error    xerror       xstd\n## 1 0.17583016      0 1.0000000 1.0000000 0.02274794\n## 2 0.05988024      2 0.6483397 0.6483397 0.01848318\n## 3 0.01061513      3 0.5884594 0.5900925 0.01765957\n## 4 0.01000000      5 0.5672292 0.5819271 0.01754061\n## \n## Variable importance\n## random  depth  table      x      z  carat      y \n##     50     35     10      2      1      1      1 \n## \n## Node number 1: 37179 observations,    complexity param=0.1758302\n##   predicted class=1  expected loss=0.04940961  P(node) =1\n##     class counts:  1837 35342\n##    probabilities: 0.049 0.951 \n##   left son=2 (5318 obs) right son=3 (31861 obs)\n##   Primary splits:\n##       random < 0.9000108 to the right, improve=1087.58100, (0 missing)\n##       depth  < 63.05     to the right, improve=1035.24300, (0 missing)\n##       table  < 62.2      to the right, improve= 182.86980, (0 missing)\n##       x      < 4.275     to the left,  improve=  16.17553, (0 missing)\n##       y      < 5.405     to the right, improve=  10.90541, (0 missing)\n##   Surrogate splits:\n##       depth < 63.05     to the right, agree=0.872, adj=0.107, (0 split)\n##       table < 62.2      to the right, agree=0.860, adj=0.019, (0 split)\n##       z     < 6.255     to the right, agree=0.857, adj=0.000, (0 split)\n##       price < 338.5     to the left,  agree=0.857, adj=0.000, (0 split)\n## \n## Node number 2: 5318 observations,    complexity param=0.1758302\n##   predicted class=1  expected loss=0.3454306  P(node) =0.1430377\n##     class counts:  1837  3481\n##    probabilities: 0.345 0.655 \n##   left son=4 (658 obs) right son=5 (4660 obs)\n##   Primary splits:\n##       depth < 63.05     to the right, improve=625.66900, (0 missing)\n##       table < 60.1      to the right, improve=115.30470, (0 missing)\n##       x     < 4.275     to the left,  improve= 43.85720, (0 missing)\n##       y     < 4.295     to the left,  improve= 32.64826, (0 missing)\n##       carat < 0.625     to the right, improve= 26.28056, (0 missing)\n##   Surrogate splits:\n##       z      < 5.345     to the right, agree=0.878, adj=0.012, (0 split)\n##       carat  < 2.475     to the right, agree=0.877, adj=0.003, (0 split)\n##       table  < 51.5      to the left,  agree=0.877, adj=0.003, (0 split)\n##       x      < 3.885     to the left,  agree=0.877, adj=0.003, (0 split)\n##       random < 0.9000166 to the left,  agree=0.877, adj=0.003, (0 split)\n## \n## Node number 3: 31861 observations\n##   predicted class=1  expected loss=0  P(node) =0.8569623\n##     class counts:     0 31861\n##    probabilities: 0.000 1.000 \n## \n## Node number 4: 658 observations\n##   predicted class=0  expected loss=0.009118541  P(node) =0.01769816\n##     class counts:   652     6\n##    probabilities: 0.991 0.009 \n## \n## Node number 5: 4660 observations,    complexity param=0.05988024\n##   predicted class=1  expected loss=0.2542918  P(node) =0.1253396\n##     class counts:  1185  3475\n##    probabilities: 0.254 0.746 \n##   left son=10 (416 obs) right son=11 (4244 obs)\n##   Primary splits:\n##       table < 60.1      to the right, improve=130.47670, (0 missing)\n##       depth < 60.15     to the left,  improve= 87.16630, (0 missing)\n##       x     < 4.275     to the left,  improve= 44.03174, (0 missing)\n##       z     < 2.595     to the left,  improve= 34.05343, (0 missing)\n##       carat < 0.295     to the left,  improve= 28.86042, (0 missing)\n##   Surrogate splits:\n##       depth < 58.15     to the left,  agree=0.914, adj=0.034, (0 split)\n## \n## Node number 10: 416 observations\n##   predicted class=0  expected loss=0.3677885  P(node) =0.01118911\n##     class counts:   263   153\n##    probabilities: 0.632 0.368 \n## \n## Node number 11: 4244 observations,    complexity param=0.01061513\n##   predicted class=1  expected loss=0.2172479  P(node) =0.1141505\n##     class counts:   922  3322\n##    probabilities: 0.217 0.783 \n##   left son=22 (1731 obs) right son=23 (2513 obs)\n##   Primary splits:\n##       table < 57.05     to the right, improve=59.71914, (0 missing)\n##       depth < 62.75     to the right, improve=53.42411, (0 missing)\n##       x     < 4.275     to the left,  improve=43.09707, (0 missing)\n##       y     < 4.295     to the left,  improve=28.88683, (0 missing)\n##       z     < 2.575     to the left,  improve=28.47218, (0 missing)\n##   Surrogate splits:\n##       depth < 60.65     to the left,  agree=0.660, adj=0.167, (0 split)\n##       y     < 7.085     to the right, agree=0.609, adj=0.040, (0 split)\n##       x     < 6.935     to the right, agree=0.608, adj=0.039, (0 split)\n##       carat < 1.345     to the right, agree=0.607, adj=0.037, (0 split)\n##       z     < 4.355     to the right, agree=0.604, adj=0.030, (0 split)\n## \n## Node number 22: 1731 observations,    complexity param=0.01061513\n##   predicted class=1  expected loss=0.3183131  P(node) =0.04655854\n##     class counts:   551  1180\n##    probabilities: 0.318 0.682 \n##   left son=44 (71 obs) right son=45 (1660 obs)\n##   Primary splits:\n##       x     < 4.275     to the left,  improve=30.83504, (0 missing)\n##       depth < 62.75     to the right, improve=21.23691, (0 missing)\n##       carat < 0.295     to the left,  improve=18.51068, (0 missing)\n##       y     < 4.295     to the left,  improve=18.39160, (0 missing)\n##       z     < 2.595     to the left,  improve=16.82683, (0 missing)\n##   Surrogate splits:\n##       y     < 4.305     to the left,  agree=0.985, adj=0.634, (0 split)\n##       carat < 0.295     to the left,  agree=0.984, adj=0.620, (0 split)\n##       z     < 2.595     to the left,  agree=0.982, adj=0.563, (0 split)\n##       price < 383.5     to the left,  agree=0.961, adj=0.042, (0 split)\n## \n## Node number 23: 2513 observations\n##   predicted class=1  expected loss=0.1476323  P(node) =0.06759192\n##     class counts:   371  2142\n##    probabilities: 0.148 0.852 \n## \n## Node number 44: 71 observations\n##   predicted class=0  expected loss=0.2253521  P(node) =0.00190968\n##     class counts:    55    16\n##    probabilities: 0.775 0.225 \n## \n## Node number 45: 1660 observations\n##   predicted class=1  expected loss=0.2987952  P(node) =0.04464886\n##     class counts:   496  1164\n##    probabilities: 0.299 0.701\n```\n:::\n\n\nBehind the scenes rpart() is automatically applying a range of cost complexity (α values to prune the tree). To compare the error for each α value, rpart() performs a 10-fold CV (by default). \n\n\n\nIn this example we find diminishing returns after 6 terminal nodes as illustrated in Figure below\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=100%}\n:::\n:::\n\ny-axis is the CV error, \nlower x-axis is the cost complexity (α) value, \nupper x-axis is the number of terminal nodes (i.e., tree size = |T|)\n\nYou may also notice the dashed line which goes through the point |T|=4. Breiman (1984) suggested that in actual practice, it’s common to instead use the smallest tree within 1 standard error (SE) of the minimum CV error (this is called the 1-SE rule). Thus, we could use a tree with 3 terminal nodes and reasonably expect to experience similar results within a small margin of error.\n\n\nTo illustrate the point of selecting a tree with 6 terminal nodes (or 4 if you go by the 1-SE rule), we can force rpart() to generate a full tree by setting cp = 0 (no penalty results in a fully grown tree). Figure below shows that after 4 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can significantly prune our tree and still achieve minimal expected error.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n\nSo, by default, rpart() is performing some automated tuning, with an optimal subtree of 6 total splits, 6 terminal nodes, and a cross-validated SSE of 0.553.\n\n\n::: {.cell}\n\n```\n##           CP nsplit rel error    xerror       xstd\n## 1 0.17583016      0 1.0000000 1.0000000 0.02274794\n## 2 0.05988024      2 0.6483397 0.6483397 0.01848318\n## 3 0.01061513      3 0.5884594 0.5900925 0.01765957\n## 4 0.01000000      5 0.5672292 0.5819271 0.01754061\n```\n:::\n\n\n## dealing with imbalance\n### loss matrix\n\nYou can include a loss matrix, changing the relative importance of misclassifying a default as non-default versus a non-default as a default. You want to stress that misclassifying a default as a non-default should be penalized more heavily. \n\n\n::: {.cell}\n\n:::\n\n\nDoing this, you are constructing a 2x2-matrix with zeroes on the diagonal and changed loss penalties off-diagonal. The default loss matrix is all ones off-diagonal.\n\npenalization that is 20 times bigger when misclassifying an actual default as a non-default.\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n\n\n\n### weights & costs\nThe weights is for rows (e.g. give higher weight to smaller class), the cost is for columns.\n\nweights\n\n    optional case weights.\n\ncost\n\n    a vector of non-negative costs, one for each variable in the model. Defaults to one for all variables. These are scalings to be applied when considering splits, so the improvement on splitting on a variable is divided by its cost in deciding which split to choose.\n\n\n## bagging\n\nBootstrapping  can be used to create an _ensemble_ of predictions. Bootstrap aggregating, also called _bagging_, is one of the first ensemble algorithms machine learning practitioners learn and is designed to improve the stability and accuracy of regression and classification algorithms. \nBy model averaging, bagging helps to reduce variance and minimize overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.\n\n\n::: {.cell}\n\n```\n## Error in library(doParallel): there is no package called 'doParallel'\n## Error in library(caret): there is no package called 'caret'\n## Error in library(ipred): there is no package called 'ipred'\n## Error in loadNamespace(x): there is no package called 'rsample'\n## Error in loadNamespace(x): there is no package called 'rsample'\n## Error in loadNamespace(x): there is no package called 'rsample'\n```\n:::\n\n\n\nThe bagging() function comes from the ipred package and we use nbagg to control how many iterations to include in the bagged model and coob = TRUE indicates to use the OOB error rate. By default, bagging() uses rpart::rpart() for decision tree base learners but other base learners are available. Since bagging just aggregates a base learner, we can tune the base learner parameters as normal. Here, we pass parameters to rpart() with the control parameter and we build deep trees (no pruning) that require just two observations in a node to split. \n\n\n::: {.cell}\n\n```\n## Error in bagging(formula = success ~ ., data = diamonds, nbagg = 10, coob = TRUE, : could not find function \"bagging\"\n```\n:::\n\n\nOne thing to note is that typically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more). For the Ames data we see that the error is stabilizing with just over 100 trees so we’ll likely not gain much improvement by simply bagging more trees.\n\n\n::: {.cell}\n\n```\n## Error in loadNamespace(x): there is no package called 'ranger'\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n\n## grid search\n\n%% inspiration https://drsimonj.svbtle.com/grid-search-in-the-tidyverse %%\n\n### Training-Test Split\nTo help validate our hyperparameter combinations, we’ll split our data into training and test sets (in an 80/20 split):\n\n::: {.cell}\n\n:::\n\n\n### Create the Grid \n\n\nStep one for grid search is to define our hyperparameter combinations. Say we want to test a few values for minsplit and maxdepth. I like to setup the grid of their combinations in a tidy data frame with a list and cross_d as follows:\n\n\n::: {.cell}\n\n```\n## Error:\n## ! `cross_d()` was deprecated in purrr 0.2.3 and is now defunct.\n```\n:::\n\n\nNote that the list names are the names of the hyperparameters that we want to adjust in our model function.\n\n### Create a model function\n\nWe’ll be iterating down the gs data frame to use the hyperparameter values in a rpart model. The easiest way to handle this is to define a function that accepts a row of our data frame values and passes them correctly to our model. Here’s what I’ll use:\n\n\n::: {.cell}\n\n:::\n\n\n### Fit the models\nNow, to fit our models, use pmap to iterate down the values. The following is iterating through each row of our gs data frame, plugging the hyperparameter values for that row into our model.\n\n\n::: {.cell}\n\n```\n## Error in mutate(., fit = pmap(gs, mod)): object 'gs' not found\n```\n:::\n\n\n### Obtain accuracy\n\n\nNext, let’s assess the performance of each fit on our test data. To handle this efficiently, let’s write another small function:\n\n::: {.cell}\n\n:::\n\nNow apply this to each fit:\n\n::: {.cell}\n\n```\n## Error in mutate(., test_accuracy = map_dbl(fit, compute_accuracy, test_features, : object 'gs' not found\n## Error in eval(expr, envir, enclos): object 'gs_acc' not found\n## Error in rpart.plot(gs$fit[[1]], box.palette = \"RdBu\", shadow.col = \"gray\", : object 'gs' not found\n```\n:::\n\n\n\n## bootstraping\n\n\n%% inspiration https://rapidsurveys.io/learn/statistics/bootstrap/ %%\n\n::: {.cell}\n\n```\n## Error in loadNamespace(x): there is no package called 'rsample'\n## Error in eval(expr, envir, enclos): object 'df_bootstraping' not found\n## Error in library(modeldata): there is no package called 'modeldata'\n## Error in loadNamespace(x): there is no package called 'rsample'\n## Error in vctrs_vec_compat(.x, .purrr_user_env): object 'resample1' not found\n## Error in eval(expr, envir, enclos): object 'resample1' not found\n## Error in eval(expr, envir, enclos): object 'wa_churn' not found\n```\n:::\n\n\n## random Forest\n\n\n::: {.cell}\n\n```\n## Error in library(ranger): there is no package called 'ranger'\n## Error in library(h2o): there is no package called 'h2o'\n```\n:::\n\n\n# CV\n\nperform CV directly within certain ML functions:\n\n\n::: {.cell}\n\n```\n## Error in library(h2o): there is no package called 'h2o'\n## Error in h2o.init(): could not find function \"h2o.init\"\n## Error in loadNamespace(x): there is no package called 'AmesHousing'\n## Error in loadNamespace(x): there is no package called 'h2o'\n## Error in h2o.glm(x = \"Lot_Area\", y = \"Lot_Frontage\", training_frame = ames.h2o, : could not find function \"h2o.glm\"\n```\n:::\n\n\nOr externally as in the below chunk5. When applying it externally to an ML algorithm as below, we’ll need a process to apply the ML model to each resample, which we’ll also cover.\n\n\n::: {.cell}\n\n```\n## Error in loadNamespace(x): there is no package called 'rsample'\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}