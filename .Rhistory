library(blogdown)
new_site(theme ='yihui/hugo-xmin')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::new_post(title = "Hi Hugo",
ext = '.Rmarkdown',
subdir = "post")
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
options(blogdown.method = "markdown")
file.edit(".Rprofile")
file.edit(file.path("~", ".Rprofile"))
file.edit(file.path("~", ".Rprofile"))
# HUGO XMIN
file.edit(file.path("~", ".Rprofile"))
## _Keep it simple, but not simpler_
candidates <- c( Sys.getenv("R_PROFILE"),
file.path(Sys.getenv("R_HOME"), "etc", "Rprofile.site"),
Sys.getenv("R_PROFILE_USER"),
file.path(getwd(), ".Rprofile"),
file.path(Sys.getenv("HOME"), ".Rprofile"))
Filter(file.exists, candidates)
file.edit(file.path("~", "/Users/lrabalski1/.Rprofile"))
file.edit(file.path("/Users/lrabalski1/.Rprofile"))
Sys.getenv("HOME")
file.path(Sys.getenv("HOME"), ".Rprofile")
file.path(Sys.getenv("HOME"), ".Rprofile")
file.edit(file.path(Sys.getenv("HOME"), ".Rprofile"))
blogdown::serve_site()
`options(blogdown.method = "markdown")`
options(blogdown.method = "markdown")
file.edit(".Rprofile")
file.edit(file.path("~", ".Rprofile"))
blogdown::serve_site()
options(blogdown.method = "markdown")
file.edit(file.path("~", ".Rprofile"))
file.edit(file.path("~", ".Rprofile"))
file.edit(".Rprofile")
blogdown::serve_site()
file.edit(file.path("~", ".Rprofile"))
file.edit(".Rprofile")
blogdown::serve_site()
file.edit(".Rprofile")
blogdown::serve_site()
blogdown::serve_site()
#markdown ----
tabela<- function(df, caption){
DT::datatable(data = df,
extensions = 'Buttons',
caption = caption,
options = list(dom = "Blfrtip",pageLength = 10, lengthMenu = c(10, 25, 200),scrollX=TRUE, buttons = c('copy', 'csv', 'excel')
,columnDefs = list(list(className = 'dt-center',targets="_all"))
),
rownames = FALSE,
class = "display")
}
knitr::opts_chunk$set(#fig.width=12,
fig.height=4,
out.width = '100%'
)
knitr::opts_chunk$set(echo = FALSE,
warning = FALSE,
message =FALSE,
include =FALSE)
options(scipen=999)
#plotly ----
#https://stackoverflow.com/questions/34093169/horizontal-vertical-line-in-plotly
vline <- function(x = 0, color = "blue", dash="dash") {
list(
type = "line",
y0 = 0,
y1 = 1,
yref = "paper", #(this puts the graph region on a 0-1 scale, rather than on the x/y data scale)
x0 = x,
x1 = x,
line = list(color = color, dash=dash)
)
}
hline <- function(y = 0, color = "blue") {
list(
type = "line",
x0 = 0,
x1 = 1,
xref = "paper", #(this puts the graph region on a 0-1 scale, rather than on the x/y data scale)
y0 = y,
y1 = y,
line = list(color = color)
)
}
library(DBI)
library(stringr)
library(stringi)
library(readr)
library(dplyr)
library(ggplot2)
library(RPostgreSQL)
library(readr)
library(data.table)
library(scales)
library(lubridate)
library(plotly)
#dla celow markdowna
library(kableExtra)
library(knitr)
library(DT)
# polaczenia ----
tryCatch({
source(("/data/www/final/rscripts/current/header_shiny.R"), encoding='UTF-8')
}, error=function(e) {
source(("C:/Users/lrabalski/Desktop/Mrowisko/header.R"), encoding='UTF-8')
}, error=function(e) {
source('/Users/lrabalski1/Desktop/Repozytoria/rscripts/header.R', encoding='UTF-8')
}
)
polaczenie_taskbox_slave = polaczenie_taskbox_slave()
polaczenie_analizy_slave = polaczenie_analizy_slave()
polaczenie_analizy_master = polaczenie_analizy_master()
polaczenie_ino_slave = polaczenie_ino_slave()
#polaczenie_hurtownia_master_RODBC = polaczenie_hurtownia_master_RODBC()
polaczenie_hurtownia_master = polaczenie_hurtownia_master()
polaczenie_mysql_slave = polaczenie_mysql_slave()
polaczenie_agreements_slave = polaczenie_agreements_slave()
polaczenie_mongo = polaczenie_mongo()
## na macu polaczenie RODBC ma prawo sie wywalic
tryCatch({
polaczenie_hurtownia_master_RODBC = polaczenie_hurtownia_master_RODBC()},
error=function(cond) { },
warning=function(cond) {}
)
## jezeli RODBC sie wywala, ergo laczymy sie z maca, wtedy zastap funkcje i polaczenie odpowiednikami z ODBC
if (class(polaczenie_hurtownia_master_RODBC) == 'function'){
print("RODBC - connection NEGATIVE")
#sqlQuery = dbGetQuery
zapytanie_do_bazy <- function(query){
DBI::dbGetQuery(polaczenie_hurtownia_master, query)
}
}else {
print("RODBC - connection POSITIVE")
zapytanie_do_bazy <- function(query){
RODBC::sqlQuery(polaczenie_hurtownia_master_RODBC, query)
}
}
#autoryzowanie chmury
library('Microsoft365R')
library('AzureAuth')
scopes <- c(
"https://graph.microsoft.com/Files.ReadWrite.All",
"https://graph.microsoft.com/User.Read",
"openid", "offline_access"
)
app <- "d44a05d5-c6a5-4bbb-82d2-443123722380" # for local use only
token <- AzureAuth::get_azure_token(scopes, "grupawppl", app, version=2, auth_type="device_code")
odb <- get_business_onedrive(token=token,tenant="grupawppl")
x<-odb$list_files('Tagowanie/Inne')
# odb$download_file(src="Tagowanie/Inne/TASKI NAZWY KLIENT_AGENT priorytet kategorii (2).xlsx",
#                   dest='/Users/lrabalski1/Desktop/Mrowisko/000Pracownicy/000_Różycki_Adrian/2022.03.05 test automatyzacji/mapowanie.xlsx',
#                   overwrite = TRUE)
dbCallProc(polaczenie_hurtownia_master,'analizy.[dbo].[Analizy_graph_product_feed]')
?sqlExecute
RODBC::sqlExecute(polaczenie_hurtownia_master, 'exec analizy.[dbo].[Analizy_graph_product_feed]',
fetch = TRUE, as.is=T)
??sqlExecute
dbExecute(polaczenie_hurtownia_master,'exec analizy.[dbo].[Analizy_graph_product_feed]')
df<-zapytanie_do_bazy('select * from analizy.pub.graph_product_feed')
View(df)
x<-df %>%
group_by(product_id)  %>%
summarise(starts_at_date=as.character(list(unique(starts_at_date))),
id_lotnisko_wylotu=as.character(list(unique(id_lotnisko_wylotu))),
mean_cena_1_pax = round(mean(cena_wak_1_pax),2))
df_final<-df %>% select(-starts_at_date,-id_lotnisko_wylotu, -cena_wak_1_pax) %>% unique() %>%
left_join(x)
View(df_final)
x<-df %>%
group_by(product_id)  %>%
summarise(cron_started_at = max(cron_started_at),
starts_at_date=as.character(list(unique(starts_at_date))),
id_lotnisko_wylotu=as.character(list(unique(id_lotnisko_wylotu))),
mean_cena_1_pax = round(mean(cena_wak_1_pax),2))
df_final<-df %>% select(-starts_at_date,-id_lotnisko_wylotu, -cena_wak_1_pax,-cron_started_at) %>% unique() %>%
left_join(x)
View(df_final)
sum(duplicated(df_final$product_id))
sum(duplicated(df_final$id_mds_oferty))
df_final[duplicated(df_final$id_mds_oferty)]
df_final[duplicated(df_final$id_mds_oferty),]
View(df_final)
dbExecute(polaczenie_hurtownia_master,'exec analizy.[dbo].[Analizy_graph_product_feed]')
df<-zapytanie_do_bazy('select * from analizy.pub.graph_product_feed')
x<-df %>%
group_by(product_id)  %>%
summarise(cron_started_at = max(cron_started_at),
starts_at_date=as.character(list(unique(starts_at_date))),
id_lotnisko_wylotu=as.character(list(unique(id_lotnisko_wylotu))),
mean_cena_1_pax = round(mean(cena_wak_1_pax),2))
df_final<-df %>% select(-starts_at_date,-id_lotnisko_wylotu, -cena_wak_1_pax,-cron_started_at) %>% unique() %>%
left_join(x)
View(df_final)
dbExecute(polaczenie_hurtownia_master,'exec analizy.[dbo].[Analizy_graph_product_feed]')
df<-zapytanie_do_bazy('select * from analizy.pub.graph_product_feed')
x<-df %>%
group_by(product_id)  %>%
summarise(cron_started_at = max(cron_started_at),
starts_at_date=as.character(list(unique(starts_at_date))),
id_lotnisko_wylotu=as.character(list(unique(id_lotnisko_wylotu))),
mean_cena_1_pax = round(mean(cena_wak_1_pax),2))
df_final<-df %>% select(-starts_at_date,-id_lotnisko_wylotu, -cena_wak_1_pax,-cron_started_at) %>% unique() %>%
left_join(x)
View(df_final)
blogdown:::preview_site()
Sys.setenv(RETICULATE_PYTHON = "/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python")
#reticulate::py_config()
reticulate::py_config()
myenvs=conda_list()
library(reticulate)
myenvs=conda_list()
myenvs=conda_list()
envname=myenvs$name[3]
envname
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
#reticulate::py_config()
reticulate::repl_python()
library(reticulate)
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[3]
envname
use_condaenv(envname, required = TRUE)
reticulate::repl_python()
quit
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[5]
use_condaenv(envname, required = TRUE)
reticulate::repl_python()
#markdown ----
knitr::opts_chunk$set(#fig.width=12,
fig.height=4,
out.width = '100%'
)
knitr::opts_chunk$set(include =TRUE,
warning = FALSE,
message =FALSE,
collapse=TRUE
)
options(scipen=999)
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python")
reticulate::py_config()
myenvs=conda_list()
envname=myenvs$name[3]
use_condaenv(envname, required = TRUE)
envname=myenvs$name[3]
envname
use_condaenv(envname, required = TRUE)
blogdown::build_site()
knitr::opts_chunk$set(fig.align='left', echo=TRUE, error=FALSE)
df_titanic<- read.csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv') %>%
mutate(Fare_log=round(log(Fare+0.001)))
knitr::opts_chunk$set(fig.align='left', echo=TRUE, error=FALSE)
library(tidyverse)
library(dplyr)
library(broom)
library(moderndive)
library(knitr)
library(ggplot2)
df_titanic<- read.csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv') %>%
mutate(Fare_log=round(log(Fare+0.001)))
knitr::opts_chunk$set(fig.align='left', echo=TRUE, error=FALSE)
library(tidyverse)
library(dplyr)
library(broom)
library(moderndive)
library(knitr)
library(ggplot2)
df_titanic<- read.csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv') %>%
mutate(Fare_log=round(log(Fare+0.001)))
df_titanic %>% select(Survived,Fare,Fare_log) %>% head() %>% kable()
model <- glm(data=df_titanic, Survived ~ Fare_log, family = 'binomial')
tidy(model)  %>% kable(caption='Table 1. Summary statistics for logistic regression model')
coef(model)
#Tak przemnozone wspolczynniki interpretujemy nastepujaco:
#  o ile % wzrosnie odds wystapienia zdarzenia jezeli wzrosnie nam wartosc predyktora o 1
exp(coef(model))
df_aug <- augment(model, type.predict = "response") # without response argument, the fitted value will be on log-odds scale
p3 = df_aug$.fitted[df_aug$Fare_log==3][1]
p2 = df_aug$.fitted[df_aug$Fare_log==2][1]
x <- round(p3/(1-p3)/(p2/(1-p2)),5)
# i sprawdzenie czy dobrze rozumiem zależnosc
x1<-round(exp(coef(model))['Fare_log'],5)
x1==x
#While the odds scale is more useful than the probability scale for certain things, it isn't entirely satisfying. Statisticians also think about logistic regression models on the log-odds scale, which is formed by taking the natural log of the odds.
#The benefit to this approach is clear: now the logistic regression model can be visualized as a line! Unfortunately, understanding what the log of the odds of an event means is very difficult for humans.
df_aug <- df_aug %>% mutate(odds=(.fitted/(1-.fitted)), log_odds=log(odds))
df_aug %>%
ggplot()+
geom_line(aes(x=Fare_log,y=.fitted), color='green')+
geom_line(aes(x=Fare_log,y=odds ), color='red')+
geom_line(aes(x=Fare_log,y=log_odds ), color='blue', label='log_odds')
df_aug %>% mutate(Survived_hat=round(.fitted)) %>%
select(Survived, Survived_hat) %>% table
#Out of sample predictions
DiCaprio<-data.frame(Fare_log=1)
augment(model, newdata = DiCaprio, type.predict = 'response')
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_samples, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)
par(mfrow=c(1,2))
# Visualize proportion clicks
hist(proportion_clicks)
# Visualize n_visitors
hist(n_visitors)
prior <- data.frame(proportion_clicks,n_visitors)
p<-prior %>%
ggplot(aes(x= n_visitors, y= proportion_clicks))+
geom_point()
library(ggExtra)
ggMarginal(p, type="histogram")
# Create the posterior data frame
posterior <- prior[prior$n_visitors == 13, ]
# Visualize posterior proportion clicks - below I condition the joint distribution - of prior distribution of proportion_clicks and distribution of n_visitors
hist(posterior$proportion_clicks)
# Assign posterior to a new variable called prior
prior <- posterior
# Take a look at the first rows in prior
head(prior)
# Replace prior$n_visitors with a new sample and visualize the result
n_samples <-  nrow(prior)
n_ads_shown <- 100
prior$n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = prior$proportion_clicks)
hist(prior$n_visitors)
# Explore using the rbeta function
beta_1 <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)
beta_2 <- rbeta(n = 1000000, shape1 = 100, shape2 = 100)
beta_3 <- rbeta(n = 1000000, shape1 = 100, shape2 = 20)
beta_4 <- rbeta(n = 1000000, shape1 = 5, shape2 = 95)
par(mfrow=c(2,2))
hist(beta_1, breaks=seq(0,1,0.02), main = "shape1 = 1, shape2 = 1")
hist(beta_2, breaks=seq(0,1,0.02), main = "shape1 = 100, shape2 = 100")
hist(beta_3, breaks=seq(0,1,0.02), main = "shape1 = 100, shape2 = 20")
hist(beta_4, breaks=seq(0,1,0.02), main = "shape1 = 5, shape2 = 95")
# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown,
prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)
# Create the posteriors for video and text ads
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]
# Visualize the posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))
posterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000],
text_prop = posterior_text$proportion_click[1:4000])
# Calculate the posterior difference: video_prop - text_prop
posterior$prop_diff <- posterior$video_prop - posterior$text_prop
# Visualize prop_diff
hist(posterior$prop_diff)
# Calculate the median of prop_diff
median(posterior$prop_diff)
# Calculate the proportion
mean(posterior$prop_diff > 0.0)
#Different adds have differnt costs then:
visitor_spend <- 2.53
video_cost <- 0.25
text_cost <- 0.05
# Add the column posterior$video_profit
posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
# Add the column posterior$text_profit
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
# Visualize the video_profit and text_profit columns
hist(posterior$video_profit)
hist(posterior$text_profit)
# Add the column posterior$profit_diff
posterior$profit_diff <- posterior$video_profit - posterior$text_profit
# Visualize posterior$profit_diff
hist(posterior$profit_diff)
# Calculate a "best guess" for the difference in profits
median(posterior$profit_diff)
# Calculate the probability that text ads are better than video ads
mean(posterior$profit_diff < 0)
#So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose the text ads is better.
# Change the model according to instructions
n_draws <- 100000
mean_clicks <- runif(n_draws, min = 0, max = 80) #this is my prior
n_visitors <- rpois(n = n_draws, mean_clicks)
prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]
hist(prior$mean_clicks)
hist(posterior$mean_clicks)
#  the temperatures of Sweden water in 21 th of June in few following year
temp <- c(19,23,20,17,23)
# Defining the parameter grid - here are are my priors about the posible values of parameters of distribution
pars <- expand.grid(mu = seq(8,30, by = 0.5),
sigma = seq(0.1, 10, by= 0.3))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)
pars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])
pars$likelihood[i] <- prod(likelihoods)
}
# Calculate the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability )
library(lattice)
levelplot(probability ~ mu * sigma, data = pars)
head(pars)
sample_indices <- sample(1:nrow(pars), size=10000, replace=TRUE, prob=pars$probability)
pars_sample <- pars[sample_indices,c("mu","sigma")]
head(pars_sample)
#rnorm is vectorized and implicitly loops over mu and sigma
pred_temp<- rnorm(10000, mean=pars_sample$mu, sd=pars_sample$sigma)
par(mfrow=c(1,2))
hist(pars_sample$mu,30, main = 'probability distribution of mean temperature')
hist(pred_temp,30, main = 'probability distribution of tempeture' )
mean(pred_temp>=18)
library(rjags)
2+2
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
